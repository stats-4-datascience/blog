[
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Our Software",
    "section": "",
    "text": "Direct and Indirect Additive Modeling with Model Sculpting"
  },
  {
    "objectID": "software.html#stats4phc",
    "href": "software.html#stats4phc",
    "title": "Our Software",
    "section": "stats4phc ",
    "text": "stats4phc \n \nPerformance Evaluation for the Prognostic Value of Predictive Models Intended to Support Personalized Healthcare Through Predictiveness Curves and Positive / Negative Predictive Values"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Coming Soon!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nHow to get the most out of prognostic baseline variables in clinical trials\n\n\nEffective strategies for employing covariate adjustment and stratification\n\n\n\n\n\n\nOct 12, 2022\n\n\nCourtney Schiffman, Christina Rabe, Michel Friesenhahn\n\n\n\n\n\n\n  \n\n\n\n\nEverything you wanted to know about R2 but were afraid to ask\n\n\nPart 1: Using a fundamental decomposition to gain insights into predictive model performance\n\n\n\n\n\n\nOct 26, 2023\n\n\nMichel Friesenhahn, Christina Rabe, Courtney Schiffman\n\n\n\n\n\n\n  \n\n\n\n\nEverything you wanted to know about R2 but were afraid to ask\n\n\nPart 2: Binary outcomes and generalizing the fundamental decomposition\n\n\n\n\n\n\nOct 31, 2023\n\n\nMichel Friesenhahn, Christina Rabe, Courtney Schiffman\n\n\n\n\n\n\n  \n\n\n\n\nOptimizing Pre-screening Tools for Clinical Trials\n\n\nA practical guide with intuitive performance metrics and examples\n\n\n\n\n\n\nMar 11, 2024\n\n\nChristina Rabe, Michel Friesenhahn, Courtney Schiffman, Tobias Bittner\n\n\n\n\n\n\n  \n\n\n\n\nThe Unreasonable Effectiveness of Additive Models\n\n\nBuilding high-performance, trustworthy and insightful Machine Learning Models\n\n\n\n\n\n\nSep 20, 2024\n\n\nMichel Friesenhahn, Ondrej Slama, Kenta Yoshida\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/covariate_adjustment/index.html#covariate-adjustment-increases-precision-and-power",
    "href": "posts/covariate_adjustment/index.html#covariate-adjustment-increases-precision-and-power",
    "title": "How to get the most out of prognostic baseline variables in clinical trials",
    "section": "Covariate Adjustment Increases Precision and Power",
    "text": "Covariate Adjustment Increases Precision and Power\n\n\n\nFigure 1: Covariate adjustment removes explained variability\n\n\nUnexplained variability in the outcome data limits the precision with which you can estimate your marginal average treatment effect (ATE). If baseline covariates are prognostic, subtracting off any reasonable linear combination of those covariates from the observed outcomes, and analyzing the differences instead reduces variability of the treatment effect estimate. The marginal ATE estimate calculated using the residuals (observed-predicted) is still asymptotically unbiased, because the expected value of the predictions is equal across all treatment arms. However, the residuals have less variability than the original outcomes, leading to a more precise estimate."
  },
  {
    "objectID": "posts/covariate_adjustment/index.html#covariate-adjustment-removes-the-impact-of-imbalance",
    "href": "posts/covariate_adjustment/index.html#covariate-adjustment-removes-the-impact-of-imbalance",
    "title": "How to get the most out of prognostic baseline variables in clinical trials",
    "section": "Covariate Adjustment Removes the Impact of Imbalance",
    "text": "Covariate Adjustment Removes the Impact of Imbalance\nA common concern in randomized trials is that treatment groups, by chance, differ in the distributions of one or more important prognostic baseline covariates. When this occurs, one perspective is that this is not a problem since the estimates are still unbiased and type 1 error is still preserved. However, such statistical properties are unconditional on the observed covariate imbalance. When we do condition on observed imbalance, the treatment effect estimates can be biased and type 1 errors can drastically differ from their nominal levels. A simple, clear and insightful analysis of this issue was provided by Stephen Senn who has amusingly remarked “If you are at 35,000 ft, four engines are on fire and the captain has had a heart-attack can you say: ‘Why worry, on average air travel is very safe?’” (S. J. Senn 1989; S. Senn 2010).\n\n\n\nFigure 2: Conditional type-1 error depending on imbalance\n\n\nHow do you address concerns over imbalance? Following the analysis from Stephen Senn, Figure 2 shows the conditional type 1 error as a function of standardized observed imbalance for a range of assumed correlations between the baseline covariate and outcome. This shows that significance tests for imbalance (a commonly used and misguided practice) can fail to flag imbalance effects even when the conditional probability of a type 1 error well exceeds the nominal level. Therefore, significance tests do not reliably control type 1 error. Instead, Senn showed that covariate adjustment removes conditional bias from treatment effect estimates and leads to correct conditional type 1 errors of constant size.\n\n“Analysis of covariance can be recommended on two grounds: increased power and constant conditional size. The former should be sufficient to recommend the method to those who consider that the latter is irrelevant but for those who are concerned about conditional size this is an added advantage” (S. J. Senn 1989)."
  },
  {
    "objectID": "posts/covariate_adjustment/index.html#covariate-adjustment-is-rigorous",
    "href": "posts/covariate_adjustment/index.html#covariate-adjustment-is-rigorous",
    "title": "How to get the most out of prognostic baseline variables in clinical trials",
    "section": "Covariate Adjustment is Rigorous",
    "text": "Covariate Adjustment is Rigorous\nTo address the historical confusion and controversy around covariate adjustment, methodological development has clearly specified the sampling framework, target estimand (i.e. population level parameter) and required assumptions. While we focus on continuous outcomes, the rigor of covariate adjustment also applies to non-continuous outcomes.\n\nSampling framework\nAn important but often overlooked component of specifying an estimand and estimator is the assumed sampling framework. A rigorous, adjusted analysis makes the assumed sampling framework transparent. There are several choices for the sampling framework, but we assume here the commonly used super-population framework. Let \\(N\\) be the total sample size for a randomized trial with several treatment arms and let \\(A=0,1,\\ldots,a\\) be a random variable denoting treatment arm assignment. In the super-population framework, \\(N\\) full data vectors are assumed to be drawn independently from some unknown, joint distribution. Patient \\(i's\\) full data vector \\((X_i, Y_i(0),Y_i(1),\\ldots,Y_i(a))\\) contains baseline covariates \\(X_i\\) and a potential outcome \\(Y_i(k)\\) for each possible treatment assignment \\(k=0,1,\\ldots,a\\). Patient \\(i's\\) observed data vector \\((A_i, X_i, Y_i)\\) consists of their treatment assignment \\(A_i\\), baseline covariates \\(X_i\\) and observed outcome \\(Y_i=Y_i(0)I(A_i=0)+\\ldots+Y_i(a)I(A_i=a)\\). In the case of simple random sampling, \\(A\\) and \\(X\\) are independent.\nIt is worth noting that rigorous theory behind covariate adjustment has also been developed under the Neyman framework, which assumes a fixed, finite population where the only source of randomness is treatment assignment (Lin 2013; Ding, Li, and Miratrix 2017, 2019).\n\n\nTarget Estimand\nConfusion often arises since there are various kinds of covariate adjusted analyses targeting different estimands. For example, covariate adjustment is often used in the estimation of conditional average treatment effects, which are contrasts in treatment arm means that are conditional on baseline covariates. A conditional treatment effect is not a single value, but rather a function of baseline covariates, unless one makes the assumption that the conditional treatment effect is constant.\n\n\n\n\n\n\nConditional Average Treatment Effect\n\n\n\nA contrast (difference, ratio, etc.) between treatment arm means conditional on baseline covariates. Example of a conditional ATE:\n\\[\nE(Y_{active}|X) - E(Y_{control}|X)\n\\tag{1}\\]\n\n\nHowever, in this document, we do not use covariate adjustment for that purpose. Instead, we us covariate adjustment as a tool to more efficiently estimate the marginal ATE:\n\n\n\n\n\n\nMarginal Average Treatment Effect\n\n\n\nA contrast (difference, ratio, etc.) between marginal treatment arm means. Example of a marginal ATE:\n\\[\nE(Y_{active}) - E(Y_{control})\n\\tag{2}\\]\n\n\nSome researchers suggest focusing on estimating conditional treatment effects and using covariate adjustment for that purpose. While estimating conditional and individualized treatment effects is an important research objective, a discussion about how to do so is beyond the scope of this work. Here, we assume estimating the marginal ATE is the primary objective of a label-enabling clinical trial.\nWe note that the terms ‘marginal’ and ‘conditional’ are often conflated with ‘unadjusted’ and ‘adjusted’, however these terms are not synonymous (Daniel, Zhang, and Farewell 2020):\n\n\n\n\n\n\nMarginal vs. Conditional\n\n\n\nUsed to distinguish different kinds of estimands.\n\n\n\n\n\n\n\n\nUnadjusted vs. Adjusted\n\n\n\nUsed to distinguish different kinds of estimators.\n\n\n\n\nAssumptions\nHistorically, researchers have debated how to safely estimate marginal ATEs using covariate adjustment. Some have been hesitant to use covariate adjustment over concerns around model misspecification, bias and possible losses in precision (D. A. Freedman 2008a; David A. Freedman 2008b), while others have enthusiastically used covariate adjustment without a second thought.\nAcademic research ultimately settled that debate (Yang and Tsiatis 2001; Tsiatis et al. 2008; Rosenblum and Laan 2009; Rosenblum and Laan 2010). A number of approaches to covariate adjustment have been characterized that result in consistent, asymptotically normal estimators of marginal ATEs even if the regression model is misspecified. This means that even if important interactions or non-linearities are excluded, prognostic factors are missed, or factors are included that have no prognostic value, covariate adjustment consistently estimates the marginal ATE.\nThe covariate adjustment approach we use is called a standardized regression estimator, which uses predictions from a working regression model to obtain marginal ATEs. The model is referred to as a working regression model because it does not have to represent the true data generating distribution. Therefore, we refer to covariate adjustment as a model-assisted, not model-dependent, analysis. Examples of working regression models include logistic, OLS, and Poisson regression models.\nFor the standardized regression estimator, it is sufficient to assume that treatment is assigned via simple random sampling, that the full data vectors are i.i.d. and that all variables are bounded (Rosenblum and Laan 2009; Rosenblum and Laan 2010). Extensions to other sampling frameworks have also been explored (Wang et al. 2021)."
  },
  {
    "objectID": "posts/covariate_adjustment/index.html#covariate-adjustment-is-encouraged-by-the-fda",
    "href": "posts/covariate_adjustment/index.html#covariate-adjustment-is-encouraged-by-the-fda",
    "title": "How to get the most out of prognostic baseline variables in clinical trials",
    "section": "Covariate Adjustment is Encouraged by the FDA",
    "text": "Covariate Adjustment is Encouraged by the FDA\nIn a May 2023 guidance, pre-specified covariate adjustment was fully supported by the FDA as a primary endpoint analysis in clinical trials:\n\nSponsors can adjust for baseline covariates in the analyses of efficacy endpoints in randomized clinical trials. Doing so will generally reduce the variability of estimation of treatment effects and thus lead to narrower confidence intervals and more powerful hypothesis testing.” (FDA 2023).\n\nImportantly, the guidance states that “[s]ponsors should prospectively specify the detailed procedures for executing covariate adjusted analysis before any unblinding of comparative data. FDA review will emphasize the prespecified primary analysis rather than post-hoc analyses using different models or covariates”(FDA 2023). We interpret this to mean that the exact working regression model form should be pre-specified before working with the trial data. Independent data from observational cohorts or previous trials can be used to select or develop covariates. These covariates may be existing baseline variables or even the output from a multivariable prediction model which takes as input other baseline covariates.\nIn contrast to pre-specifying an ANCOVA regression model, there is active research into applying machine learning directly to the unblinded trial data under investigation (as opposed to historical data) (Williams, Rosenblum, and Diaz 2021; Benkeser et al. 2020; Tsiatis et al. 2008; Tian et al. 2012) as part of the covariate adjusted analysis. While the role machine learning procedures can play in covariate adjustment is an interesting and important area of research, we do not yet recommend using them as part of the primary endpoint analysis of a clinical trial due to several considerations. First, the finite sample properties of the estimators are not understood. Second, these methods add complexity and burden to the study yet are unlikely to provide meaningful benefits.\nIt may not be appreciated that a lot of things need to happen between the unblinding of a randomized clinical trial and the reporting of topline results. Outputs and code are quality controlled, pre-specified sensitivity analyses are performed, and internal reviews are conducted with key decision makers and stakeholders. All of these activities need to occur in a tight time frame for practical considerations such as appropriate control of insider information.\nIn addition, during the planning phase, many choices have to be made when applying machine learning directly to the trial data such as which covariates to present to the machine learning procedure, the method of cross validation to use, which machines to include if using ensemble learning and how to perform standard error estimation. These choices, many of which are based on simulation studies, add to the difficulty of finalizing the statistical analysis plan.\nIn summary, applying machine learning to the trial data as part of the primary endpoint analysis adds considerable burden to the study team. This added burden would only be worthwhile if the expected precision gains were meaningful. However, in our experience, complex black-box models rarely outperform simple linear models when working with tabular data and do not support this additional complexity and time."
  },
  {
    "objectID": "posts/covariate_adjustment/index.html#standardized-regression-estimator",
    "href": "posts/covariate_adjustment/index.html#standardized-regression-estimator",
    "title": "How to get the most out of prognostic baseline variables in clinical trials",
    "section": "Standardized Regression Estimator",
    "text": "Standardized Regression Estimator\nTable 1 provides instructions and code for estimating the Treatment Arm Means (TAMs) and the marginal ATE (in this case, the difference in means) using covariate adjustment. The example code shows how to estimate these quantities for trials with two treatment arms, but easily generalizes to more than two arms. This estimator is often referred to as the standardized regression estimator, and is also an example of a targeted maximum likelihood estimator. There are other consistent estimators of the marginal ATE that use covariate adjustment (see Colantuoni and Rosenblum (2015) for examples). However, we suggest using the standardized regression estimator because it is easy to use, is a “reliable method for covariate adjustment” (FDA 2023), has comparable power and can also be used to estimate TAMS and marginal ATEs for non-continuous outcomes.\nIn the notation below, \\(A\\) is a two-level factor indicating treatment assignment, \\(Y\\) is the observed outcome, \\(X\\) is a baseline covariate, \\(E[Y|a, X_i] = \\mu(a,X_i)\\) is the expected value of \\(Y\\) for patient \\(i\\) given their treatment assignment \\(A_i=a\\) and baseline covariate \\(X_i\\), \\(\\hat{\\mu}(a,X_i)\\) is the predicted outcome for patient \\(i\\) from the working regression model and an estimate of \\(\\mu(a,X_i)\\), and \\(N\\) is the total trial sample size. The standardized regression estimator for the mean outcome in treatment arm \\(A=a\\) is:\n\\[\n\\widehat{TAM_a}=\\frac{1}{N}\\sum_{i=1}^{N}\\hat{\\mu}(a,X_i)\n\\tag{3}\\]\nThe standardized regression estimator for the difference in means for treatment arms \\(A=0\\) and \\(A=1\\) is:\n\\[\n\\widehat{ATE}=\\frac{1}{N}\\sum_{i=1}^{N}\\hat{\\mu}(1,X_i)-\\frac{1}{N}\\sum_{i=1}^{N}\\hat{\\mu}(0,X_i)\n\\tag{4}\\]\nOther contrasts of the marginal means (TAMs) may be similarily estimated.\n\n\n\nTable 1: Standardized regression estimator for TAMs and marginal ATE\n\n\n\n\n\n\n\n\nStep\nEstimation Instructions\nExample R Code\n\n\n\n\n1\nFit a working regression model using data from all treatment arms, regressing the outcome on treatment and prognostic baseline covariates\n# Additive Working Regression Models\nmod.fit <- lm(Y ~ A + X, data = d)  # continuous\nmod.fit <- glm(Y ~ A + X, data = d, family = binomial)  # binary\n\n# Working Regression Models with Interaction\nmod.fit <- lm(Y ~ A * X, data = d)  # continuous\nmod.fit <- glm(Y ~ A * X, data = d, family = binomial)  # binary\n\n\n\n2\nFor each subject, use the model from step 1 and the subject’s baseline covariates to compute their predicted outcome under EACH treatment of interest (regardless of what their assigned treatment was)\n# Continuous outcome\npred0 <- predict(mod.fit, newdata = data.frame(A=0, X=d$X))\npred1 <- predict(mod.fit, newdata = data.frame(A=1, X=d$X))\n\n# Binary outcome\npred0 <- predict(mod.fit, newdata = data.frame(A=0, X=d$X), type=‘response’)\npred1 <- predict(mod.fit, newdata = data.frame(A=1, X=d$X, type=‘response’)\n\n\n\n3\nTake the average of the predicted outcomes in each treatment group to get estimates of the Treatment Arm Means (TAMs)\n# Mean of predictions in treatment arm A=0\nTAM0 <- mean(pred0)\n\n# Mean of predictions in treatment arm A=1\nTAM1 <- mean(pred1)\n\n\n\n4\nCompute the desired contrast of the TAMs to get an estimate of the marginal ATE\n# Estimate the desired contrast, for example, the\n# difference between treatment arm A=1 and A=0\nATE <- TAM1 - TAM0\n\n\n\n\n\n\nIn the example code in Table 1, \\(X\\) is a single covariate, but \\(X\\) can also be a matrix containing a set of individual covariates, a prediction from an independent prognostic model, or a combination thereof. When the working regression model is an additive OLS regression, the standardized regression estimate of the difference in means marginal ATE equals the estimated coefficient for \\(A\\). This is also true for an interaction model when the covariates are centered. Note that, unlike OLS regression, in generalized linear models the estimated coefficient for treatment does not translate into a marginal ATE.\nTheoretically, it is possible to lose precision with covariate adjustment when using an additive model compared to an unadjusted analysis. In contrast, a standardized regression estimator that uses an interaction working regression model is asymptotically guaranteed to be at least as precise as an unadjusted estimator or adjustment with an additive model. However, it is only under unrealistic conditions that covariate adjustment with an additive model will be less precise than an unadjusted estimator. What is more, if randomization is 1:1 or the covariances between covariates and outcome are equal across treatment arms, then covariate adjustment with an additive model will be as efficient as with an interaction model. In our experience, interaction models do not lead to considerable gains in precision over additive models. Likewise, Tsiatis et al. (2001) performed simulations comparing precision when using an additive vs. an interaction model and “found the loss of efficiency to be trivial in all cases” (Yang and Tsiatis 2001). Therefore, additive models are a good default choice especially if your trial sample size is too small to support treatment-by-covariate interactions (see model budget section).\nIn case of missing values in baseline covariates, a baseline covariate that has too many missing values should not be used for adjustment, but if the number of missing values is small, we recommend simple, single imputation as opposed to using indicator variables in order to spend the model budget wisely."
  },
  {
    "objectID": "posts/covariate_adjustment/index.html#inference-for-tams-and-ates",
    "href": "posts/covariate_adjustment/index.html#inference-for-tams-and-ates",
    "title": "How to get the most out of prognostic baseline variables in clinical trials",
    "section": "Inference for TAMs and ATEs",
    "text": "Inference for TAMs and ATEs\nWe suggest using robust standard error estimators from the targeted maximum likelihood estimation (TMLE) literature because they are robust to model misspecification, are fast and easy to implement with a few lines of code, are valid under any randomization ratio and data generating distribution, provide standard error estimates for both ATEs and treatment arm means, are aligned with the superpopulation framework and are deterministic (Rosenblum and Laan 2010).\nLet \\(I(A_i=a)\\) be an indicator of patient \\(i\\) receiving treatment \\(A=a\\) and \\(p(A=a)\\) be the probability of being assigned to treatment arm \\(A=a\\). The TMLE standard error estimate for the mean of treatment arm \\(A=a\\) is:\n\\[\n\\sqrt{\\frac{\\frac{1}{N}\\sum_{i=1}^{N}\\Bigg(\\frac{I(A_i=a)(Y_i-\\hat{\\mu}(a,X_i))}{p(A=a)}+\\hat{\\mu}(a,X_i)-\\frac{1}{N}\\sum_{i=1}^{N}\\hat{\\mu}(a,X_i)\\Bigg)^2}{N}}\n\\tag{5}\\]\nThe standard error estimate for the difference in Means for Treatment arms \\(A=1\\) and \\(A=0\\) is:\n\n\\[\n\\sqrt{\\frac{\\frac{1}{N}\\sum_{i=1}^{N}\\Bigg(\\frac{I(A_i=1)(Y_i-\\hat{\\mu}(1,X_i))}{p(A=1)}+\\hat{\\mu}(1,X_i)-\\frac{1}{N}\\sum_{i=1}^{N}\\hat{\\mu}(1,X_i) - \\Big(\\frac{I(A_i=0)(Y_i-\\hat{\\mu}(0,X_i))}{p(A=0)}+\\hat{\\mu}(0,X_i)-\\frac{1}{N}\\sum_{i=1}^{N}\\hat{\\mu}(0,X_i)\\Big) \\Bigg)^2}{N}}\n\\tag{6}\\]\n\nThe code for the above SE estimators is shown below.\n\n## SE estimate for treatment arm mean A=0\nsqrt(mean(((A == 0) * (Y - pred0) / mean(A == 0) + pred0 - mean(pred0)) ^ 2) / nrow(d))\n\n## SE estimate for treatment arm mean A=1\nsqrt(mean(((A == 1) * (Y - pred1) / mean(A == 1) + pred1 - mean(pred1)) ^ 2) / nrow(d))\n\n## SE estimate for the marginal ATE, the difference between treatment arm mean A=1 and A=0\nsqrt(mean(((A == 1) * (Y - pred1) / mean(A == 1) + pred1 - mean(pred1) -\n        ((A == 0) * (Y - pred0) / mean(A == 0) + pred0 - mean(pred0))) ^ 2) / nrow(d))\n\nUsing the estimated standard errors and the asymptotic normality of the standardized regression estimator, one can then easily obtain \\(p\\)-values and \\(95\\%\\) confidence intervals for both the treatment arm means and the marginal ATE.\nWe note that the above standard error estimates assume observations are i.i.d. (i.e. randomization is not stratified), and we address this assumption in a later section. For standard error estimates corresponding to marginal ATEs using different contrasts (e.g. ratio instead of difference) see Rosenblum and van der Laan (2010).\nAlternative standard error estimators include the Huber-White estimator and re-sampling based methods like the non-parametric bootstrap. In their recent guidance, the FDA recommends using a “robust standard error method such as the Huber-White “sandwich” standard error when the model does not include treatment by covariate interactions” and they recognize that “other robust standard error methods proposed in the literature can also cover cases with interactions” (FDA 2023). If using a working regression model with treatment-by-covariate interactions, the expected sandwich standard error estimates may be too small when treatment effects are heterogeneous (see Appendix and Imbens and Wooldridge (2009) , Lin (2013)). Bootstrap methods are a reasonable alternative, but we prefer TMLE estimators since they are easier to implement and less computationally intensive. Furthermore, bootstrap methods lead to confidence intervals and p-values that are not deterministic. An intriguing alternative to the bootstrap that is deterministic is the jackknife (Wolbers et al. 2022)."
  },
  {
    "objectID": "posts/covariate_adjustment/index.html#determining-your-model-budget",
    "href": "posts/covariate_adjustment/index.html#determining-your-model-budget",
    "title": "How to get the most out of prognostic baseline variables in clinical trials",
    "section": "Determining your Model Budget",
    "text": "Determining your Model Budget\nIs there such a thing as adjusting for too many prognostic covariates? Yes! The marvelous properties of covariate adjustment discussed thus far (consistency, robustness to model misspecification, precision gains, etc.) are large sample properties. Thus, adjusting for too many covariates in your working model relative to your sample size may invalidate the analysis (Rosenblum 2020; Colantuoni and Rosenblum 2015; Rosenblum and Steingrimsson 2016), and the FDA advises against adjusting for too many covariates:\n\n“The statistical properties of covariate adjustment are best understood when the number of covariates adjusted for in the study is small relative to the sample size.”(FDA 2023)\n\nExplicit guidance on how many covariates to adjust for is not offered, so we borrow from prognostic modeling practices (Harrell 2011).\n\n\n\n\n\n\nModel Budget\n\n\n\nThe number of allowable terms in the working regression model, excluding the overall intercept. Suggestions for a model budget based on classic events per variable considerations are (Harrell 2011):\n\n\n\n\n\n\n\n\nterms (% of total sample size)\n\nguidance\n\n\n\n\n\\(\\leq 5\\%\\)\n\nlikely a safe choice\n\n\n\\(\\approx7.5\\%\\)\n\nprobably reasonable\n\n\n\\(\\gt 10\\%\\)\n\npotentially unsafe\n\n\n\n\n\nPre-specifying a set of covariates and a working regression model form that satisfies the model budget maintains the rigor and integrity of the statistical analysis. We note that this is a rough guidance and should be considered a useful starting point. Recently, model budget calculations for general prognostic modeling have been refined and in future work it would be helpful to adapt such an approach to covariate adjustment (see R. D. Riley et al. (2019) and R. Riley et al. (2020))."
  },
  {
    "objectID": "posts/covariate_adjustment/index.html#spending-your-model-budget-wisely",
    "href": "posts/covariate_adjustment/index.html#spending-your-model-budget-wisely",
    "title": "How to get the most out of prognostic baseline variables in clinical trials",
    "section": "Spending your Model Budget Wisely",
    "text": "Spending your Model Budget Wisely\n\n\n\nThe model budget tells you how complex the working regression model can be while ensuring treatment effect estimates are unbiased, confidence intervals have good coverage, and type 1 error rates for hypothesis testing do not exceed nominal levels. However, there are many options when pre-specifying the working regression model and it is easy to spend beyond one’s model budget, so strategic choices will need to be made prior to looking at the data.\nWe suggest determining the model form and spending the model budget according to the following four principles:\n\nLeverage external/historical data\nHistorically, pre-specification of a regression model was guided by ad hoc analyses reported in the literature, input from subject matter experts, and precedent set by previous trials. While these continue to be important considerations, big data collection and curation and the introduction of machine learning methods to drug development significantly improve the process of pre-specifying the regression model form.\nPrioritize maximally prognostic covariates\nThe list of candidate prognostic covariates may be too large according to the model budget, in which case a subset of prognostic covariates will need to be selected. We use external/historical data to guide that selection (see upcoming section on Performance Metrics).\n\n\n\n\n\n\n\nPro Tip\n\n\n\nIf the model budget is tight and several covariates are expected to meaningfully add prognostic value, consider combining them into a single score and using that in the model instead of the individual covariates.\n\n\n\n\n\nFigure 3: Leverage independent data to find the most promising prognostic factors/models for covariate adjustment\n\n\n\nAvoid Dichotomania!\nUsually the relationship between a continuous variable and outcome varies gradually and abrupt dichotomization loses prognostic information Stephen J. Senn (2005). We recommend that, unless there is evidence to the contrary, prognostic covariates that have been discretized be replaced with their underlying continuous forms. Note that, in particular, this will impact how stratification factors enter into the model.\nBy default, do not include interactions between covariates\nThere is a large body of evidence showing that for tabular data, purely additive models are very rarely, if ever, meaningfully improved by including interactions (Rudin 2019). This has also been our experience with many prognostic modeling projects. We do not recommend including interactions between prognostic covariates in the model since these are usually wasteful expenses in the model budget, unless evidence to the contrary is available.\nBy default, do not include interactions between covariates and treatment arm\nAs discussed above, such interactions rarely provide meaningfully improved precision and power under realistic assumptions.\n\nUse the above five principles to pre-specify a working regression model and to spend your model budget wisely. Then, use this model in the previously described standardized estimator to estimate the treatment arm means and marginal ATEs and to perform inference."
  },
  {
    "objectID": "posts/covariate_adjustment/index.html#accounting-for-stratified-randomization",
    "href": "posts/covariate_adjustment/index.html#accounting-for-stratified-randomization",
    "title": "How to get the most out of prognostic baseline variables in clinical trials",
    "section": "Accounting for Stratified Randomization",
    "text": "Accounting for Stratified Randomization\nThe estimation and inference procedures described in the previous sections assume simple randomization, where each patient’s treatment assignment is an independent and identically distributed Bernoulli random variable for a trial with two treatment arms and a categorical random variable for a trial with more than two treatment arms. However, randomization in clinical trials is usually stratified by selected baseline covariates, for example, using permuted block randomization. In this case, the FDA recommends “that the standard error computation account for stratified randomization” because “an analysis ignoring stratified randomization is likely to overestimate standard errors and can be unduly conservative when performing inference for the average treatment effect” (FDA 2023).\nHowever, we recommend that it is NOT necessary to modify estimation and inference to account for stratified randomization. Instead, we propose that the previously described strategy for spending the model budget wisely be used and applied to all prognostic candidates, making no distinction between stratification factors and other baseline covariates. Then use covariate adjustment as described in the previous sections for estimation and inference of the marginal ATEs, as if treatment were allocated using simple random sampling.\nNote that, as a consequence:\n\nBy default, if stratification variables do enter into the working regression model, they will do so as additive terms.\nDichotomized stratification factors will be replaced by their underlying continuous variables.\nStratification variables not expected to be correlated with outcome in any treatment group may be omitted entirely.\n\nSuch an approach still maintains type 1 error and leads to consistent estimators for marginal ATEs. At worst, inference is conservative (p-values may be too large and confidence intervals too wide) (Wang et al. 2021). However, if the model budget is spent wisely to obtain a sensible working regression model, conservatism will be negligible."
  },
  {
    "objectID": "posts/covariate_adjustment/index.html#intuitive-metrics-for-quantifying-the-expected-benefit-of-adjustment",
    "href": "posts/covariate_adjustment/index.html#intuitive-metrics-for-quantifying-the-expected-benefit-of-adjustment",
    "title": "How to get the most out of prognostic baseline variables in clinical trials",
    "section": "Intuitive Metrics for Quantifying the Expected Benefit of Adjustment",
    "text": "Intuitive Metrics for Quantifying the Expected Benefit of Adjustment\nWhen analyzing external/historical data in preparation for pre-specifying a covariate-adjusted primary endpoint analysis, it’s important to understand the following:\n\nWhich covariates to adjust for\nThe expected precision gains from adjustment\n\nBelow, we provide performance metrics that allow study statisticians to easily select which covariates to adjust for, to understand the impact of heterogeneous treatment effects (HTEs) on precision gains and to estimate the expected benefit from adjustment without having to perform simulations.\nA great way to derive a single, high-quality covariate is to use external data to construct a prognostic model that combines multiple covariates into a single prognostic score. Thus, we focus here on how to evaluate a single prognostic covariate. However, if the model budget allows and the prognostic model is originally linear in form, the individual covariates can be expanded out in the working regression model and re-fit, usually with no loss in performance.\nFor practical use in a trial, prognostic models should achieve maximum performance with minimal complexity. Complexity is determined by the:\n\nCost, incovenience, and invasiveness of covariates.\nEase of deployment and implementation of the prognostic model.\nAlgorithmic complexity (e.g. linear models vs deep learners).\n\nWe find that linear prognostic models are hard to beat when working with tabular data. Deep learning models are more likely to provide a boost in performance over linear models when working with imaging data. Deep learners are natural candidates when working with non-tabular data, such as raw image files. Later we will show an example where considerable gains are achieved using a deep learner trained to predict progression in an ophthalmologic disease from raw images.\nAn intuitive metric for the expected benefit of a prognostic score is the amount by which it effectively increases the study’s sample size via the gain in precision, i.e. the Effective Sample Size Increase (ESSI). For example, suppose adjusting for a particular covariate is associated with an estimated ESSI of \\(40\\%\\). That means, asymptotically, the following two analyses have the same power:\n\nA covariate-adjusted analysis with sample size \\(N\\)\nAn unadjusted analysis but with \\(40\\%\\) more patients, \\(1.4 * N\\)\n\nESSI estimates can be used to compare sets of covariates or prognostic models and make transparent the expected benefits of adjustment."
  },
  {
    "objectID": "posts/covariate_adjustment/index.html#essi-formulas",
    "href": "posts/covariate_adjustment/index.html#essi-formulas",
    "title": "How to get the most out of prognostic baseline variables in clinical trials",
    "section": "ESSI Formulas",
    "text": "ESSI Formulas\nFor simplicity, we assume 1:1 randomization and equal, marginal variances across the treatment arms. We also provide ESSI formulas that relax these assumptions.\nIt turns out that the ESSI from covariate adjustment for continuous outcomes can be easily estimated since it only depends on two parameters:\n\n\n\n\n\n\nGeneral ESSI\n\n\n\n\\[\nGeneral \\space ESSI=\\Bigg(\\frac{1}{1 - \\big(\\frac{r_{control} \\space + \\space r_{active}}{2}\\big)^2}-1\\Bigg) * 100\\%\n\\tag{7}\\]\n\\[\n\\begin{align*}\n  \\text{where} \\\\\n  r_{active} &= \\text{Correlation between the outcome and covariate in the active,} \\\\\n  r_{control} &= \\text{Correlation between the outcome and covariate in the control arm}\n\\end{align*}\n\\]\n\n\nIt makes intuitive sense that the ESSI depends on the squared average of the correlations in the two treatment arms, increasing as the correlations increase. The correlation between the covariate and the outcome among untreated patients (\\(r_{control}\\)) can be estimated using historical trial data, observational cohorts, etc.\nThis leaves \\(r_{active}\\) as the only quantity left to estimate. To help estimate \\(r_{active}\\), note that the correlation between the outcome and the covariate in the treatment arm depends on the “treatment effect scenario.” Two plausible treatment effect scenarios are discussed in the following sections.\n\nConstant absolute treatment effect\nOne commonly assumed treatment effect scenario is that the absolute treatment effect is constant across the baseline covariate:\n\n\n\n\n\n\nConstant absolute treatment effect:\n\n\n\n\\(E(Y_{active}|X)=E(Y_{control}|X)-\\delta\\), for some constant \\(\\delta\\). With a constant absolute treatment effect, \\(r_{control}=r_{active}\\). Plugging this into the general ESSI formula we get\n\\[\nESSI \\space Assuming \\space Constant \\space Absolute \\space \\delta =\\Bigg(\\frac{1}{1-r^2_{control}}-1\\Bigg) * 100\\%\n\\tag{8}\\]\n\n\n\n\n\n\n\nFigure 4: Constant Absolute Treatment Effect\n\n\n\n\n\n\nConstant proportional treatment effect\nA second plausible treatment effect scenario is that the relative treatment effect is constant across the baseline covariate:\n\n\n\n\n\n\nConstant proportional treatment effect\n\n\n\n\\(E(Y_{active}|X)=(1-\\delta)E(Y_{control}|X)\\), for some constant \\(\\delta\\). With a constant proportional treatment effect, \\(r_{active}=(1-\\delta)r_{control}\\). Plugging this into the general ESSI formula we get\n\\[\nESSI\\space Assuming \\space Constant \\space Proportional \\space \\delta=\\Bigg(\\frac{1}{1-r^2_{control}\\big(1-\\frac{\\delta}{2}\\big)^2}-1\\Bigg) * 100\\%\n\\tag{9}\\]\n\n\n\n\n\n\n\nFigure 5: Constant Proportional Treatment Effect"
  },
  {
    "objectID": "posts/covariate_adjustment/index.html#impact-of-htes-on-gains-from-covariate-adjustment",
    "href": "posts/covariate_adjustment/index.html#impact-of-htes-on-gains-from-covariate-adjustment",
    "title": "How to get the most out of prognostic baseline variables in clinical trials",
    "section": "Impact of HTEs on gains from covariate adjustment",
    "text": "Impact of HTEs on gains from covariate adjustment\nA constant proportional treatment effect is an example of HTEs. A treatment effect is heterogeneous if the magnitude of the treatment effect (on the absolute scale) depends on the value of the baseline covariate. HTEs decrease the precision gains from adjustment. For example, notice how, in the constant proportional treatment effect formula, the ESSI decreases as \\(\\delta\\) increases (i.e. as heterogeneity increases). It is important to recognize the impact of HTEs when making decisions about reducing sample size in anticipation of increased power due to covariate adjustment."
  },
  {
    "objectID": "posts/covariate_adjustment/index.html#a-hypothetical-example",
    "href": "posts/covariate_adjustment/index.html#a-hypothetical-example",
    "title": "How to get the most out of prognostic baseline variables in clinical trials",
    "section": "A Hypothetical Example",
    "text": "A Hypothetical Example\nSuppose you are working on pre-specifying an adjusted analysis for an upcoming phase 2 trial. Assume that the targeted effect size is a \\(25\\%\\) reduction in mean decline in the treatment arm compared to the mean decline in the placebo arm for a progressive disease that has a continuous outcome measure. Furthermore, assume that you have developed a prognostic model using external/historical data and that the estimated correlation between the prognostic model predictions and the outcome was \\(0.45\\) (\\(r^2_{control}\\approx0.2\\)). The following is an example of how to calculate ESSIs from covariate adjustment under two plausible treatment effect scenarios that are consistent with the targeted effect size:\n\n\n\n\n\n\nTreatment Effect Scenario\n\n\n\\(r_{control}\\)\n\n\n\\(r_{active}\\)\n\n\nESSI\n\n\n\n\n\n\nConstant Absolute Delta\\(E(Y_{active}|X)=E(Y_{control}|X)-\\delta\\)\n\n\n\\(0.45\\)\n\n\n\\(0.45\\)\n\n\n\\(\\Big(\\frac{1}{1-r^2_{control}}-1\\Big)*100\\%=\\)\\(\\Big(\\frac{1}{1-{0.2}}-1\\Big)*100\\% =\\) \\(25\\%\\)\n\n\n\n\nConstant Proportional Delta \\(E(Y_{active}|X)=(1-\\delta)E(Y_{control}|X)\\)\n\n\n\\(0.45\\)\n\n\n\\((1-\\delta)r_{control} =\\)\\((1-.25)*0.45\\)\n\n\n\\(\\Bigg(\\frac{1}{1-r^2_{control} \\; \\; \\big ( 1-\\frac{\\delta}{2}\\big)^2}-1\\Bigg)*100\\%=\\)\\(\\Bigg(\\frac{1}{1-0.2\\big(1-\\frac{\\delta}{2}\\big)^2}-1\\Bigg)*100\\% =\\) \\(18\\%\\)\n\n\n\n\nNo Correlation in Treatment Arm\n\n\n\\(0.45\\)\n\n\n\\(0\\)\n\n\n\\(\\Bigg(\\frac{1}{1-\\frac{r^2_{control}}{4}}-1\\Bigg)*100\\%=\\)\\(\\Bigg(\\frac{1}{1-\\frac{0.2}{4}}-1\\Bigg)*100\\% =\\) \\(5\\%\\)\n\n\n\n\n\n\nAssuming a constant absolute delta, the effective sample size increase from covariate adjustment is expected to be \\(25\\%\\). However, with a constant proportional treatment effect (which implies HTEs), the ESSI decreases to \\(18\\%\\). A third, not entirely unrealistic worst-case scenario of no correlation between the covariate and outcome in the active treatment arm reduces the ESSI to just \\(5\\%\\). The drop in ESSI with HTEs would be an important caveat to communicate if there is a temptation to decrease trial sample size.\nAll ESSI formulas assume a particular pattern of HTEs. Due to the risk of under-powering, if no prior data is available on HTEs, we do not routinely recommend reducing sample sizes for Phase 2 trials. Usually when designing a Phase 3 trial, there are data from earlier studies on HTEs that can be used to get direct estimates of HTEs and improve ESSI estimates. These improved ESSI estimates could be used to inform sample size reduction, if other trial requirements allow for it (e.g. safety and subgroup analyses). Whenever sample size is reduced, be sure to communicate these risks, consider the noise in ESSI estimates and study-to-study effects and tailor sample size reductions accordingly."
  },
  {
    "objectID": "posts/covariate_adjustment/index.html#prognostic-model-development",
    "href": "posts/covariate_adjustment/index.html#prognostic-model-development",
    "title": "How to get the most out of prognostic baseline variables in clinical trials",
    "section": "Prognostic Model Development",
    "text": "Prognostic Model Development\nThe modeling and data strategy was pre-specified and allowed a rigorous and fair comparison of a number of models with different levels of complexity and operational burden. The goal was to find the least complex model with the best performance.\nData from a previous GA development program was harmonized and divided into training, hold-out, and independent test sets as shown in the figure. The independent tests sets consisted of two studies that were entirely excluded from the training and hold-out evaluations. Within the training data set, models were trained using additional re-sampling techniques for parameter tuning and performance estimation.\n\n\n\nFigure 8: Data Strategy for Model Development\n\n\nDuring training a model was selected from each of three model groups for hold-out and independent test set evaluation. These model groups were defined as:\n\nModel Group 1 (Benchmark): The first group of models was based on a set of standard, pre-specified features that are assessed at baseline. Various types of models were evaluated and compared, from a simple linear model to different types of machine learning models (e.g. support vector machines, gradient boosted machines, random forests, etc.). The features were:\n\nDemographics: age, sex, smoking status\nMeasures of visual acuity: best corrected visual acuity (BCVA), low luminance visual acuity (LLVA), low luminance deficit (LLD = BCVA - LLVA)\nAnatomical features assessed by a reading center: lesion size, lesion contiguity, lesion location, lesion distance to fovea, reticular pseudodrusen\n\nModel Group 2 (Benchmark + Run-in): This group additionally included a “run-in” estimate of growth rate, i.e. using the first 6 month to estimate the GA growth rate and predicting the future. Note that this required re-baselining the remaining outcome data to the 6 month time point. Due to the linear growth and low within patient noise, this is a promising approach but operationally complex to include in a trial as it requires high quality historical images from patients, or starting the trial with a pre-treatment run-in period, which may slow trial recruitment and delay the trial readout.\nModel Group 3 (DL Models): The third group of models consists of end-to-end deep learning (DL) models from FAF and also OCT images at baseline. Three types of models were developed (each exploring different architectures): using FAF only, using OCT only and using FAF and OCT together (Anegondi et al. 2022). The FAF only model would be the operationally preferred model.\n\nOne model was selected from each model group using the relevant performance metrics (\\(r^2\\) and ESSI) estimated using re-sampling techniques. The three selected models were then tested and compared on the holdout and independent test sets."
  },
  {
    "objectID": "posts/covariate_adjustment/index.html#results",
    "href": "posts/covariate_adjustment/index.html#results",
    "title": "How to get the most out of prognostic baseline variables in clinical trials",
    "section": "Results",
    "text": "Results\nThe selected models were:\n\nModel Group 1 (Benchmark): A simple linear model with four features (lesion size, lesion contiguity, lesion distance to fovea and LLD) gave the best performance. More complex models like tree based models or added features did not add performance.\nModel Group 2 (Benchmark + Run-in): A simple linear model with the four features from model group 1 and an estimate of growth rate as an additional fifth feature.\nModel Group 3 (DL Models): A multi-task DL (CNN) model that uses a single FAF image as input to predict GA lesion size (same image) and GA growth rate (Anegondi et al. 2022).\n\nThe results were quite impressive; although operationally complex to implement, the run-in model appears to effectively increase the sample size by \\(1/3\\) compared to an analysis using the benchmark model. If no DL FAF model were available, the additional complexity of the run-in might be justified by the \\(ESSI\\). However, the DL FAF model outperformed the benchmark and even the benchmark+run-in model in the hold-out as well as the two independent test sets (Anegondi et al. 2022). This implies that the logistical complexities of the run-in model can be avoided with the DL FAF model, which is based on a single baseline time point and is still relatively simple to implement (Anegondi et al. 2022). The table shows that the DL FAF model substantially increases the ESSI by at least 90% when compared to an unadjusted analysis and by 40%-80% compared to a simple adjustment using the known baseline features.\n\n\n\nFigure 9: Results from selected model per group"
  },
  {
    "objectID": "posts/covariate_adjustment/index.html#model-in-action",
    "href": "posts/covariate_adjustment/index.html#model-in-action",
    "title": "How to get the most out of prognostic baseline variables in clinical trials",
    "section": "Model in Action",
    "text": "Model in Action\nThe MAHALO study independent test set shown in the previous section was the Phase 2 trial for Lampalizumab. There was an observed treatment effect of ∼20% slowing progression in the higher dose (LQ4 - monthly treatment) compared to the Sham (no treatment) arm. This effect was not confirmed by the two large global Phase 3 trials (SPECTRI and CHROMA), so in retrospect we know this was a false-positive result. When the decision was made to move forward with a Phase 3 trial, the DL FAF model was not available. However, it is an interesting post-hoc exercise to see how results from the Phase 2 trial would have changed had the DL FAF model been used for covariate adjustment.\nFigure 10 shows the estimated mean changes in GA lesion size with and without adjusting for the FAF DL model. Covariate adjustment leads to better precision and hence tighter confidence intervals for the treatment effect estimate and also changes the estimate in case of imbalances. In this example, the estimate changes (from 20% to 6%) and becomes non-significant (a p-value threshold of 0.2 is usually used in a Phase 2 trial).\n\n\n\nFigure 10: GA progression over time by treatment arm for unadjusted and adjusted analysis\n\n\n\n\n\nFigure 11: Treatment effect estimates\n\n\nWhile there were many other considerations in the decision to move into Phase 3 trials (e.g. subgroup analyses), if results from the adjusted analysis had been available, Phase 3 decision making may have been different."
  },
  {
    "objectID": "posts/covariate_adjustment/index.html#essi-formulas-1",
    "href": "posts/covariate_adjustment/index.html#essi-formulas-1",
    "title": "How to get the most out of prognostic baseline variables in clinical trials",
    "section": "ESSI Formulas",
    "text": "ESSI Formulas\n\n\\(Y\\) continuous outcome, \\(X\\) baseline covariate\nConstant absolute \\(\\delta\\): \\(E(Y_{active}|X)=E(Y_{control}|X)-\\delta\\)\nConstant proportional \\(\\delta\\): \\(E(Y_{active}|X)=(1-\\delta)E(Y_{control}|X)\\)\n\\(r_{control}\\) and \\(r_{active}\\): correlation between \\(Y\\) and \\(X\\) given control and active treatment, respectively.\n\\(k=\\frac{\\sigma_{Y_{active}}}{\\sigma_{Y_{control}}}\\)\n\\(\\pi=Probability\\;of\\;active\\;treatment\\). Note, when \\(\\pi=1/2\\) the ESSI corresponding to the additive model will always equal the ESSI corresponding to the interaction model.\n\n\n\n\nFigure 12: ESSI Formulas"
  },
  {
    "objectID": "posts/covariate_adjustment/index.html#huber-white-sandwich-estimator",
    "href": "posts/covariate_adjustment/index.html#huber-white-sandwich-estimator",
    "title": "How to get the most out of prognostic baseline variables in clinical trials",
    "section": "Huber White Sandwich Estimator",
    "text": "Huber White Sandwich Estimator\nBelow is a simple simulation demonstrating how the Huber-White robust “sandwich” estimator will give standard error estimates that are too small when treatment effects are heterogeneous and a working regression model with treatment-by-covariate interactions is used.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(sandwich)\nlibrary(doParallel)\nlibrary(foreach)\nregisterDoParallel(cores=4)\n\n################### SIMULATION STUDIES\n\n####### Paradigm 1: Superpopulation\n\n# Sample of size N from a population of covariates\n# Each patient's treatment assignment is a Bernoulli(1/2) random variable\n# Then generate a random outcome from the model using the covariate value \n# and treatment for all N patients\n\na <- -3\nb <- 5\nc <- 2\nd  <- 5\n\nvar.eps0 <- var.eps1 <-  .6\n\np <- 1 / 2\nN <- 10000\nnsamp <- 10000\n\np1.estimates <- foreach(i=1:nsamp,.combine = rbind) %dopar% {\n  X.sim <- runif(N,0,1) # sample covariate\n  X.sim.center <- X.sim - mean(X.sim) # center covariate\n\n  # outcome under control\n  y0.sim  <-  a*X.sim + b + rnorm(N,0,sqrt(var.eps0))\n\n  # outcome under treatment\n  y1.sim  <-   c*X.sim + d + rnorm(N,0,sqrt(var.eps1))\n\n  A.sim <- rbinom(N,1,p)\n\n  Y.sim <- y1.sim*A.sim + y0.sim*(1-A.sim)\n\n  df.sim <- data.frame(Y.sim, A.sim, X.sim, X.sim.center) # A=0,1,2,...treatment arm factor\n\n  ## Additive Model\n\n  lm.fit <- lm(Y.sim ~ A.sim + X.sim, data = df.sim)\n\n  lm.se <- summary(lm.fit)[[4]][2,2]\n\n  # set treatment indicator to active treatment for all subjects\n  pred1 <- predict(lm.fit, newdata = df.sim %>% mutate(A.sim=1))\n\n  # set treatment indicator to control for all subjects\n  pred0 <- predict(lm.fit, newdata = df.sim %>% mutate(A.sim=0)) \n\n  ate.add <- mean(pred1) - mean(pred0)\n\n  ate.add.se <- sqrt(mean((A.sim*(Y.sim-pred1)/mean(A.sim==1) + pred1 -\n                mean(pred1)-((1-A.sim)*(Y.sim-pred0)/mean(A.sim==0) + \n                               pred0 - mean(pred0)))^2))/sqrt(N)\n\n  ## Interaction Model\n\n  lm.fit <- lm(Y.sim ~ A.sim*X.sim, data = df.sim)\n\n  # set treatment indicator to active treatment for all subjects\n  pred1 <- predict(lm.fit, newdata = df.sim %>% mutate(A.sim=1))\n\n  # set treatment indicator to control for all subjects\n  pred0 <- predict(lm.fit, newdata = df.sim %>% mutate(A.sim=0))\n\n  ate.int <- mean(pred1) - mean(pred0)\n\n  ate.int.se <- sqrt(mean((A.sim*(Y.sim-pred1)/mean(A.sim==1) + pred1 - \n                             mean(pred1)-((1-A.sim)*(Y.sim-pred0)/mean(A.sim==0) + \n                                            pred0 - mean(pred0)))^2))/sqrt(N)\n\n  ## HW SEs\n\n  lm.fit <- lm(Y.sim ~ A.sim + X.sim, data = df.sim)\n  hw.add <- sqrt(vcovHC(lm.fit, type=\"HC0\")[2,2])\n\n  lm.fit <- lm(Y.sim ~ A.sim*X.sim.center, data = df.sim)\n  hw.int <- sqrt(vcovHC(lm.fit, type=\"HC0\")[2,2]) \n\n  lm.fit <- lm(Y.sim ~ A.sim*X.sim, data = df.sim)\n  ate <- coefficients(lm.fit)[2] + coefficients(lm.fit)[4]*mean(df.sim$X.sim)\n  myvcov <- vcovHC(lm.fit, type=\"HC0\")\n  ate.sd <- sqrt(mean(df.sim$X.sim)^2*myvcov[4,4]+myvcov[2,2] + 2*mean(df.sim$X.sim)*myvcov[2,4])\n\n\n  return(c(ate.add, ate.int, ate.add.se, ate.int.se, hw.add, hw.int, lm.se))\n}\n\np1.se.truth <- sqrt(apply(p1.estimates[,1:2],2,var))\n\ntmle.mean.se <- apply(p1.estimates[,3:4],2,mean)\n\nhw.mean.se <- apply(p1.estimates[,5:6],2,mean)\n\nt <- t(data.frame(truth = p1.se.truth, tmle = tmle.mean.se, HW = hw.mean.se))\n\nrownames(t) <- c(\"True SE\",\"Mean TMLE SE\",\"Mean HW SE\")\n\ncolnames(t) <- c(\"Additive Working Regression Model\",\"Interaction Working Regression Model\")\n\nknitr::kable(t, align = \"cc\", caption = \"Superpopulation Sampling Framework\")\n\n\nSuperpopulation Sampling Framework\n\n\n\n\n\n\n\n\nAdditive Working Regression Model\nInteraction Working Regression Model\n\n\n\n\nTrue SE\n0.0213702\n0.0213635\n\n\nMean TMLE SE\n0.0211727\n0.0211716\n\n\nMean HW SE\n0.0211753\n0.0154888"
  },
  {
    "objectID": "posts/additive_model/index.html",
    "href": "posts/additive_model/index.html",
    "title": "The Unreasonable Effectiveness of Additive Models",
    "section": "",
    "text": "Preface\nAdditive modeling algorithms are an essential part of the machine learning modeler’s tool kit when working with tabular data. Such predictive models have excellent intrinsic interpretability and often have optimal or near optimal performance. While linear additive models are the most common type of additive models, and they often have optimal performance, there are also many cases where additive functions need more flexibility.\nTo address this need we have developed the Model Sculpting method for building additive models. This is a model building pipeline that starts from developing a strong learner, such as boosted trees, and then extracts an additive model that best approximates the strong learner, thereby providing a more interpretable model with limited cost in performance.\nThe slide deck below provides an overview of the Model Sculpting method and its applications. The modsculpt R package provides an implementation of the method and the example workflow demonstrates how to use the package to build an additive model.\n\n\nContents\n\nSlide deck\nmodsculpt R package\nExample workflow for model sculpting\n\n\n\n\n\nThe Unreasonable Effectiveness of Additive Models\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{friesenhahn2024,\n  author = {Michel Friesenhahn and Ondrej Slama and Kenta Yoshida},\n  title = {The {Unreasonable} {Effectiveness} of {Additive} {Models}},\n  date = {2024-09-20},\n  url = {https://go.roche.com/stats4datascience},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMichel Friesenhahn, Ondrej Slama, and Kenta Yoshida. 2024. “The\nUnreasonable Effectiveness of Additive Models.” https://go.roche.com/stats4datascience."
  },
  {
    "objectID": "posts/three_metrics/index.html",
    "href": "posts/three_metrics/index.html",
    "title": "Everything you wanted to know about R2 but were afraid to ask",
    "section": "",
    "text": "\\[\\\\[.02in]\\]\nThere are two steps to state-of-the-art clinical prediction model performance evaluation. The first step, often referred to as overall model performance evaluation, involves interrogating the general quality of predictions in terms of how close they are to observed outcomes. Overall model performance is assessed via three general concepts that are widely applicable: accuracy, discrimination and calibration (Steyerberg et al., 2010, 2014).\nThere is some confusion around the relationship between accuracy, discrimination and calibration and what the best approach is to evaluate these concepts. This post aims to provide some clarity via a remarkable decomposition of \\(R^2\\) that beautifully unifies these three concepts. While similar decompositions have existed for decades in meteorological forecasting (Murphy, 1973), we wish to call greater attention to this underutilized evaluation tool and provide additional insights based on our own scaled version of the decomposition. Part 1 of this post provides details around which existing and newly proposed performance metrics to use for an overall assessment of prediction model quality. We will reveal how the suggested metrics are affected by transformations of the predicted values, explain the difference between \\(R^2\\) and \\(r^2\\), demonstrate that \\(R^2\\) can indeed be negative, and give practical guidance on how to compare the metrics to gain information about model performance. Part 2 of the post explains how the metrics and decompositions discussed here are applicable to binary outcomes as well and can also be generalized beyond the familiar squared error loss.\nWhile accuracy, calibration, and discrimination provide important insights into the overall quality of the predictions, they are not direct metrics of the model’s fitness for use in a particular application. Therefore, for a complete performance evaluation, the second step is to evaluate utility using tailored metrics and visualizations. For example, if a model’s predictions are intended to be used as a covariate for a trial’s adjusted primary endpoint analysis, the effective sample size increase metric evaluates the expected precision gains from the adjustment (Schiffman et al., 2023). Another example is net benefit decision curves, an elegant decision-analytic metric for assessing the utility in clinical practice of prognostic and diagnostic prediction models (Calster et al., 2018; Vickers et al., 2006). The focus of this post, however, will be only on the first step of general model performance evaluation, and further discussion around assessing a model’s clinical utility is beyond its scope.\nThis blog mostly focuses on defining and characterizing population level performance metrics. However, how to estimate metrics of these performance characteristics is covered in a later section."
  },
  {
    "objectID": "posts/three_metrics/index.html#example",
    "href": "posts/three_metrics/index.html#example",
    "title": "Everything you wanted to know about R2 but were afraid to ask",
    "section": "Example",
    "text": "Example\nGeographic atrophy (GA) is an advanced form of age-related macular degeneration (AMD) that leads to vision loss. GA progression can be assessed by the change in GA lesion area (\\(mm^2\\)) over time using Fundus Autofluorescence (FAF) images. Salvi et al. use data from several clinical trials and observational studies to develop deep learning (DL) models that can predict the future region of growth (ROG) of GA lesions at l-year using FAF images (Salvi et al., 2023).\nTo develop and evaluate the models, the data were split into a development set (\\(n=388\\)) and test set (\\(n=209\\)). The development set was further split into a training (\\(n=310\\)) and validation (\\(n=78\\)) set. Models were built using the training set, selected using the validation set and evaluated using the test set. See Salvi et al. for further details around the development of the various DL models.\n\n\n\nTo demonstrate how to use the previously discussed metrics to evaluate model performance, we estimate the metrics for one of the DL models from this publication (referred to as model #5 multiclass whole lesion in the manuscript), before and after recalibration based on the test set (Salvi et al., 2023).\nTable 2 shows the metric estimates for the model in the test set. The first row of Table 2 shows the metric estimates prior to recalibration. The model is imperfectly calibrated, demonstrated visually with the calibration plot in Figure 7 and with the relatively high estimate of the miscalibration index (\\(\\widehat{\\mathrm{MI}}=0.338\\)). However, the estimated discrimination index is relatively high, \\(\\widehat{\\mathrm{DI}}=0.678\\). The fundamental decomposition tells us that \\(\\widehat{\\mathrm{R^2}}\\) could reach this value if the model were calibrated. The shape of the calibration curve and the low estimate of the nonlinearity index (\\(\\widehat{\\mathrm{NI}}=0.030\\)) indicate that linear recalibration would go a long way in improving the model’s accuracy\n\n\n\n\nFigure 7: Predicted vs observed outcomes in the test set for the whole lesion model from Salvi et al.\n\n\n\n\n\nTable 2: Performance metrics estimated on the test set based on original, linearly recalibrated and recalibrated results\n\n\n\n\n\n\n\n\n\n\nPredictor\n\\(\\widehat{\\mathrm{R^2}}\\)\n\\(\\widehat{\\mathrm{r^2}}\\)\n\\(\\widehat{\\mathrm{DI}}\\)\n\\(\\widehat{\\mathrm{MI}}\\)\n\\(\\widehat{\\mathrm{NI}}\\)\n\n\n\n\n\\(\\hat{Y}\\)\n\\(0.338\\)\n\\(0.648\\)\n\\(0.678\\)\n\\(0.338\\)\n\\(0.030\\)\n\n\n\\(L(\\hat{Y})\\)\n\\(0.648\\)\n\\(0.648\\)\n\\(0.678\\)\n\\(0.030\\)\n\\(0.030\\)\n\n\n\\(C(\\hat{Y})\\)\n\\(0.678\\)\n\\(0.678\\)\n\\(0.678\\)\n\\(0.000\\)\n\\(0.000\\)\n\n\n\n\nThe second row of Table 2 shows the metric estimates after linearly recalibrating the model. The miscalibration index decreases to \\(\\widehat{\\mathrm{MI}}=0.030\\) and \\(\\widehat{\\mathrm{R^2}}\\) increases to \\(\\widehat{\\mathrm{R^2}}=\\widehat{\\mathrm{r^2}}=0.648\\), as expected. Since recalibration does not affect discrimination, \\(\\widehat{\\mathrm{DI}}\\) remains the same. Finally, the bottom row of Table 2 shows the metric estimates after recalibrating the model with the estimated calibration curve (the red gam curve in Figure 7). As expected, after transforming the predictions via \\(C(\\hat{Y})\\), \\(\\widehat{\\mathrm{MI}}=0\\) and \\(\\widehat{\\mathrm{R^2}}\\) has increased to \\(\\widehat{\\mathrm{DI}}\\)."
  },
  {
    "objectID": "posts/three_metrics/index.html#appendix-a",
    "href": "posts/three_metrics/index.html#appendix-a",
    "title": "Everything you wanted to know about R2 but were afraid to ask",
    "section": "Appendix A",
    "text": "Appendix A\n\nA Note on the Intercept and Slope of \\(L(\\hat{Y})\\)\nCalibration is often assessed by reporting the intercept and slope of the calibration line \\(L(\\hat{Y})\\) (Calster et al., 2016; Stevensa et al., 2020; Steyerberg et al., 2014). An intercept of 0 and slope of 1 correspond to perfect calibration if the calibration curve is linear. The intercept of \\(L(\\hat{Y})\\) is generally interpreted as a measure of “calibration in the large”, i.e. how different the mean outcome is from the mean prediction. The slope of \\(L(\\hat{Y})\\) can be interpreted as a measure of overfitting (slope < 1) or underfitting (slope > 1) during model development and when performing internal validation.\nHowever, as metrics for miscalibration, the intercept and slope of \\(L(\\hat{Y})\\) have limitations. If the calibration function is nonlinear, calibration performance may not be well assessed by the calibration line. The \\(\\mathrm{MI}\\) metric discussed above, however, is appropriate for any shape the calibration function may have. The presence of nonlinear miscalibration can be assessed via a Nonlinearity Index \\(NI = DI - r^2\\). \\(NI >= 0\\) by the fundamental inequality and \\(NI=0\\) if and only if the calibration curve is linear. When \\(NI>0\\), it indicates how much recalibration via the calibration curve improves accuracy over recalibration via the calibration line.\nFurthermore, since there are two parameters, model comparisons for miscalibration using the intercept and slope of \\(L(\\hat{Y})\\) are only partially ordered since they cannot be compared based on these parameters if one has a better intercept and a worse slope than the other. \\(MI\\), on the other hand, is a single metric that enables comparison of any two models.\nEven if the calibration curve is linear and the prediction model is calibrated in the large, there is an additional issue: the calibration intercept and slope do not take into account the distribution of the predicted outcomes. Unless the slope of the calibration line is 1, the discrepancies between the predicted outcomes and the calibration line will depend on the predicted outcomes and the distribution of discrepancies will depend on the distribution of predictions. \\(MI\\) accounts for this since it captures the expected squared discrepancies over the distribution of predictions, but the intercept and slope of \\(L(\\hat{Y})\\) do not.\n\n\n\nFor these reasons, we prefer to do the following to gain a comprehensive understanding of miscalibration:\n\nFocus on \\(R^2\\), \\(r^2\\), \\(\\mathrm{DI}\\), \\(\\mathrm{MI}\\) and \\(NI\\)\nIf the evaluation data set is too small to estimate a flexible calibration curve, estimate a calibration line and calculate the above metrics using that line as the calibration curve. Since \\(r^2 = DI\\) and \\(NI = 0\\) in this case, only a subset of the above metrics needs to be reported. Note, \\(MI\\) should still be reported even if recalibration was done with a linear function.\nIf \\(NI \\approx 0\\), the intercept and slope of \\(L(\\hat{Y})\\) may also be reported for the purposes of describing or approximating the calibration curve."
  },
  {
    "objectID": "posts/three_metrics/index.html#appendix-b",
    "href": "posts/three_metrics/index.html#appendix-b",
    "title": "Everything you wanted to know about R2 but were afraid to ask",
    "section": "Appendix B",
    "text": "Appendix B\n\nProof that predictions transformed via the calibration curve are calibrated\nIf a model is miscalibrated, its predictions can be transformed via the calibration curve \\(C_{Y,\\hat{Y}}\\). To show this, note that the calibration curve for the new predictions \\(C_{Y,\\hat{Y}}(\\hat{Y})\\) is the identity function:\n\\[\n\\begin{align}\n  \\mathrm{C}_{Y,\\mathrm{C}_{Y,\\hat{Y}}(\\hat{Y})}(\\mathrm{C}_{Y,\\hat{Y}}(\\hat{Y})) &= \\mathbb{E}\\big[Y \\mid \\mathrm{C}_{Y,\\hat{Y}}(\\hat{Y})\\big] \\notag \\\\\n  &= \\mathbb{E}\\big[Y \\mid \\mathbb{E}[Y \\mid \\hat{Y}]\\big] \\notag \\\\\n  &= \\mathbb{E}\\big[\\mathbb{E}\\big[Y \\mid \\hat{Y},\\mathbb{E}[Y \\mid \\hat{Y}]\\big] \\mid \\mathbb{E}[Y \\mid \\hat{Y}]\\big] \\notag \\\\\n  &= \\mathbb{E}\\big[\\mathbb{E}[Y \\mid \\hat{Y}] \\mid \\mathbb{E}[Y \\mid \\hat{Y}]\\big] \\notag \\\\\n  &= \\mathbb{E}[Y \\mid \\hat{Y}] \\notag \\\\\n  &= \\mathrm{C}_{Y,\\hat{Y}}(\\hat{Y})\n\\end{align}\n\\]\nAnd therefore \\(\\mathrm{MI}=0\\) after transforming the original predictions with \\(C_{Y,\\hat{Y}}\\):\n\\[\n\\begin{align}\n  \\mathrm{MI} &=  \\frac{\\mathbb{E}\\Big[\\Big(\\mathrm{C}_{Y,\\mathrm{C}_{Y,\\hat{Y}}(\\hat{Y})}(\\mathrm{C}_{Y,\\hat{Y}}(\\hat{Y})) - \\mathrm{C}_{Y,\\hat{Y}}(\\hat{Y})\\Big)^2\\big]}{Var(Y)} \\\\\n  &= \\frac{\\mathbb{E}\\Big[\\Big(\\mathrm{C}_{Y,\\hat{Y}}(\\hat{Y}) - \\mathrm{C}_{Y,\\hat{Y}}(\\hat{Y})\\Big)^2\\big]}{Var(Y)} \\notag \\\\\n  &= 0\n\\end{align}\n\\tag{9}\\]\n\n\nProof that \\(0\\leq\\mathrm{DI}\\leq1\\)\nIn general, the \\(\\mathrm{DI}\\) is a non-negative number that can be at most \\(1\\) since\n\\[\n\\begin{align}\n  \\mathrm{DI} &=  \\frac{Var\\Big( \\mathrm{C}_{Y,\\hat{Y}}(\\hat{Y})\\Big)}{Var(Y)} \\\\\n  &= \\frac{Var(\\mathbb{E}[Y|\\hat{Y}])}{Var(Y)} \\\\\n  &= \\frac{Vary(Y) - \\mathbb{E}(Var[Y|\\hat{Y}])}{Var(Y)} \\\\\n  &\\leq 1\n\\end{align}\n\\tag{10}\\]\n\n\nProof that transformations do not affect \\(\\mathrm{DI}\\)\n\\[\n\\begin{align}\n  DI \\space of \\space calibrated \\space predictions &= \\frac{Var\\big(\\mathrm{C}_{Y,\\mathrm{C}_{Y,\\hat{Y}}(\\hat{Y})}(\\mathrm{C}_{Y,\\hat{Y}}(\\hat{Y}))\\big)}{Var(Y)} \\notag \\\\\n  &= \\frac{Var\\big(\\mathrm{C}_{Y,\\hat{Y}}(\\hat{Y})\\big)}{Var(Y)} \\notag \\\\\n  &= DI \\space of \\space original \\space predictions\n\\end{align}\n\\tag{11}\\]\n\n\nProof of the \\(\\mathrm{MSE}\\) decomposition\n\\[\n\\begin{align}\n  \\mathrm{MSE} &= \\mathbb{E}\\big[(Y-\\hat{Y})^2\\big] \\\\\n&= \\mathbb{E}\\big[\\mathbb{E}[(Y-\\hat{Y})^2 \\mid \\hat{Y}]\\big] \\\\\n&= \\mathbb{E}\\big[\\mathbb{E}[(Y-\\mathbb{E}[Y \\mid \\hat{Y}] +\\mathbb{E}[Y \\mid \\hat{Y}] - \\hat{Y})^2 \\mid \\hat{Y}]\\big] \\\\\n&= \\mathbb{E}\\big[\\mathbb{E}[(Y-\\mathbb{E}[Y \\mid \\hat{Y}])^2 \\mid \\hat{Y}]\\big] +\\mathbb{E}\\big[\\mathbb{E}[(\\mathbb{E}[Y \\mid \\hat{Y}] - \\hat{Y})^2 \\mid \\hat{Y}]\\big] \\\\\n&= \\mathbb{E}\\big[Var(Y \\mid \\hat{Y})\\big] +\\mathbb{E}\\big[(\\mathbb{E}[Y \\mid \\hat{Y}] - \\hat{Y})^2 \\big] \\\\\n&= Var(Y) - Var(\\mathbb{E}[Y \\mid \\hat{Y}]) + \\mathbb{E}\\big[(\\mathbb{E}[Y \\mid \\hat{Y}] - \\hat{Y})^2 \\big] \\\\\n&= Var(Y) - Var(\\mathrm{C}_{Y,\\hat{Y}}(\\hat{Y})) + \\mathbb{E}\\big[(\\mathrm{C}_{Y,\\hat{Y}}(\\hat{Y}) - \\hat{Y})^2 \\big] \\\\\n\\end{align}\n\\]\n\n\nProof of the fundamental decomposition of \\(R^2\\)\n\\[\n\\begin{align}\n  R^2& = 1-\\frac{\\mathrm{MSE}}{Var(Y)} \\\\\n& = 1-\\frac{\\mathrm{Var(Y) - Var(\\mathrm{C}_{Y,\\hat{Y}}(\\hat{Y})) + \\mathbb{E}\\big[(\\mathrm{C}_{Y,\\hat{Y}}(\\hat{Y}) - \\hat{Y})^2 \\big]}}{Var(Y)} \\\\\n&= \\frac{Var(\\mathrm{C}_{Y,\\hat{Y}}(\\hat{Y}))}{Var(Y)} -  \\frac{\\mathbb{E}\\big[(\\mathrm{C}_{Y,\\hat{Y}}(\\hat{Y}) - \\hat{Y})^2 \\big]}{Var(Y)} \\\\\n&= DI - MI\n\\end{align}\n\\tag{12}\\]\n\n\nProof that \\(\\mathrm{C}_{Y,\\hat{Y}}(\\hat{Y})\\) maximizes \\(R^2\\)\n\n\n\n\n\n\nThe calibration curve is the transformation that maximizes \\(R^2\\)\n\n\n\n\\[\n\\begin{align}\nC(\\hat{Y}) &= argmax_{h \\in H} \\space 1-\\frac{\\mathrm{MSE}}{Var(Y)} \\\\\n&=argmin_{h \\in H} \\space \\mathbb{E}\\big[\\big(Y-h(\\hat{Y})\\big)^2\\big]\n\\end{align}\n\\tag{13}\\]\n\n\n\\[\n\\begin{align}\nargmax_{h \\in H} \\space 1-\\frac{\\mathrm{MSE}}{Var(Y)} &=argmin_{h \\in H} \\space \\mathbb{E}\\big[\\big(Y-h(\\hat{Y})\\big)^2\\big] \\\\\n&= argmin_{h \\in H} \\space \\mathbb{E}\\big[\\big(Y-\\mathbb{E}[Y|\\hat{Y}]+\\mathbb{E}[Y|\\hat{Y}]-h(\\hat{Y})\\big)^2\\big] \\\\\n&= argmin_{h \\in H} \\space \\mathbb{E}\\big[\\big(Y-\\mathbb{E}[Y|\\hat{Y}]\\big)^2 + \\\\  &2\\big(Y-\\mathbb{E}[Y|\\hat{Y}]\\big)\\big(\\mathbb{E}[Y|\\hat{Y}]-h(\\hat{Y})\\big) + \\big(\\mathbb{E}[Y|\\hat{Y}]-h(\\hat{Y})\\big)^2\\big] \\\\\n&= argmin_{h \\in H} \\space \\mathbb{E}\\big[\\big(\\mathbb{E}[Y|\\hat{Y}]-h(\\hat{Y})\\big)^2\\big] \\\\\n&= \\mathbb{E}[Y|\\hat{Y}]\n\\end{align}\n\\]\n\n\nProof that \\(R^2=r^2\\) when predictions are transformed via the calibration line \\(L(\\hat{Y})\\)\n\\[\n\\begin{align}\nR^2 \\space for \\space L(\\hat{Y}) &= 1-\\frac{\\mathbb{E}\\big[\\big(Y-L(\\hat{Y})\\big)^2\\big]}{Var(Y)} \\\\\n&= \\frac{Var(Y) - \\mathbb{E}\\big[\\big(Y-\\mathbb{E}(Y)+\\frac{Cov(Y,\\hat{Y})}{Var(\\hat{Y})}\\mathbb{E}(\\hat{Y})-\\frac{Cov(Y,\\hat{Y})}{Var(\\hat{Y})}\\hat{Y}\\big)^2\\big]}{Var(Y)} \\\\\n&= \\frac{Var(Y) - \\mathbb{E}\\big[Var(Y) -2\\frac{Cov(Y,\\hat{Y})}{Var(\\hat{Y})}Cov(Y,\\hat{Y})+\\frac{Cov^2(Y,\\hat{Y})}{Var^2(\\hat{Y})}Var(\\hat{Y})\\big]}{Var(Y)} \\\\\n&= \\frac{Cov^2(Y,\\hat{Y})}{Var(\\hat{Y})Var(Y)} \\\\\n&= cor^2(Y, \\hat{Y}) \\\\\n\\end{align}\n\\]\n\n\nProof that \\(L(\\hat{Y})\\) is the population least squares regression line\nLet \\(Y\\) be an \\(n\\times1\\) outcome vector and \\(X\\) be an \\(n\\times2\\) design matrix with an intercept, and suppose you want to approximate \\(E(Y|X)\\) with a simple linear function of \\(X\\):\n\\[E(Y|X) \\approx Xb\\]\nThe coefficients that minimize the expected squared error of the approximation are called the population least squares regression coefficients \\(\\beta\\):\n\\[ \\beta = argmin_{b \\in \\mathbb{R}^2}\\mathbb{E}\\big(E(Y|X)-Xb\\big)^2\\]\nTaking the derivative w.r.t \\(b\\) and setting equal to zero:\n\\[\n\\begin{align}\n\\mathbb{E}\\big(-2X'E(Y|X) + 2(X'X)\\beta \\big)=0 \\\\\n\\beta=\\mathbb{E}(X'X)^{-1}\\mathbb{E}(X'Y) \\\\ =\\bigg(\\mathbb{E}(Y)-\\frac{Cov(Y,\\hat{Y})}{Var(\\hat{Y})}\\mathbb{E}(\\hat{Y}),\\frac{Cov(Y,\\hat{Y})}{Var(\\hat{Y})}\\bigg)' \\\\\n=(\\alpha_{opt},\\beta_{opt})' \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "posts/Prescreener/index.html#sec-MonaLisa",
    "href": "posts/Prescreener/index.html#sec-MonaLisa",
    "title": "Optimizing Pre-screening Tools for Clinical Trials",
    "section": "A picture worth more than a thousand words",
    "text": "A picture worth more than a thousand words\nIn the previous section, we have treated the pre-screening tool as if it was binary (\\(prescreener-\\) and \\(prescreener+\\)). Although ultimately the decision to further screen a patient is a binary decision, the starting point, i.e. the \\(prescreener\\) output, in most cases is not binary. In our Alzheimer example, the blood test we are looking at is a continuous variable (the ratio of plasma concentrations of Ab42/Ab40). That means, we have a range of possible decision thresholds and need to evaluate the trade-off between the screen-out rate and NPV (or alternatively screen-in and PPV).\nLet’s walk through this with our Alzheimer’s Disease example, where we want to evaluate if the biomarker Ab42/Ab40 has utility as a pre-screening tool for determining the amyloid status (PET), which is our confirmatory gold standard screening tool.\nStep 1: Convert the input variable \\(x\\) (here Ab42/Ab40) into the probability \\(P(E+|x = x_{c})\\). In this case \\(E+\\) is amyloid positive by PET: \\(P(PET+|Ab42/Ab40)\\). For brevity, we will refer to this probability as the “risk” in the following figures. If your input variable is already a calibrated probability, e.g. an output coming from a prediction model, this step is skipped.\n\n\n\nFigure 1: Probability of being PET+ depending on the plasma Ab42/Ab40 concentration. Note that following convention the x-axis was reversed since for this biomarker, lower levels are indicative of disease. Red: PET+, Blue: PET-”\n\n\nLooking at this figure, we first want to ensure that the risk \\(P(PET+|Ab42/Ab40)\\) is meaningfully different from the population prevalence (horizontal line) in certain ranges of the biomarker - this tells us that the biomarker has information. Population prevalence in this case means \\(P(E+) = P(PET+)\\) and in our AD example, this prevalence is a low value of 15%, meaning that 85% would screenfail in an unselected (before per-screening) population. How can this potential pre-screener be used? Can it be used as a rule-in or a rule-out test for the amyloid positivity eligibility criteria?\nWe can see that for high Ab42/Ab40 levels (note x-axis is reversed!), the risk is extremely low, very close to zero. This indicates that for high Ab42/Ab40 levels we can safely rule-out amyloid pathology. However, we don’t reach very high risk estimates anywhere, so rule-in doesn’t seem feasible. Note that this is not just a matter of prevalence, meaning that the prevalence is generally low and hence easier to achieve low risk estimates, but also due the performance of this biomarker.\nOne piece is missing from this plot: does a sufficiently large subset of the population obtain these extremely low risk estimates? Consider this: If only 2% of patients were assigned risks close to zero, this marker still wouldn’t have much utility as a rule-out marker. To understand the utility of a biomarker for a population, we need to look at the distribution of risks. This also helps us to really compare performance between different biomarkers: we don’t just need to know how extreme risks can get but also for what proportion of patients these risks would occur. This leads us to step 2.\nStep 2: When evaluating a tool for its usefulness, it is important to understand its distribution of implied risks. How frequently to we obtain risks with actionable magnitude? A tool that returns more extreme risks (low or high) in a larger fraction of the intended use population will have more utility. A useful tool to evaluate the risk distribution was introduced by Pepe et al. (Pepe et al. 2008) - the predictiveness curve. So, now we plot the estimated risk \\(P(E+|x = x_{c})\\) versus the percentile of the risk (or equivalently x) of the population. This is equivalent to the inverse of the cumulative distribution function \\(F(x) = P(x \\leq x_{c})\\) of the risks, so the percentage negative at threshold \\(x_c\\).\n\n\n\nFigure 2: Predictiveness curve\n\n\nStep 3: If you have been wondering where and why we lost our key metrics for rule-in and rule-out pre-screening tools, NPV and PPV, let’s bring them back into the picture! Note that the risk \\(P(E+|x=x_{c})\\) is the probability at threshold \\(x_c\\) whereas PPV and 1-NPV are just the cumulative probabilities or average risks to the left or right of the threshold:\n\n\\(PPV = P(E+|x \\ge x_c)\\)\n\\(NPV = P(E-|x < x_c)\\) or \\(1-NPV = P(E+|x < x_c)\\)\n\nSince we are evaluating this biomarker as a pre-screening tool for a clinical trial, we are more interested in these cumulative risks since a cutoff would be applied and the cumulative risks would then describe the populations that are being screened-out (prescreener-) or send to further screening (prescreener+).\nBuilding on the predictiveness curve, we propose an integrated risk profile plot, which adds the cumulative risks versus the percentile of the tool and contains all information to gain a quick understand of the potential utility of a tool - A picture worth more than a thousand words. Note that we choose to plot 1-NPV here as the positive rate in patients that are called negative by the pre-screening tool.\n\n\n\nFigure 3: Integrated risk profile plot. Grey reference lines are introduced in the next section. below!\n\n\nNow we see that we can screen-out about 45% while keeping \\(1-NPV\\) really low (2%), which is quite promising for a rule-out pre-screening tool. The PPV of the population that would be send to further testing reaches 25% - this is an increase of 10% from the initial prevalence of 15% without a pre-screening tool. To further assess whether this performance is good enough, we use this plot as a starting point for all further evaluations which translate these metrics into other characteristics like cost and enrollment duration. However, if the integrated risk profile plot does not look promising, e.g. low risks are never achieved, you can stop your evaluations. If it does look promising, continue with additional metrics which are covered in [a later section][Derived trials screening operating characteristics]."
  },
  {
    "objectID": "posts/Prescreener/index.html#the-perfect-curve",
    "href": "posts/Prescreener/index.html#the-perfect-curve",
    "title": "Optimizing Pre-screening Tools for Clinical Trials",
    "section": "The perfect curve",
    "text": "The perfect curve\nIn Figure 3, we have added some informative reference lines that indicate what a perfect prescreener would look like. for PPV and 1-NPV that help us understand how far our tool is from being perfect! How would the integrated risk profile plot look like for a perfect tool? This is shwon in Figure 4. In case of a perfect tool (classifying all patients correctly), the risk would be 0 for any percentile below 1-prevalence and 1 for any percentile above. The steeper the risk curve is around the vertical 1-prevalence line, the better the tool. PPV must start at the prevalence as we are calling all patients positive at the lowest possible threshold (lowest percentile). For a perfect prescreener, the optimal threshold percentile is obviously 1-prevalence. Note that at any lower threshold \\(x_{low}\\), the \\(PPV(x_{low})\\) for a perfect marker would be \\(PPV(x_{low}) = P(E+)/(1-F(x_{low}))\\). This is the upper bound that any pre-screener can achieve in terms of PPV at a certain screen-out rate. Similarly, 1-NPV must also end at the prevalence when we are calling all patients negative at the highest percentile and is bounded for any threshold larger than 1-prevalence.\n\n\n\nFigure 4: The risk profile plot of a perfect tool (here assuming 30% prevalence)"
  },
  {
    "objectID": "posts/Prescreener/index.html#further-notes-on-interpretation",
    "href": "posts/Prescreener/index.html#further-notes-on-interpretation",
    "title": "Optimizing Pre-screening Tools for Clinical Trials",
    "section": "Further notes on interpretation",
    "text": "Further notes on interpretation\nA high NPV (or PPV) is the first requirement for a prescreening tool to ensure that our clinical trial population is very similar to the clinical trial population that would have been enrolled without the prescreener. Let’s assume the prescreener is used for rule-out. If compromising the NPV, that means we are losing some true positive patients which would have been enrolled without the prescreener. This might not always be a problem, but can easily lead to a bias if our prescreening tool leads to an unintended enrichment. This can be evaluated by comparing the population that would be enrolled with or without the prescreener in the training data with respect to important variables. Factors like age or disease severity can easily interfere with the intended use population as they are also often prognostic for other diagnostic criteria. So, when designing a prescreener, we also need to be careful what factors to include in the tool. And if we decide to compromise the NPV, it should be evaluated if that leads to any enrichment according to important variables.\nWhen interpreting the integrated risk profile plot, there are two very important assumptions that we have not discussed yet:\n\nRepresentativeness: The evaluation should be done in the intended use population, e.g. similar to the screening population for our planned clinical trial. Otherwise we can not rely on our risk estimates.\nCalibration: The ability to interpret the risk and the predictiveness curve requires well calibrated risk estimates. This means that predicted probabilities (risks) match the observed probabilities, i.e. if we were to take a large group of patients with a predicted risk of 15%, the observed prevalence in that group of patients should be close to 15%.\n\nBoth aspects will be further addressed and discussed in a later section.\n\n\n\n\n\n\nThe integrated risk profile plot\n\n\n\nThe integrated risk profile plot builds on the predictiveness curve and gives us key insights into the properties of the tool under evaluation: Are the estimated probabilities extreme enough (low or high) for a sufficiently large proportion of the population so that a decision high certainty can be made in a subset of subjects?"
  },
  {
    "objectID": "posts/Prescreener/index.html#sec-N",
    "href": "posts/Prescreener/index.html#sec-N",
    "title": "Optimizing Pre-screening Tools for Clinical Trials",
    "section": "Number needed to screen",
    "text": "Number needed to screen\nIn our AD example, the total number needed to be screened rapidly increases when using a threshold that is too aggressive. Also, the reduction in PET scans starts to plateau. At about 45% screen-out rate, the %increase was less than 10% and deemed tolerable given the number of downstream procedures (e.g. PET) that would be saved at that threshold (about 40%).\n\n\n\nFigure 5: Increase in the total number that need to be screened and the decrease in PET scans that need to be performed\n\n\nTo derive these curves, consider the following. Without a prescreening tool, if we want to enroll \\(N_{enroll}\\) subjects into a trial, and our screenfailure rate is determined by the eligibility criteria \\(E\\). Then the total number we need to screen to reach our enrollment goal is \\[ N_{w/o}=\\frac{N_{enroll}}{P(E+)}\\] In this notation \\(N_{w/o}\\) refers to total number to screen in the setting without a prescreener.\nNow, if we have a prescreening tool, the number of patients that need to be screened for \\(E\\) when applying a potential cutoff \\(x_c\\) for the prescreener are determined by the \\(PPV(x_c)\\): \\[N_{E}(x_c) = \\frac{N_{enroll}}{PPV(x_c)}\\] Remember that \\(PPV(x_c) = P(E+|prescreener \\ge x_c)\\). However, the total number of subjects that we need to screen, starting with the prescreener can be larger (but never smaller) than \\(N_{w/o}\\): \\[N_{w} = \\frac{N_{E}(x_c)}{P(prescreener \\ge x_c)} = \\frac{N_{enroll}}{P(prescreener \\ge x_c) \\times PPV(x_c)}\\]\nLet’s illustrate this with the perfect pre-screening tool: A perfect tool, would correctly identify all \\(E+\\) and \\(E-\\). If the tool is continuous, it means that the perfect state happens at screenout threshold where \\(F(x_c) = 1-P(E+)\\), meaning that we would screen-out all \\(E-\\) so the “prescreening prevalence” is \\(P(prescreening \\ge x_c) = P(E+)\\). With that threshold, \\(PPV = 1\\) (and \\(1-NPV = 0)\\). With that, \\(N_{w}=N_{w/o}\\), so does not change compared to the situation without pre-screener but the number to screen with \\(E\\) reduces to it’s minimum to \\(N_{E} = N_{enroll}\\). In general, since \\(P(prescreener \\ge x_c) \\times PPV(x_c) \\leq P(E+)\\), it follows that \\(N_{w/o} \\leq N_w\\) (law of total probability).\nThe time to enroll the trial can be negatively impacted by a pre-screener, if \\(N_{w}\\) increases too much. This happens when the \\(NPV\\) is not high (too many true positives would be screened-out) - this will result in insufficient increase of the prevalence, indicated by a low \\(PPV\\) at that screen-out rate. Hence it is important to understand how easy it is to find patients who are willing to screen and how much of an increase in the total number to screen can be tolerated."
  },
  {
    "objectID": "posts/Prescreener/index.html#sec-cost",
    "href": "posts/Prescreener/index.html#sec-cost",
    "title": "Optimizing Pre-screening Tools for Clinical Trials",
    "section": "Screening costs",
    "text": "Screening costs\n\n\n\nFigure 6: Percentage change in cost using the pre-screening tool\n\n\nFigure 6 shows an example of a cost curve with specific cost assumptions for the AD example. Note that initially (very low screen-out rate), the cost can be higher since many subjects will then undergo an “additional” test with the pre-screener and both costs apply. This increase in cost can be larger if the pre-screening tool is not substantially cheaper than the gold standard tool! Also note that the cost curve reaches almost it’s minimum at around 50% screen-out and then flattens before increasing drastically when we enter a screen-out percentage that screens out all eligible subjects.\nThe total screening costs depend on how many patients go through each step of the screening funnel. In our simplified scenario, let’s assume the \\(c_{main}\\) = cost for each patient being screened with \\(E\\) (main screening). So, the total cost in the setting without pre-screener using the notation from the previous section is: \\[\ncost_{w/o} = c_{main} \\times N_{w/o}\n\\] Now let’s assume \\(c_{pre}\\) is the cost for the pre-screening tool per patient. The total cost for screening when implementing the prescreener is:\n\\[\ncost_{w} = c_{pre}\\times N_{w} + c_{main}\\times N_E\n\\] Note that all patients going on to main screening for \\(E\\) are first screened with the pre-screener, so both costs apply to them and they are a subset of \\(N_w\\).\nThis means that the cost is sufficiently smaller with pre-screener if \\(c_{pre}\\) is smaller than \\(c_{main}\\). The amount it needs to be smaller depends on how many we can screen-out successfully.\n\\[\ncost_w \\leq cost_{w/o} \\iff \\frac{c_{pre}}{c_{main}} \\leq P(prescreener \\ge x_c)\\times (\\frac{PPV(x_c)}{P(E+)}-1)\n\\]\nNote that depending on the ratio in cost between \\(c_{pre}\\) and \\(c_{main}\\), the cost can become higher at low screening thresholds since many subjects will then undergo an “additional” test with the prescreener and both costs apply.\nThe screening cost is not always the primary driver - remember time is money! So, both the number needed to screen and cost need to be taken into account."
  },
  {
    "objectID": "posts/Prescreener/index.html#sec-nonp",
    "href": "posts/Prescreener/index.html#sec-nonp",
    "title": "Optimizing Pre-screening Tools for Clinical Trials",
    "section": "Non-parametric estimate",
    "text": "Non-parametric estimate\nThe non-parametric estimate for NPV and PPV is a simple frequency estimate at each cutoff. Using Bayes’ rule, this can also be written as function of sensitivity and specificity, which can also be written in terms of distribution functions. This is the approach we are using here, since we can easily use this form to adjust the prevalence (see also this section):\n\\[PPV(x) = \\frac{sensitivity(x)\\times P(E+)}{(sensitivity(x)\\times P(E+) + (1-specificity(x))\\times(1-P(E+))}\\]\nwith \\(sensitivity(x) = 1 - F_{E+}(x)\\) and \\(specificity = F_{E-}(x)\\)\nSimilarily,\n\\[NPV(x) = \\frac{specificity(x)\\times (1-P(E+))}{(1-sensitivity(x))\\times P(E+) + specificity(x)\\times(1-P(E+))}\\]\nThe screen-out percentage is determined by the marker distribution. This can be directly estimated as the empirical distribution function \\(F(x)\\).\nNote that in addition to PPV and NPV, a non-parametric estimate of the risk can also be fitted with binning methods or pava which also implements a monotonicity constrain (more details in our R-package stats4phc (Slama et al. 2024))."
  },
  {
    "objectID": "posts/Prescreener/index.html#smoothed-estimate",
    "href": "posts/Prescreener/index.html#smoothed-estimate",
    "title": "Optimizing Pre-screening Tools for Clinical Trials",
    "section": "Smoothed estimate",
    "text": "Smoothed estimate\nThere are multiple approaches to obtain a smoothed estimates for the PPV and NPV. For example, by using a parametric model, e.g. fitting densities. Those often have issues with generating non-proper ROC curves (non convex) which also lead to non-monotone PPV and NPV estimates (hook effects). For example, this happens with the binormal model when variances are not equal. We chose a different approach which is more general and can avoid this effect: We fit a smoother for the outcome on the percentile F(X), optionally with shape constraints such as monotonicity. An alternative is to fit a smoother for the outcome on X and then transform X to F(X). The latter approach leads to a slightly non smoothed risk profile while the former approach consists of a smoothed estimate. The predictions from this model provide our risk estimate \\(P(E+|(F(x))\\) and we can derive the PPV and NPV by integrating the area below the risk curve (Sachs and Zhou 2013 ; Gu and Pepe 2009).\nNote that binary outcomes are often modeled using logistic regression. We have observed that this often leads to non-calibrated risk estimates for single markers as the shape does not seem flexible enough. This is why we prefer a more flexible version using the gam approach which allows us to tune the shape of the curve."
  },
  {
    "objectID": "posts/Prescreener/index.html#sec-prev",
    "href": "posts/Prescreener/index.html#sec-prev",
    "title": "Optimizing Pre-screening Tools for Clinical Trials",
    "section": "Prevalence adjustment for risk estimates",
    "text": "Prevalence adjustment for risk estimates\nThis follows from the following consideration: \\(f\\): density of \\(E+\\) \\(g\\): density of \\(E-\\)\n\\(h_0 = \\frac{\\rho_0\\times f}{\\rho_0 \\times f + (1 - \\rho_0) \\times g}\\)\n\\(\\frac{1}{h_0} = 1 + \\frac{1-\\rho_0}{\\rho_0} \\times \\frac{g}{f}\\)\n\\(\\frac{g}{f} = (\\frac{1}{h_0} - 1) \\times \\frac{1-\\rho_0}{\\rho_0}\\)\nnow we can replace \\(\\frac{g}{f}\\) in the previous equation and update with the new prevalence \\(\\rho\\):\n\\(\\frac{1}{h_1} = 1 + \\frac{1-\\rho}{\\rho} \\times (\\frac{1}{h_0} - 1) \\times \\frac{1-\\rho_0}{\\rho_0}\\)\n\\(h_1 = \\frac{1}{1 + \\frac{1-\\rho}{\\rho} \\times (\\frac{1}{h_0} - 1) \\times \\frac{1-\\rho_0}{\\rho_0}}\\)"
  },
  {
    "objectID": "posts/three_metrics_binary/index.html",
    "href": "posts/three_metrics_binary/index.html",
    "title": "Everything you wanted to know about R2 but were afraid to ask",
    "section": "",
    "text": "\\[\\\\[.02in]\\] Below is the second part of a two part post on model performance evaluation and the fundamental decomposition of \\(R^2\\). Part 1 of this post discussed how the general quality of predictions is assessed by accuracy, calibration, and discrimination. An approach was proposed that unifies these concepts via a scaled decomposition of accuracy into miscalibration and discrimination, \\(R^2 = \\mathrm{DI} - \\mathrm{MI}\\). In Part 1 it was discovered that the three key metrics are \\(R^2\\), \\(r^2\\), and \\(\\mathrm{DI}\\), that they satisfy a key inequality, \\(R^2 \\leq r^2 \\leq DI\\) and that \\(R^2\\) can indeed be negative!\nWe now discuss the binary outcome setting and how the fundamental decompostion can be generalized…\n\nBinary Outcomes\nWhen outcomes are binary, \\(Y\\in\\{0,1\\}\\), the metrics and decomposition discussed in Part 1 of this post that were based on quadratic loss still apply without change. Importantly, the interpretations of the metrics hold for binary outcomes as well:\n\n\n\n\n\n\nInterpretations of \\(R^2\\) and \\(DI\\) as Proportions of Explained Uncertainty\n\n\n\n\n\\(\\mathrm{DI}\\) is the proportion of variability in the observed outcomes explained by recalibrated predictions.\nIf predictions are calibrated, then \\(R^2 = \\mathrm{DI}=Var(C(\\hat{Y})) / Var(Y)= Var(\\hat{Y}) / Var(Y)\\), so \\(R^2\\) is the proportion of variability in the observed outcomes explained by the predictions.\nIf predictions are not calibrated, then \\(R^2\\) cannot be interpreted as the proportion of variability in the observed outcomes explained by the predictions.\n\\(R^2\\) is always the proportional reduction of \\(\\mathrm{MSE}\\) relative to the best constant prediction, \\(\\mathbb{E}(Y)\\).\n\nThese interpretations still apply when outcomes are binary and take values in {0, 1}!\n\n\nRecall that quadratic loss is a strictly proper scoring rule (defined in more detail below). Remarkably, it has been shown that the decomposition into miscalibration and discrimination applies more generally to all strictly proper scoring rules, not just quadratic loss, and still retains its general interpretation (Brocker, 2009). An alternative strictly proper scoring rule that is commonly used for binary outcomes is the log loss–sometimes referred to as cross entropy.\n\n\nScoring Rules\nA scoring rule takes a predicted outcome \\(\\hat{y}\\) and a realized outcome \\(y\\) and assigns a real valued score \\(s(\\hat{y}, y)\\). Scoring rules can have negative or positive orientation, where negative orientation refers to larger scores representing worse discrepancies between \\(\\hat{y}\\) and \\(y\\). We assume negative orientation so that scores represent a loss. The score, \\(S(\\hat{Y}, Y)\\) is defined as the expected value of the scoring rule, \\(\\mathbb{E}[s(\\hat{Y}, Y)]\\). For example, \\(\\mathrm{MSE}\\) is the score based on the quadratic loss scoring rule. In the binary outcome setting, \\(\\mathrm{MSE}\\) is often called the Brier Score, first proposed in 1950 for applications in meteorology (Brier, 1950).\nA scoring rule is proper if the score based on that scoring rule is optimized at \\(\\mathbb{E}[Y]\\). It is strictly proper if it is uniquely optimized there. In other words, scores that are based on strictly proper scoring rules are optimized when predictions are equal to the true expected values (probabilities in the case of binary outcomes), so strictly proper scoring rules incentivize accurate predictions.\nAny score that is based on a strictly proper scoring rule can be decomposed into calibration and discrimination components. To show this, we first define the following terms which are also shown mathematically in Table 1. This is simply meant to be a brief sketch of these concepts, and for more details see Brocker (2009):\n\nUncertainty: \\(S(\\mathbb{E}(Y),Y)\\). Uncertainty is the score evaluated at the constant prediction \\(\\hat{y}=\\mathbb{E}(Y)\\). In general, uncertainty measures the spread of the outcomes and does not depend on the prediction model. When the scoring rule is quadratic loss, uncertainty is the variance of \\(Y\\) which is simply \\(\\mathbb{E}(Y)(1-\\mathbb{E}(Y))=p(1-p)\\) for binary outcomes. When the scoring rule is log loss, uncertainty is the same as Shannon’s information entropy. Figure 1 shows uncertainty as a function of \\(\\mathbb{E}(Y)\\) for quadratic and log loss scoring rules. These functions are both unimodal and symmetric about \\(\\mathbb{E}(Y) = \\frac{1}{2}\\), with minimum of zero uncertainty at the two extremes of \\(\\mathbb{E}(Y)\\) equal to 0 or 1.\nMiscalibration: \\(S(\\hat{Y},Y) - S(C(\\hat{Y}),Y)\\). Miscalibration is the difference between scores evaluated at the original and recalibrated predictions. It is also often referred to as reliability. Using language from information geometry, miscalibration is the expected divergence between the predictions, \\(\\hat{Y}\\), and the recalibrated predictions, \\(C(\\hat{Y})\\). In information geometry, expected divergence is a type of statistical distance, so miscalibration measures the distance between the original and recalibrated predictions. For quadratic loss, the divergence is simply squared Euclidean distance; for log loss, the divergence is Kullback-Leibler divergence.\nDiscrimination: \\(S(E(Y),Y) - S(C(\\hat{Y}),Y)\\). Discrimination, also often referred to as resolution, measures the distance between the best constant prediction \\(\\mathbb{E}(Y)\\) and the recalibrated predictions. Therefore, discrimination is a measure of spread in the recalibrated predictions about \\(\\mathbb{E}(Y)\\). For quadratic loss, this measure of spread is simply the variance of the recalibrated predictions; for log loss, it is the Kullback-Leibler divergence between the constant prediction of \\(\\mathbb{E}(Y)\\) relative to the recalibrated predictions. Note that Uncertainty, \\(S(E(Y),Y)\\), which is the first term of the difference, is not affected by the prediction model. The second term of the difference, \\(S(C(\\hat{Y}),Y)\\), is minimized and thus discrimination is maximized when recalibrated predictions are at the extremes of 0 or 1. Intuitively this makes sense since we would want to say that a model with many calibrated predicted probabilities close to the extremes is able to discriminate well between observations.\n\n\n\n\n\n\nFigure 1: Uncertainty for quadratic and log loss scoring rules\n\n\n\n\n\n\n\nTable 1: Miscalibration, discrimination and uncertainty under general, squared error and log loss scoring rules\n\n\n\n\n\n\n\n\n\nGeneral\nQuadratic Loss\nLog Loss\n\n\n\n\nScoring rule\n\\(s(\\hat{y},y)\\)\n\\((\\hat{y}-y)^2\\)\n\\(-[y*log(\\hat{y})+(1-y)*log(1-\\hat{y})]\\)\n\n\nScore\n\\(\\mathbb{E}[s(\\hat{Y},Y)]\\)\n\\(\\mathbb{E}[(\\hat{Y}-Y)^2]\\)\n\\(-\\mathbb{E}[Y*log(\\hat{Y})+(1-Y)*log(1-\\hat{Y})]\\)\n\n\nUncertainty\n\\(S(\\mathbb{E}(Y),Y)\\)\n\\(\\mathbb{E}(Y)(1-\\mathbb{E}(Y))\\)\n\\(-[\\mathbb{E}(Y)*log(\\mathbb{E}(Y))+(1-\\mathbb{E}(Y))*log(1-\\mathbb{E}(Y))]\\)\n\n\nMiscalibration\n\\(S(\\hat{Y},Y)-S(C(\\hat{Y}),Y)\\)\n\\(\\mathbb{E}[(\\hat{Y}-C(\\hat{Y}))^2]\\)\n\\(-\\mathbb{E}\\Big[C(\\hat{Y})*log\\Big(\\frac{\\hat{Y}}{C(\\hat{Y})}\\Big)+(1-C(\\hat{Y}))*log\\Big(\\frac{1-\\hat{Y}}{1-C(\\hat{Y})}\\Big)\\Big]\\)\n\n\nDiscrimination\n\\(S(\\mathbb{E}(Y),Y)-S(C(\\hat{Y}),Y)\\)\n\\(Var(C(\\hat{Y}))\\)\n\\(-\\mathbb{E}[C(\\hat{Y})*log\\Big(\\frac{\\mathbb{E}(Y)}{C(\\hat{Y})}\\Big)+(1-C(\\hat{Y}))*log\\Big(\\frac{1-\\mathbb{E}(Y)}{1-C(\\hat{Y})}\\Big)]\\)\n\n\n\n\n\n\n\nGeneralized Fundamental Decomposition\nFor strictly proper scoring rules, the Fundamental Decomposition discussed in Part 1 of this post can then be generalized to:\n\n\n\n\n\n\nGeneralized Fundamental Decomposition\n\n\n\n\\[\n\\begin{align}\nR^2 = DI - MI\n\\end{align}\n\\tag{1}\\] where\n\\(R^2 = 1 - \\frac{S(\\hat{Y},Y)}{S(\\mathbb{E}(Y),Y)}\\)\n\\(DI=\\frac{S(E(Y),Y) - S(C(\\hat{Y}),Y)}{S(\\mathbb{E}(Y),Y)}\\)\n\\(MI=\\frac{S(\\hat{Y},Y) - S(C(\\hat{Y}),Y)}{S(\\mathbb{E}(Y),Y)}\\)\n\n\nNote that \\(R^2\\) is simply the score expressed as a skill score for the prediction model relative to the optimal null model (Fissler et al., 2022).\nWith the metrics and decomposition now defined under a general proper scoring rule, we re-emphasize that all properties described in the preceding sections still apply for log loss scoring. In addition, the interpretations of the metrics discussed previously in Part 1 of this post also apply with suitable modifications:\n\n\n\n\n\n\nInterpretations of \\(R^2\\) and \\(DI\\) for any proper scoring rule\n\n\n\n\n\\(\\mathrm{DI}\\) is the spread of the recalibrated predictions divided by the spread of the original outcomes. Therefore, it can be interpreted as the proportion of uncertainty explained by the recalibrated predictions\nIf predictions are calibrated, then \\(R^2 = \\mathrm{DI}\\), so \\(R^2\\) is the proportion of uncertainty explained by the predictions.\nIf predictions are not calibrated, then \\(R^2\\) cannot be interpreted as the proportion of uncertainty explained by the predictions.\n\\(R^2\\) is always the proportional reduction in score relative to the best constant prediction, \\(\\mathbb{E}(Y)\\).\n\n\n\nWe note that in practice these metrics can all be estimated using straightforward empirical estimators similar to those described for quadratic loss (see section on estimation discussed in Part 1 of this post).\n\n\nComments on log loss\nBecause of the connections to maximum likelihood estimation and information theory, some researchers prefer log loss to quadratic loss for binary outcomes. We note two issues with log loss scoring. The first is that log loss is not defined if \\(\\hat{y} = 0\\) and \\(y = 1\\), even in a large data set. So log loss should be restricted to the evaluation of models that will never make predictions that are exactly 0. The second issue is that metrics based on quadratic loss are arguably more interpretable or at least familiar to many researchers and data scientists. Both quadratic and log loss are frequently used for binary outcomes and we recommend either (better yet, both if possible) for performance evaluation.\nSince Brier scores are equivalent to MSE, the definition of a calibration line for quadratic loss scoring rules is the same as defined in the preceding sections. However, note that since the outcomes are binary, the calibration line is not guaranteed to have predictions between 0 and 1. Therefore we recommend being cautious about the use of calibration lines when using quadratic loss.\nRecall that the calibration line was previously defined as the linear transformation of the predictions that maximizes the quadratic loss-based \\(R^2\\). If log loss is used instead, the calibration line is then defined as the population logistic regression of outcome versus \\(\\hat{Y}\\) and in this case \\(logit(\\mathbb{E}(Y|\\hat{Y}))\\) would be some linear function of \\(\\hat{Y}\\). However, this is problematic, since if the model is calibrated then \\(logit(\\mathbb{E}(Y|\\hat{Y}))=logit(\\hat{Y})\\), which is not a linear function of \\(\\hat{Y}\\), let alone one with intercept 0 and slope 1. This issue is solved by performing a population logistic regression of outcome on \\(logit(\\hat{Y})\\) rather than on \\(\\hat{Y}\\), which is usually how an appropriate calibration line is defined for log loss.\nFigure 2 shows that when \\(\\hat{Y}\\) is logit transformed prior to fitting the logistic regression (i.e. both conditional outcome probabilities \\(C(\\hat{Y})\\) and \\(\\hat{Y}\\) are logit transformed), calibration lines will actually be lines. However, on the original probability scale calibration lines are actually curves (except when the predictions are perfectly calibrated).\n\n\n\n\n\n\n\n(a) Logit-logit calibration line\n\n\n\n\n\n\n\n(b) Corresponding calibration line on original scales\n\n\n\n\nFigure 2: Calibration lines for log loss\n\n\n\n\nComments on misclassification error\nFinally, we note that misclassification error is also a commonly used metric for probabilistic predictions of binary outcomes. It has been noted that this metric is a score based on a proper scoring rule (Dimitriadis et al., 2021): \\(I(\\hat{y}<1/2,y=1)+I(\\hat{y}>1/2,y=0)+ \\frac{1}{2} I(\\hat{y}=1/2)\\).\nThis scoring rule dichotomizes the probabilistic predictions via a threshold at 1/2 and the proportion of discrepancies with the outcome is the misclassification error metric. However it is not strictly proper since it only matters if the predicted probability is above or below the decision threshold. Misclassification error will not change if the probabilistic predictions are adjusted arbitrarily, as long as those adjustments do not cross the decision threshold. Consequently misclassification error is not uniquely optimized at the true probabilities. Since misclassification error is not a strictly proper scoring rule, a decomposition into calibration and discrimination cannot meaningfully be used for that scoring rule.\nMore importantly, in most applications it is the predicted probabilities that are of interest so that individual users of the model can make their own decisions on what actions to take. In such settings it is premature for the builder of the prediction model to use an arbitrary cut point, such as 1/2, as a decision threshold. An exception to this would be for prediction models that have nearly perfect accuracy for the binary outcomes and where costs of the few misclassifications are roughly the same for predicted events vs predicted non events. An example of this would be modern optical character recognition systems. However, most applications do not satisfy such prerequisites and we do not recommend the use of misclassification error as a performance metric. Frank Harrell’s insightful blog on statistical thinking has further discussion of this topic.\n\n\nPseudo \\(R^2\\)\nIn parallel with the development of metrics based on scoring rules, there has been considerable work to develop pseudo-\\(R^2\\)s that are applicable to binary outcomes (see MITTLBOCK et al. (1996) for a nice summary). We review some of them below and place them in context of what we have learned about quadratic and log loss-based metrics. Pseudo-\\(R^2\\) metrics are usually calculated in-sample. However, we will comment on the more relevant population performance when these are applied out of sample as we have done in the rest of the blog:\n\nSum of Squares \\(R^2\\) (\\(R^2_{SS}\\)): This is equivalent to the \\(R^2\\) based on quadratic loss.\nEntropy \\(R^2\\) (\\(R^2_E\\)): This is the same as the \\(R^2\\) based on the log loss scoring rule. It was proposed by Theil (1970) and is equivalent to McFadden’s \\(R^2\\) (McFadden, 1974; Theil, 1970).\nGini’s Concentration Measure \\(R^2\\) (\\(R^2_G\\)): This is the expected nominal variance of the conditional Bernoulli outcomes. If predictions are calibrated, this is the same as \\(R^2 = DI\\) for quadratic loss. If predictions are not calibrated there is no guarantee \\(R^2_G\\) represents anything meaningful.\nTjur’s Coefficient of Discrimination \\(R^2\\) (\\(R^2_T\\)) (Tjur, 2009): This is the mean of the predictions for all events minus the mean of the predictions for all non events: \\(\\mathbb{E}(\\hat{Y} | Y = 1) - \\mathbb{E}(\\hat{Y} | Y = 0)\\). We note it is also the same as discrimination slope (Pencina et al., 2008). Surprisingly, if the predictions are calibrated then this is the same as \\(R^2 = DI\\) for the quadratic loss scoring rule (Pepe et al., 2008). If predictions are not calibrated, there is no guarantee this represents anything meaningful. For example, all predictions can be multiplied by a small positive scalar, \\(k\\). These transformed scores should have the same discrimination as the original predictions, but \\(R^2_T\\) will be multiplied by that same factor, \\(k\\).\nCox and Snell \\(R^2\\) (\\(R^2_{CS}\\)): This is \\(1 - \\big(\\frac{\\mathbb{L}_{null}}{\\mathbb{L}_{model}}\\big)^{(2/n)}\\), also known as the Maximum Likelihood \\(R^2\\). If the normal theory based likelihood is used for regression, then this is equivalent to the usual in-sample OLS \\(R^2\\). It is therefore viewed as a generalization of \\(R^2\\) applicable to a variety of distributions. However, for binary outcomes, the upper limit of this pseudo-\\(R^2\\) is strictly less than 1, even with perfect predictions. It also doesn’t correspond to the skill score of a strictly proper scoring rule so the fundamental decomposition and interpretations do not apply to it.\nNagelkerke \\(R^2\\) (\\(R^2_N\\)): This rescales the Cox and Snell pseudo \\(R^2_{CS}\\) by its maximum value, \\(U = 1 - \\mathbb{L}_{null}^{(2/n)}\\). So \\(R^2_N = R^2_{CS} / U\\) now has a maximum value of 1 which is achieved when all predictions are perfect. However, it still has the disadvantage of the Cox and Snell pseudo-\\(R^2\\) that it does not correspond to the skill score of a strictly proper scoring rule.\n\nWe prefer the metrics suggested in this blog over both \\(R^2_{CS}\\) and \\(R^2_N\\) due to the greater interpretation provided by the Fundamental Decomposition and interpretation as Proportion of Explained Uncertainty, potentially after suitable recalibration. The other pseudo-\\(R^2\\)s, when viewed as out of sample population parameters represent either \\(R^2\\) or \\(DI\\), potentially after suitable recalibration, for either quadratic or log Loss, so they do not add anything to the metrics we have proposed. We note also that this is not an exhaustive list of pseudo-\\(R^2\\)s, and there are additional variations, for example, to account for over-fitting (Hemmert et al., 2018). Rather than addressing over-fitting by incorporating explicit adjustments into the metrics, we handle this by out of sample estimation and, in case the same data is being used to develop the predictive models, resampling based on out of sample estimates.\n\n\nSummary/Key Points\n\nThe general quality of predictions is assessed by accuracy, calibration, and discrimination. These three evaluation domains are often assessed completely independently of each other.\nWe propose an approach that unifies these concepts via a scaled decomposition of accuracy into miscalibration and discrimination, \\(R^2 = \\mathrm{DI} - \\mathrm{MI}\\). This decomposition is unitless and clarifies that \\(\\mathrm{DI}\\) is the accuracy when there is no miscalibration, equivalently when the predictions are re-calibrated using the calibration curve.\nInterestingly, \\(r^2\\) can be interpreted as the \\(R^2\\) for the best linear recalibration. That metric is also inherently of interest for some applications such as covariate adjustment.\nThe three key metrics are \\(R^2\\), \\(r^2\\), and \\(\\mathrm{DI}\\) and they satisfy a key inequality, \\(R^2 \\leq r^2 \\leq DI\\). The remaining metrics of \\(\\mathrm{MI}\\) and \\(\\mathrm{NI}\\) are derived from these.\nDiscrimination can never be improved via recalibration, but miscalibration and accuracy can. These metrics are very informative for assessing how much accuracy can be improved via linear recalibration and recalibration via the calibration curve.\nFor binary outcomes, both Brier Scores and Log Loss can be used to obtain meaningful performance metrics. For both, the decomposition and interpretations (with suitable modifications) all hold. In general, we recommend use of either score.\n\n\n\n\n\n\nReferences\n\nBrier, G. W. (1950). VERIFICATION OF FORECASTS EXPRESSED IN TERMS OF PROBABILITY. U. S. Weather Bureau, Monthly Weather Review.\n\n\nBrocker, J. (2009). Stable reliability diagrams for probabilistic classifiers. The Quarterly Journal of the Royal Meteorological Society.\n\n\nDimitriadis, T., Gneitingb, T., & Jordan, A. I. (2021). Stable reliability diagrams for probabilistic classifiers. PNAS.\n\n\nFissler, T., Lorentzen, C., & Mayer, M. (2022). Model comparison and calibration assessment. arXiv.\n\n\nHemmert, G. A. J., Schons, L. M., Wieseke, J., & Schimmelpfennig, H. (2018). Log-likelihood-based pseudo-R2 in logistic regression: Deriving sample-sensitive benchmarks. Sociological Methods & Research.\n\n\nMcFadden, D. (1974). The measurement of urban travel demand. Journal of Public Economics.\n\n\nMITTLBOCK, M., & SCHEMPER, M. (1996). EXPLAINED VARIATION FOR LOGISTIC REGRESSION. Statistics in Medicine.\n\n\nPencina, M. J., Sr, R. B. D., Jr, R. B. D., & Vasan, R. S. (2008). Evaluating the added predictive ability of a new marker: From area under the ROC curve to reclassification and beyond. Statist. Med.\n\n\nPepe, M. S., Feng, Z., & Gu, J. W. (2008). Comments on “evaluating the added predictive ability of a new marker: From area under the ROC curve to reclassification and beyond” by m. J. Pencina et al., Statistics in medicine (DOI: 10.1002/sim.2929). STATISTICS IN MEDICINE.\n\n\nTheil, H. (1970). On the estimation of relationships involving qualitative variables. American Journal of Sociology.\n\n\nTjur, T. (2009). Coefficients of determination in logistic regression models—a new proposal: The coefficient of discrimination. The American Statistician.\n\nCitationBibTeX citation:@misc{friesenhahn,christinarabeandcourtneyschiffman2023,\n  author = {Michel Friesenhahn, Christina Rabe and Courtney Schiffman},\n  title = {Everything You Wanted to Know about {R2} but Were Afraid to\n    Ask. {Part} 2, {Binary} Outcomes and Generalizing the Fundamental\n    Decomposition.},\n  date = {2023-10-31},\n  url = {https://www.go.roche.com/stats4datascience},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMichel Friesenhahn, Christina Rabe and Courtney Schiffman. (2023,\nOctober 31). Everything you wanted to know about R2 but were afraid\nto ask. Part 2, Binary outcomes and generalizing the fundamental\ndecomposition. Retrieved from https://www.go.roche.com/stats4datascience"
  }
]