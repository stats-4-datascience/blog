{
  "hash": "5d86e3076fc05f57c46e1b95d9c5c6b1",
  "result": {
    "markdown": "---\ntitle: \"How to get the most out of prognostic baseline variables in clinical trials\"\nsubtitle: \"Effective strategies for employing covariate adjustment and stratification\"\nimage: images/increased_precision.png\ncategories: \n  - covariate adjustment\n  - stratification\nauthor:\n  - name: Courtney Schiffman\n    affiliation: Genentech\n    affiliation-url: https://www.gene.com/\n    email: schiffman.courtney@gene.com\n  - name: Christina Rabe\n    affiliation: Genentech\n    affiliation-url: https://www.gene.com/\n    email: rabe.christina@gene.com    \n  - name: Michel Friesenhahn\n    affiliation: Genentech\n    affiliation-url: https://www.gene.com/\n    email: friesenhahn.michel@gene.com\ndate: 10/12/2022\ndate-modified: last-modified\ncitation:\n  type: post\n  title: How to get the most out of prognostic baseline variables in clinical trials. Effective strategies for employing covariate adjustment and stratification.\n  author: Courtney Schiffman, Christina Rabe and Michel Friesenhahn \n  url: https://www.go.roche.com/stats4datascience\n  issued: 2022/10/12\nbibliography: references.bib\n---\n\n::: {.cell}\n\n:::\n\n\n# New Insights into an Old Method\n\n<center>\n\n![](images/new_insights.png){width=60%}\n\n</center>\n\n\nIn randomized trials, a wide range of baseline covariates are collected prior to\nrandomization. These include demographic variables, baseline outcome\nassessments, biomarkers from biological samples and medical images. Baseline\ncovariates that are prognostic for trial outcomes are often used for enrichment,\nstratified randomization, imbalance checking and subgroup analyses. However, it\nis not widely recognized that one of the most powerful uses of prognostic\nbaseline variables in randomized clinical trials is for covariate adjustment.\n*A pre-specified primary endpoint analysis that adjusts for prognostic baseline\ncovariates is a rigorous and virtually risk-free analysis that increases\nprecision and power.*\n\nWhile the concept of covariate adjustment and its potential value for increasing\ntrial power is decades old, historically there has been some controversy around\nhow to ensure the validity of trial analyses without relying on model\nassumptions. This has been particularly challenging for non-continuous\nendpoints. Fortunately, there have been advancements that have resolved this\ncontroversy. Our first aim for this blog is to build on those advancements and\nprovide concrete guidance on how to perform treatment effect estimation and\ninference using covariate adjustment that is both rigorous and\neasy-to-implement. In general, standard ANCOVA-based inference is correct when\nthe regression model is correctly specified, whereas the covariate adjustment\nmethods proposed here are asymptotically robust to model misspecification.\n\nFurthermore, the pre-specified covariates that are currently used in clinical\ntrials can be sub-optimal and strategies are needed to better determine which\nbaseline covariates are best to use for adjustment. These baseline covariates\ncould be a simple collection of individual variables or a single prediction from\na complex machine learning model. External data, e.g. data from historical\ntrials and observational studies, are an excellent resource for identifying and\nevaluating such candidate covariates. Accordingly, our second aim is to provide\nstrategic guidance and relevant performance metrics for these evaluations.\n\nThis document will be a practical how-to guide for clinical trial statisticians\non effective, trustworthy, and practical use of covariate adjustment in clinical\ndevelopment for maximizing power and minimizing bias. We will also discuss the\nvalue of stratified randomization compared to covariate adjustment. We focus\nhere on continuous outcomes, but many of the points discussed apply to binary\nand ordinal outcomes as well. \n\n# Covariate Adjustment Has Gotten Hip\n\n<center>\n\n![](images/collage.png){width=80%}\n\n</center>\n\n\nCovariate adjustment has recently gained popularity in a variety of industries.\nIn drug development, the FDA has provided a [guidance\ndocument](https://www.fda.gov/regulatory-information/search-fda-guidance-documents/adjusting-covariates-randomized-clinical-trials-drugs-and-biological-products) that strongly encourages its use. There are\nalso several start-ups who offer covariate adjustment to optimize analysis of\nonline experiments as a key use case for their prognostic models [@Netflix;\n@CUPED; @CUPAC; @MLRATE]. Across these different applications, what\nstatisticians and the FDA call covariate adjustment is frequently given\nalternative names, for example CUPED (controlled-experiment using pre-experiment\ndata), CUPAC (Control Using Predictions As Covariates) and MLRATE (machine\nlearning regression-adjusted treatment effect estimator). Companies like Owkin\nand Unlearn.AI focus on the use of covariate adjustment in randomized trials,\nhoping to potentially decrease trial sample size by incorporating machine\nlearning [@owkin; @unlearn].\n\nWe heartily celebrate this increased enthusiasm for covariate adjustment in a\nvariety of settings and hope the current blog post enables clinical trial\nstatisticians to make the most effective use of baseline prognostic data.\n\n# What’s the Big Deal About Covariate Adjustment?\n\n## Covariate Adjustment Increases Precision and Power\n\n![Covariate adjustment removes explained variability](images/increased_precision.png){#fig-cov-adj}\n\nUnexplained variability in the outcome data limits the precision with which you\ncan estimate your marginal average treatment effect (ATE). If baseline\ncovariates are prognostic, subtracting off any reasonable linear combination of\nthose covariates from the observed outcomes, and analyzing the differences\ninstead reduces variability of the treatment effect estimate. The marginal ATE\nestimate calculated using the residuals (observed-predicted) is still\nasymptotically unbiased, because the expected value of the predictions is equal\nacross all treatment arms. However, the residuals have less variability than the\noriginal outcomes, leading to a more precise estimate.\n\n## Covariate Adjustment Removes the Impact of Imbalance\n\nA common concern in randomized trials is that treatment groups, by chance,\ndiffer in the distributions of one or more important prognostic baseline\ncovariates. When this occurs, one perspective is that this is not a problem\nsince the estimates are still unbiased and type 1 error is still preserved.\nHowever, such statistical properties are unconditional on the observed covariate\nimbalance. When we do condition on observed imbalance, the treatment effect\nestimates can be biased and type 1 errors can drastically differ from their\nnominal levels. A simple, clear and insightful analysis of this issue was\nprovided by Stephen Senn who has amusingly remarked \"If you are at 35,000 ft,\nfour engines are on fire and the captain has had a heart-attack can you say:\n'Why worry, on average air travel is very safe?'” [@Senn; @Senn2010].\n\n![Conditional type-1 error depending on imbalance](images/imbalance.png){#fig-imbalence}\n\nHow do you address concerns over imbalance? Following the analysis from Stephen\nSenn, @fig-imbalence shows the conditional type 1 error as a function of\nstandardized observed imbalance for a range of assumed correlations between the\nbaseline covariate and outcome. This shows that significance tests for imbalance\n(a commonly used and misguided practice) can fail to flag imbalance effects even\nwhen the conditional probability of a type 1 error well exceeds the nominal\nlevel. Therefore, significance tests do not reliably control type 1 error.\nInstead, Senn showed that covariate adjustment removes conditional bias from\ntreatment effect estimates and leads to correct conditional type 1 errors of\nconstant size.\n\n> \"Analysis of covariance can be recommended on two grounds: increased power and\n> constant conditional size. The former should be sufficient to recommend the\n> method to those who consider that the latter is irrelevant but for those who\n> are concerned about conditional size this is an added advantage\" [@Senn].\n\n## Covariate Adjustment is Rigorous\n\nTo address the historical confusion and controversy around covariate adjustment,\nmethodological development has clearly specified the sampling framework, target\nestimand (i.e. population level parameter) and required assumptions. While we\nfocus on continuous outcomes, the rigor of covariate adjustment also applies to\nnon-continuous outcomes.\n\n### Sampling framework\n\nAn important but often overlooked component of specifying an estimand and\nestimator is the assumed sampling framework. A rigorous, adjusted analysis makes\nthe assumed sampling framework transparent. There are several choices for the\nsampling framework, but we assume here the commonly used super-population\nframework. Let $N$ be the total sample size for a randomized trial with several\ntreatment arms and let $A=0,1,\\ldots,a$ be a random variable denoting treatment\narm assignment. In the super-population framework, $N$ full data vectors are\nassumed to be drawn independently from some unknown, joint distribution. Patient\n$i's$ full data vector $(X_i, Y_i(0),Y_i(1),\\ldots,Y_i(a))$ contains baseline\ncovariates $X_i$ and a potential outcome $Y_i(k)$ for each possible treatment\nassignment $k=0,1,\\ldots,a$. Patient $i's$ observed data vector $(A_i, X_i,\nY_i)$ consists of their treatment assignment $A_i$, baseline covariates $X_i$\nand *observed* outcome $Y_i=Y_i(0)I(A_i=0)+\\ldots+Y_i(a)I(A_i=a)$. In the case\nof simple random sampling, $A$ and $X$ are independent.\n\nIt is worth noting that rigorous theory behind covariate adjustment has also\nbeen developed under the Neyman framework, which assumes a fixed, finite\npopulation where the only source of randomness is treatment assignment \n[@lin2013; @Peng; @Ding]. \n\n### Target Estimand\n\nConfusion often arises since there are various kinds of covariate adjusted\nanalyses targeting different estimands. For example, covariate adjustment is\noften used in the estimation of conditional average treatment effects, which are\ncontrasts in treatment arm means that are conditional on baseline covariates. A\nconditional treatment effect is not a single value, but rather a function of\nbaseline covariates, unless one makes the assumption that the conditional\ntreatment effect is constant.\n\n:::{ .callout-definition }\n# Conditional Average Treatment Effect\n\nA contrast (difference, ratio, etc.) between treatment arm means conditional on\nbaseline covariates. Example of a conditional ATE: \n\n$$\nE(Y_{active}|X) - E(Y_{control}|X)\n$$ {#eq-cond-avg-trt-eff}\n:::\n\nHowever, in this document, we do not use covariate adjustment for that purpose.\nInstead, we us **covariate adjustment as a tool to more efficiently estimate the\nmarginal ATE**:\n\n:::{ .callout-definition }\n# Marginal Average Treatment Effect\n\nA contrast (difference, ratio, etc.) between marginal treatment arm means.\nExample of a marginal ATE: \n\n$$\nE(Y_{active}) - E(Y_{control})\n$$ {#eq-marg-avg-trt-eff}\n:::\n\nSome researchers suggest focusing on estimating conditional treatment effects\nand using covariate adjustment for that purpose. While estimating conditional\nand individualized treatment effects is an important research objective, a\ndiscussion about how to do so is beyond the scope of this work. Here, we assume\nestimating the marginal ATE is the primary objective of a label-enabling\nclinical trial.\n\nWe note that the terms 'marginal' and 'conditional' are often conflated with\n'unadjusted' and 'adjusted', however these terms are not synonymous [@Daniel]:\n\n:::{ .callout-tip }\n# Marginal vs. Conditional\nUsed to distinguish different kinds of estimands.\n:::\n\n:::{ .callout-tip }\n# Unadjusted vs. Adjusted\nUsed to distinguish different kinds of estimators. \n:::\n\n### Assumptions\n\nHistorically, researchers have debated how to safely estimate marginal ATEs\nusing covariate adjustment. Some have been hesitant to use covariate adjustment\nover concerns around model misspecification, bias and possible losses in\nprecision [@Freedman2008a; @Freedman2008b], while others have enthusiastically\nused covariate adjustment without a second thought.\n\nAcademic research ultimately settled that debate [@Tsiatis2001; @Tsiatis2008;\n@Rosenblum2009; @TMLE2010]. A number of approaches to covariate adjustment have\nbeen characterized that result in consistent, asymptotically normal estimators\nof marginal ATEs even if the regression model is misspecified. This means that\neven if important interactions or non-linearities are excluded, prognostic\nfactors are missed, or factors are included that have no prognostic value,\ncovariate adjustment consistently estimates the marginal ATE.\n\nThe covariate adjustment approach we use is called a [standardized regression\nestimator][Standardized Regression Estimator], which uses predictions from a\nworking regression model to obtain marginal ATEs. The model is referred to as a\nworking regression model because it does not have to represent the true data\ngenerating distribution. Therefore, we refer to covariate adjustment as a\n**model-assisted, not model-dependent, analysis**. Examples of working\nregression models include logistic, OLS, and Poisson regression models.\n\nFor the standardized regression estimator, it is sufficient to assume that\ntreatment is assigned via simple random sampling, that the full data vectors are\ni.i.d. and that all variables are bounded [@Rosenblum2009; @TMLE2010].\nExtensions to other sampling frameworks have also been explored [@Wang2021].\n\n## Covariate Adjustment is Encouraged by the FDA\n\nIn a May 2023 [\nguidance](https://www.fda.gov/drugs/drug-safety-and-availability/fda-issues-final-guidance-adjusting-covariates-randomized-clinical-trials),\npre-specified covariate adjustment was fully supported by the FDA\nas a primary endpoint analysis in clinical trials:\n\n> Sponsors can adjust for baseline covariates in the analyses of efficacy\n> endpoints in randomized clinical trials. Doing so will generally reduce\n> the variability of estimation of treatment effects and thus lead to \n> narrower confidence intervals and more powerful hypothesis testing.\" [@FDA]. \n\nImportantly, the guidance states that \"[s]ponsors should prospectively specify the \ndetailed procedures for executing covariate adjusted analysis before any unblinding \nof comparative data. FDA review will emphasize the prespecified primary analysis\nrather than post-hoc analyses using different models or covariates\"[@FDA].\nWe interpret this to mean that the exact working regression model form\nshould be pre-specified before working with the trial data. Independent data\nfrom observational cohorts or previous trials can be used to select or develop\ncovariates. These covariates may be existing baseline variables or even the\noutput from a multivariable prediction model which takes as input other baseline\ncovariates.\n\nIn contrast to pre-specifying an ANCOVA regression model, there is active\nresearch into applying machine learning directly to the unblinded trial data\nunder investigation (as opposed to historical data) [@covid2; @covid; @Tsiatis2008;\n@Tian2012] as part of the covariate adjusted analysis. While the role machine\nlearning procedures can play in covariate adjustment is an interesting and\nimportant area of research, we do not yet recommend using them as part of the \nprimary endpoint analysis of a clinical trial due to several considerations.\nFirst, the finite sample properties of the estimators are not understood. Second, these methods add complexity and burden to the study yet are unlikely to provide meaningful benefits.\n\nIt may not be appreciated that a lot of things need to happen between the unblinding of a randomized clinical trial and the reporting of topline results. Outputs and code are quality controlled, pre-specified sensitivity analyses are performed, and internal reviews are conducted with key decision makers and stakeholders. All of these activities need to occur in a tight time frame for practical considerations such as appropriate control of insider information.\n\nIn addition, during the planning phase, many choices have to be made when applying machine learning directly to the trial data such as which covariates to present to the machine learning procedure, the method of cross validation to use, which machines to include if using ensemble learning and how to perform standard error estimation. These choices, many of which are based on simulation studies, add to the difficulty of finalizing the statistical analysis plan.\n\nIn summary, applying machine learning to the trial data as part of the primary endpoint analysis adds considerable burden to the study team. *This added burden would only be worthwhile if the expected precision gains were meaningful*. However, in our experience, complex black-box models rarely outperform simple linear models when working with tabular data and do not support this additional complexity and time.\n\n\n\n# How to Perform Covariate Adjustment\n\n## Standardized Regression Estimator\n\n@tbl-estimation provides instructions and code for estimating the Treatment Arm Means\n(TAMs) and the marginal ATE (in this case, the difference in means) using\ncovariate adjustment. The example code shows how to estimate these quantities\nfor trials with two treatment arms, but easily generalizes to more than two\narms. This estimator is often referred to as the **standardized regression\nestimator**, and is also an example of a targeted maximum likelihood estimator.\nThere are other consistent estimators of the marginal ATE that use covariate\nadjustment (see @Colantuoni2015 for examples). However, we suggest using the\nstandardized regression estimator because it is easy to use, is a \"reliable method for covariate adjustment\" [@FDA],\nhas comparable power and can also be used to estimate TAMS and\nmarginal ATEs for non-continuous outcomes.\n\nIn the notation below, $A$ is a two-level factor indicating treatment\nassignment, $Y$ is the observed outcome, $X$ is a baseline covariate, $E[Y|a,\nX_i] = \\mu(a,X_i)$ is the expected value of $Y$ for patient $i$ given their\ntreatment assignment $A_i=a$ and baseline covariate $X_i$, $\\hat{\\mu}(a,X_i)$ is\nthe predicted outcome for patient $i$ from the working regression model and an\nestimate of $\\mu(a,X_i)$, and $N$ is the total trial sample size. The\nstandardized regression estimator for the mean outcome in treatment arm $A=a$\nis:\n\n$$\n\\widehat{TAM_a}=\\frac{1}{N}\\sum_{i=1}^{N}\\hat{\\mu}(a,X_i)\n$$ {#eq-std-reg-mean}\n\nThe standardized regression estimator for the difference in means for treatment\narms $A=0$ and $A=1$ is:\n\n$$\n\\widehat{ATE}=\\frac{1}{N}\\sum_{i=1}^{N}\\hat{\\mu}(1,X_i)-\\frac{1}{N}\\sum_{i=1}^{N}\\hat{\\mu}(0,X_i)\n$$ {#eq-std-reg-diff}\n\nOther contrasts of the marginal means (TAMs) may be similarily estimated.\n\n::: { .column-page-inset }\n+------+--------------------------------+------------------------------------------------------------------------------+\n| Step | Estimation Instructions        | Example R Code                                                                |\n+======+================================+==============================================================================+\n| 1    | Fit a working regression model | ```{ .sourceCode .r }                                                        |\n|      | using data from all treatment  | # Additive Working Regression Models                                         |\n|      | arms, regressing the outcome   | mod.fit <- lm(Y ~ A + X, data = d)  # continuous                             |\n|      | on treatment and prognostic    | mod.fit <- glm(Y ~ A + X, data = d, family = binomial)  # binary             |\n|      | baseline covariates            |                                                                              |\n|      |                                | # Working Regression Models with Interaction                                 |\n|      |                                | mod.fit <- lm(Y ~ A * X, data = d)  # continuous                             |\n|      |                                | mod.fit <- glm(Y ~ A * X, data = d, family = binomial)  # binary             |\n|      |                                | ```                                                                          |\n+------+--------------------------------+------------------------------------------------------------------------------+\n| 2    | For each subject, use the      | ```{ .sourceCode .r }                                                        |\n|      | model from step 1 and the      | # Continuous outcome                                                         |\n|      | subject's baseline covariates  | pred0 <- predict(mod.fit, newdata = data.frame(A=0, X=d$X))                  |\n|      | to compute their predicted     | pred1 <- predict(mod.fit, newdata = data.frame(A=1, X=d$X))                  |\n|      | outcome under EACH treatment   |                                                                              |\n|      | of interest (regardless of     | # Binary outcome                                                             |\n|      | what their assigned treatment  | pred0 <- predict(mod.fit, newdata = data.frame(A=0, X=d$X), type=‘response’) |\n|      | was)                           | pred1 <- predict(mod.fit, newdata = data.frame(A=1, X=d$X, type=‘response’)  |\n|      |                                | ```                                                                          |\n+------+--------------------------------+------------------------------------------------------------------------------+\n| 3    | Take the average of the        | ```{ .sourceCode .r }                                                        |\n|      | predicted outcomes in each     | # Mean of predictions in treatment arm A=0                                   |\n|      | treatment group to get         | TAM0 <- mean(pred0)                                                          |\n|      | estimates of the Treatment     |                                                                              |\n|      | Arm Means (TAMs)               | # Mean of predictions in treatment arm A=1                                   |\n|      |                                | TAM1 <- mean(pred1)                                                          |\n|      |                                | ```                                                                          |\n+------+--------------------------------+------------------------------------------------------------------------------+\n| 4    | Compute the desired contrast   | ```{ .sourceCode .r }                                                        |\n|      | of the TAMs to get an          | # Estimate the desired contrast, for example, the                            |\n|      | estimate of the marginal ATE   | # difference between treatment arm A=1 and A=0                               |\n|      |                                | ATE <- TAM1 - TAM0                                                           |\n|      |                                | ```                                                                          |\n+------+--------------------------------+------------------------------------------------------------------------------+\n\n: Standardized regression estimator for TAMs and marginal ATE {#tbl-estimation}\n\n:::\n\n\nIn the example code in @tbl-estimation, $X$ is a single covariate, but $X$ can also be a\nmatrix containing a set of individual covariates, a prediction from an\nindependent prognostic model, or a combination thereof. When the working\nregression model is an additive OLS regression, the standardized regression\nestimate of the difference in means marginal ATE equals the estimated\ncoefficient for $A$. This is also true for an interaction model when the\ncovariates are centered. Note that, unlike OLS regression, in generalized linear\nmodels the estimated coefficient for treatment does not translate into a\nmarginal ATE.\n\nTheoretically, it is possible to lose precision with covariate adjustment when\nusing an additive model compared to an unadjusted analysis. In contrast, a\nstandardized regression estimator that uses an interaction working regression\nmodel is asymptotically guaranteed to be at least as precise as an unadjusted\nestimator or adjustment with an additive model. However, it is only under\nunrealistic conditions that covariate adjustment with an additive model will be\nless precise than an unadjusted estimator. What is more, if randomization is 1:1\nor the covariances between covariates and outcome are equal across treatment\narms, then covariate adjustment with an additive model will be as efficient as\nwith an interaction model. In our experience, interaction models do not lead to\nconsiderable gains in precision over additive models. Likewise, Tsiatis et al.\n(2001) performed simulations comparing precision when using an additive vs. an\ninteraction model and \"found the loss of efficiency to be trivial in all cases\"\n[@Tsiatis2001]. Therefore, **additive models are a good default choice**\nespecially if your trial sample size is too small to support\ntreatment-by-covariate interactions (see [model budget section][Determining your\nModel Budget]).\n \nIn case of missing values in baseline covariates, a baseline covariate\nthat has too many missing values should not be used for adjustment, but if the\nnumber of missing values is small, we recommend simple, single imputation as\nopposed to using indicator variables in order to spend the model budget wisely. \n\n## Inference for TAMs and ATEs\n\nWe suggest using robust standard error estimators from the targeted maximum\nlikelihood estimation (TMLE) literature because they are robust to model\nmisspecification, are fast and easy to implement with a few lines of code, are\nvalid under any randomization ratio and data generating distribution, provide\nstandard error estimates for both ATEs and treatment arm means, are aligned with\nthe superpopulation framework and are deterministic [@TMLE2010]. \n\nLet $I(A_i=a)$ be an indicator of patient $i$ receiving treatment $A=a$ and\n$p(A=a)$ be the probability of being assigned to treatment arm $A=a$. The TMLE\n**standard error estimate for the mean of treatment arm $A=a$** is:\n\n$$\n\\sqrt{\\frac{\\frac{1}{N}\\sum_{i=1}^{N}\\Bigg(\\frac{I(A_i=a)(Y_i-\\hat{\\mu}(a,X_i))}{p(A=a)}+\\hat{\\mu}(a,X_i)-\\frac{1}{N}\\sum_{i=1}^{N}\\hat{\\mu}(a,X_i)\\Bigg)^2}{N}}\n$$ {#eq-std-err-est-mean}\n\nThe standard error estimate for the **difference in Means for Treatment arms\n$A=1$ and $A=0$** is:\n\n:::{ .column-body-outset }\n$$\n\\sqrt{\\frac{\\frac{1}{N}\\sum_{i=1}^{N}\\Bigg(\\frac{I(A_i=1)(Y_i-\\hat{\\mu}(1,X_i))}{p(A=1)}+\\hat{\\mu}(1,X_i)-\\frac{1}{N}\\sum_{i=1}^{N}\\hat{\\mu}(1,X_i) - \\Big(\\frac{I(A_i=0)(Y_i-\\hat{\\mu}(0,X_i))}{p(A=0)}+\\hat{\\mu}(0,X_i)-\\frac{1}{N}\\sum_{i=1}^{N}\\hat{\\mu}(0,X_i)\\Big) \\Bigg)^2}{N}}\n$$ {#eq-std-err-est-diff}\n:::\n\nThe code for the above SE estimators is shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## SE estimate for treatment arm mean A=0\nsqrt(mean(((A == 0) * (Y - pred0) / mean(A == 0) + pred0 - mean(pred0)) ^ 2) / nrow(d))\n\n## SE estimate for treatment arm mean A=1\nsqrt(mean(((A == 1) * (Y - pred1) / mean(A == 1) + pred1 - mean(pred1)) ^ 2) / nrow(d))\n\n## SE estimate for the marginal ATE, the difference between treatment arm mean A=1 and A=0\nsqrt(mean(((A == 1) * (Y - pred1) / mean(A == 1) + pred1 - mean(pred1) -\n        ((A == 0) * (Y - pred0) / mean(A == 0) + pred0 - mean(pred0))) ^ 2) / nrow(d))\n```\n:::\n\n\nUsing the estimated standard errors and the asymptotic normality of the\nstandardized regression estimator, one can then easily obtain $p$-values and\n$95\\%$ confidence intervals for both the treatment arm means and the marginal\nATE.\n\nWe note that the above standard error estimates assume observations are i.i.d.\n(i.e. randomization is not stratified), and we address this assumption in a\n[later section][Accounting for Stratified Randomization]. For standard error\nestimates corresponding to marginal ATEs using different contrasts (e.g. ratio\ninstead of difference) see Rosenblum and van der Laan (2010).\n\nAlternative standard error estimators include the Huber-White estimator and\nre-sampling based methods like the non-parametric bootstrap.\nIn their recent guidance, the FDA recommends using a \"robust standard error method such as the Huber-White “sandwich” \nstandard error when the model does not include treatment by covariate interactions\" and they recognize that\n\"other robust standard error methods proposed in the literature can also cover cases with interactions\" [@FDA].\nIf using a working regression model with treatment-by-covariate\ninteractions, the expected sandwich standard error estimates may be too small\nwhen treatment effects are heterogeneous ([see Appendix][Appendix] and @imbens\n, @lin2013). Bootstrap methods are a reasonable alternative, but we prefer TMLE\nestimators since they are easier to implement and less computationally\nintensive. Furthermore, bootstrap methods lead to confidence intervals and\np-values that are not deterministic. An intriguing alternative to the bootstrap\nthat is deterministic is the jackknife [@jackknife].\n\n## Determining your Model Budget\n\nIs there such a thing as adjusting for too many prognostic covariates? Yes! The\nmarvelous properties of covariate adjustment discussed thus far (consistency,\nrobustness to model misspecification, precision gains, etc.) are **large sample\nproperties**. Thus, adjusting for too many covariates in your working model\nrelative to your sample size may invalidate the analysis [@ML; @Colantuoni2015;\n@Steingrimsson2016], and the FDA advises against adjusting for too many\ncovariates:\n\n> \"The statistical properties of covariate adjustment are best understood when\n> the number of covariates adjusted for in the study is small relative to the\n> sample size.\"[@FDA]\n\nExplicit guidance on how many covariates to adjust for is not offered, so we\nborrow from prognostic modeling practices [@rms].\n\n::: { .callout-definition }\n# Model Budget\n\nThe number of allowable terms in the working regression model, excluding the\noverall intercept. Suggestions for a model budget based on classic events per\nvariable considerations are [@rms]:\n\n| terms (% of total sample size) | | guidance |\n|-----------------:|:-:|:-----------------|\n| $\\leq 5\\%$                     | <i class=\"bi bi-check-circle-fill text-success\"></i> | likely a safe choice |\n| $\\approx7.5\\%$                 | <i class=\"bi bi-exclamation-triangle-fill text-warning\"></i> | probably reasonable  |\n| $\\gt 10\\%$                     | <i class=\"bi bi-question-octagon-fill text-danger\"></i>  |potentially unsafe   |\n:::\n\nPre-specifying a set of covariates and a working regression model form that\nsatisfies the model budget maintains the rigor and integrity of the statistical\nanalysis. We note that this is a rough guidance and should be considered a\nuseful starting point. Recently, model budget calculations for general\nprognostic modeling have been refined and in future work it would be helpful to\nadapt such an approach to covariate adjustment (see @Riley2019 and @Riley2020).\n\n## Spending your Model Budget Wisely\n\n<center>\n\n![](images/model_budget.png){width=60%}\n\n</center>\n\n\nThe model budget tells you how complex the working regression model can be while\nensuring treatment effect estimates are unbiased, confidence intervals have good\ncoverage, and type 1 error rates for hypothesis testing do not exceed nominal\nlevels. However, there are many options when pre-specifying the working\nregression model and it is easy to spend beyond one's model budget, so strategic\nchoices will need to be made prior to looking at the data.\n\nWe suggest determining the model form and spending the model budget according to\nthe following four principles:\n\n1. **Leverage external/historical data**  \n   Historically, pre-specification of a regression model was guided by ad hoc\n   analyses reported in the literature, input from subject matter experts, and\n   precedent set by previous trials.  While these continue to be important\n   considerations, big data collection and curation and the introduction of\n   machine learning methods to drug development significantly improve the\n   process of pre-specifying the regression model form.\n\n2. **Prioritize maximally prognostic covariates**  \n   The list of candidate prognostic covariates may be too large according to the\n   model budget, in which case a subset of prognostic covariates will need to be\n   selected. We use external/historical data to guide that selection (see\n   upcoming section on [Performance Metrics]).\n\n:::{ .callout-tip }\n# Pro Tip\n\nIf the model budget is tight and several covariates are expected to meaningfully\nadd prognostic value, consider combining them into a single score and using that\nin the model instead of the individual covariates.\n:::\n\n![Leverage independent data to find the most promising prognostic factors/models for covariate adjustment](images/prognostic_modeling.png){#fig-prog-mod}\n\n3. **Avoid Dichotomania!**  \n   Usually the relationship between a continuous variable and outcome varies\n   gradually and abrupt dichotomization loses prognostic information [@Ge2011,\n   @Dichotomania]. We recommend that, unless there is evidence to the contrary,\n   prognostic covariates that have been discretized be replaced with their\n   underlying continuous forms. Note that, in particular, this will impact how\n   stratification factors enter into the model. \n\n4. **By default, do not include interactions between covariates**  \n   There is a large body of evidence showing that for tabular data, purely\n   additive models are very rarely, if ever, meaningfully improved by including\n   interactions [@Rudin]. This has also been our experience with many prognostic\n   modeling projects. We do not recommend including interactions between\n   prognostic covariates in the model since these are usually wasteful expenses\n   in the model budget, unless evidence to the contrary is available.\n\n5. **By default, do not include interactions between covariates and treatment arm**  \n   [As discussed above][Standardized Regression Estimator], such interactions\n   rarely provide meaningfully improved precision and power under realistic\n   assumptions.\n\nUse the above five principles to pre-specify a working regression model and to\nspend your model budget wisely. Then, use this model in the [previously\ndescribed standardized estimator][Standardized Regression Estimator] to estimate\nthe treatment arm means and marginal ATEs and to [perform inference][Inference\nfor TAMs and ATEs]. \n\n\n## Accounting for Stratified Randomization\n\nThe [estimation][Standardized Regression Estimator] and [inference][Inference\nfor TAMs and ATEs] procedures described in the previous sections assume *simple\nrandomization*, where each patient's treatment assignment is an independent and\nidentically distributed Bernoulli random variable for a trial with two treatment\narms and a categorical random variable for a trial with more than two treatment\narms. However, randomization in clinical trials is usually stratified by\nselected baseline covariates, for example, using permuted block randomization.\nIn this case, the FDA recommends \"that the standard error computation account for stratified randomization\" because\n\"an analysis ignoring stratified randomization is likely to overestimate standard errors and can be unduly conservative when performing inference for the average treatment effect\" [@FDA].\n\n**However, we recommend that it is NOT necessary to modify estimation and\ninference to account for stratified randomization.** Instead, we propose that\nthe previously described strategy for [spending the model budget\nwisely][Spending your Model Budget Wisely] be used and applied to all prognostic\ncandidates, making no distinction between stratification factors and other\nbaseline covariates. Then use covariate adjustment as described in the previous\nsections for estimation and inference of the marginal ATEs, *as if treatment\nwere allocated using simple random sampling*.\n\nNote that, as a consequence:\n\n1. By default, if stratification variables do enter into the working regression\n   model, they will do so as additive terms.\n2. Dichotomized stratification factors will be replaced by their underlying\n   continuous variables.\n3. Stratification variables not expected to be correlated with outcome in any\n   treatment group may be omitted entirely.\n\nSuch an approach still maintains type 1 error and leads to consistent estimators\nfor marginal ATEs. At worst, inference is conservative (p-values may be too\nlarge and confidence intervals too wide) [@Wang2021]. However, [if the model\nbudget is spent wisely][Spending your Model Budget Wisely] to obtain a sensible\nworking regression model, conservatism will be negligible.\n\n# Performance Metrics\n\n<center>\n\n![](images/performance.png){width=50%}\n\n</center>\n\n\n## Intuitive Metrics for Quantifying the Expected Benefit of Adjustment\n\nWhen analyzing external/historical data in preparation for pre-specifying a\ncovariate-adjusted primary endpoint analysis, it's important to understand the\nfollowing:\n\n1. Which covariates to adjust for\n2. The expected precision gains from adjustment\n\nBelow, we provide performance metrics that allow study statisticians to easily\nselect which covariates to adjust for, to understand the impact of heterogeneous\ntreatment effects (HTEs) on precision gains and to estimate the expected benefit\nfrom adjustment **without having to perform simulations**.\n\nA great way to derive a single, high-quality covariate is to use external data\nto construct a prognostic model that combines multiple covariates into a single\nprognostic score. Thus, we focus here on how to evaluate a single prognostic\ncovariate. However, if the model budget allows and the prognostic model is\noriginally linear in form, the individual covariates can be expanded out in the\nworking regression model and re-fit, usually with no loss in performance. \n\nFor practical use in a trial, prognostic models should achieve **maximum\nperformance with minimal complexity.** Complexity is determined by the:\n\n1. Cost, incovenience, and invasiveness of covariates.\n2. Ease of deployment and implementation of the prognostic model.\n3. Algorithmic complexity (e.g. linear models vs deep learners). \n\nWe find that linear prognostic models are hard to beat when working with tabular\ndata. Deep learning models are more likely to provide a boost in performance\nover linear models when working with imaging data. Deep learners are natural\ncandidates when working with non-tabular data, such as raw image files. Later we\nwill show an example where considerable gains are achieved using a deep learner\ntrained to predict progression in an ophthalmologic disease from raw images.\n\nAn intuitive metric for the expected benefit of a prognostic score is the amount\nby which it effectively increases the study's sample size via the gain in\nprecision, i.e. the **Effective Sample Size Increase (ESSI)**. For example,\nsuppose adjusting for a particular covariate is associated with an estimated\nESSI of $40\\%$. That means, asymptotically, the following two analyses have the\nsame power:\n\n* A covariate-adjusted analysis with sample size $N$\n* An unadjusted analysis but with $40\\%$ more patients, $1.4 * N$\n\nESSI estimates can be used to compare sets of covariates or prognostic models\nand make transparent the expected benefits of adjustment. \n\n## ESSI Formulas\n\nFor simplicity, we assume 1:1 randomization and equal, marginal variances across\nthe treatment arms. We also provide [ESSI formulas][Appendix] that\nrelax these assumptions.\n\nIt turns out that the **ESSI from covariate adjustment for continuous outcomes**\ncan be easily estimated since it only depends on two parameters:\n\n:::{ .callout-definition }\n# General ESSI\n$$\nGeneral \\space ESSI=\\Bigg(\\frac{1}{1 - \\big(\\frac{r_{control} \\space + \\space r_{active}}{2}\\big)^2}-1\\Bigg) * 100\\% \n$$ {#eq-essi-gen}\n\n$$\n\\begin{align*}\n  \\text{where} \\\\\n  r_{active} &= \\text{Correlation between the outcome and covariate in the active,} \\\\\n  r_{control} &= \\text{Correlation between the outcome and covariate in the control arm}\n\\end{align*}\n$$\n:::\n\nIt makes intuitive sense that the ESSI depends on the squared average of the\ncorrelations in the two treatment arms, increasing as the correlations increase.\nThe correlation between the covariate and the outcome among untreated patients\n($r_{control}$) can be estimated using historical trial data, observational\ncohorts, etc.\n\nThis leaves $r_{active}$ as the only quantity left to estimate. To help estimate\n$r_{active}$, note that the correlation between the outcome and the covariate in\nthe treatment arm depends on the \"treatment effect scenario.\" Two plausible\ntreatment effect scenarios are discussed in the following sections.\n\n### Constant absolute treatment effect\n\nOne commonly assumed treatment effect scenario is that the *absolute* treatment\neffect is constant across the baseline covariate:\n\n::: { .callout-definition }\n# Constant absolute treatment effect:\n\n$E(Y_{active}|X)=E(Y_{control}|X)-\\delta$, for some constant $\\delta$. With a\nconstant absolute treatment effect, $r_{control}=r_{active}$. Plugging this into\nthe general ESSI formula we get\n\n$$\nESSI \\space Assuming \\space Constant \\space Absolute \\space \\delta =\\Bigg(\\frac{1}{1-r^2_{control}}-1\\Bigg) * 100\\%\n$$ {#eq-essi-const-abs-delta}\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Constant Absolute Treatment Effect](index_files/figure-html/fig-constant-ate-1.png){#fig-constant-ate width=672}\n:::\n:::\n\n\n### Constant proportional treatment effect\n\nA second plausible treatment effect scenario is that the *relative* treatment effect is constant across the baseline covariate:\n\n::: { .callout-definition }\n# Constant proportional treatment effect\n\n$E(Y_{active}|X)=(1-\\delta)E(Y_{control}|X)$, for some constant $\\delta$. With a\nconstant proportional treatment effect, $r_{active}=(1-\\delta)r_{control}$.\nPlugging this into the general ESSI formula we get \n\n$$\nESSI\\space Assuming \\space Constant \\space Proportional \\space \\delta=\\Bigg(\\frac{1}{1-r^2_{control}\\big(1-\\frac{\\delta}{2}\\big)^2}-1\\Bigg) * 100\\%\n$$ {#eq-essi-const-prop-delta}\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Constant Proportional Treatment Effect](index_files/figure-html/fig-constant-pte-1.png){#fig-constant-pte width=672}\n:::\n:::\n\n\n## Impact of HTEs on gains from covariate adjustment\n\nA constant proportional treatment effect is an example of HTEs. A treatment\neffect is heterogeneous if the magnitude of the treatment effect (on the\nabsolute scale) depends on the value of the baseline covariate. **HTEs decrease\nthe precision gains from adjustment**. For example, notice how, in the constant\nproportional treatment effect formula, the ESSI decreases as $\\delta$ increases\n(i.e. as heterogeneity increases). It is important to recognize the impact of\nHTEs when making decisions about reducing sample size in anticipation of\nincreased power due to covariate adjustment.\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n## A Hypothetical Example\n\nSuppose you are working on pre-specifying an adjusted analysis for an upcoming\nphase 2 trial. Assume that the targeted effect size is a $25\\%$ reduction in\nmean decline in the treatment arm compared to the mean decline in the placebo\narm for a progressive disease that has a continuous outcome measure.\nFurthermore, assume that you have developed a prognostic model using\nexternal/historical data and that the estimated correlation between the\nprognostic model predictions and the outcome was $0.45$\n($r^2_{control}\\approx0.2$). The following is an example of how to calculate\nESSIs from covariate adjustment under two plausible treatment effect scenarios\nthat are consistent with the targeted effect size:\n\n::: { .column-body-outset }\n<table class=\"table tg\">\n  <thead>\n    <tr>\n      <th class=\"tg-baqh\">Treatment Effect Scenario</th>\n      <th class=\"tg-baqh\">$r_{control}$</th>\n      <th class=\"tg-baqh\">$r_{active}$</th>\n      <th class=\"tg-baqh\">ESSI</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td class=\"tg-baqh\">Constant Absolute Delta<br><br>$E(Y_{active}|X)=E(Y_{control}|X)-\\delta$</td>\n      <td class=\"tg-baqh\">$0.45$</td>\n      <td class=\"tg-baqh\"><span style=\"font-weight:400;font-style:normal\">$0.45$</span></td>\n      <td class=\"tg-baqh\">\n      $\\Big(\\frac{1}{1-r^2_{control}}-1\\Big)*100\\%=$<br>$\\Big(\\frac{1}{1-{0.2}}-1\\Big)*100\\% =$ *$25\\%$*\n      </td>\n    </tr>\n    <tr>\n      <td class=\"tg-baqh\">Constant Proportional Delta <br><br>$E(Y_{active}|X)=(1-\\delta)E(Y_{control}|X)$</td>\n      <td class=\"tg-baqh\"><span style=\"font-weight:400;font-style:normal\">$0.45$</span></td>\n      <td class=\"tg-baqh\">$(1-\\delta)r_{control} =$<br>$(1-.25)*0.45$</td>\n      <td class=\"tg-baqh\">\n      $\\Bigg(\\frac{1}{1-r^2_{control} \\; \\; \\big ( 1-\\frac{\\delta}{2}\\big)^2}-1\\Bigg)*100\\%=$<br>$\\Bigg(\\frac{1}{1-0.2\\big(1-\\frac{\\delta}{2}\\big)^2}-1\\Bigg)*100\\% =$ *$18\\%$*\n      </td>\n    </tr>\n    <tr>\n      <td class=\"tg-baqh\">No Correlation in Treatment Arm</td>\n      <td class=\"tg-baqh\"><span style=\"font-weight:400;font-style:normal\">$0.45$</span></td>\n      <td class=\"tg-baqh\">$0$</td>\n      <td class=\"tg-baqh\">\n      $\\Bigg(\\frac{1}{1-\\frac{r^2_{control}}{4}}-1\\Bigg)*100\\%=$<br>$\\Bigg(\\frac{1}{1-\\frac{0.2}{4}}-1\\Bigg)*100\\% =$ *$5\\%$*\n      </td>\n    </tr>\n  </tbody>\n</table>\n:::\n\nAssuming a constant absolute delta, the effective sample size increase from\ncovariate adjustment is expected to be $25\\%$. However, *with a constant\nproportional treatment effect (which implies HTEs), the ESSI decreases to\n$18\\%$*. A third, not entirely unrealistic worst-case scenario of no correlation\nbetween the covariate and outcome in the active treatment arm reduces the ESSI\nto just $5\\%$. The drop in ESSI with HTEs would be an important caveat to\ncommunicate if there is a temptation to decrease trial sample size.\n\nAll ESSI formulas assume a particular pattern of HTEs. Due to the risk of\nunder-powering, if no prior data is available on HTEs, we do not routinely\nrecommend reducing sample sizes for Phase 2 trials. Usually when designing a\nPhase 3 trial, there are data from earlier studies on HTEs that can be used to\nget direct estimates of HTEs and improve ESSI estimates. These improved ESSI\nestimates could be used to inform sample size reduction, if other trial\nrequirements allow for it (e.g. safety and subgroup analyses). Whenever sample\nsize is reduced, be sure to communicate these risks, consider the noise in ESSI\nestimates and study-to-study effects and tailor sample size reductions\naccordingly.\n\n# Application to Geographic Atrophy (GA)\n\nGeographic atrophy (GA) is an advanced form of age-related macular degeneration\n(AMD) and is a major cause of blindness in developed countries worldwide. GA\nprogression is assessed by GA lesion growth from baseline over time using Fundus\nAutofluorescence (FAF) images. This is usually the primary endpoint in clinical\ntrials that evaluate treatment effects on progression. \n\n![GA lesion progression over time on an FAF image](images/ga_lesion_growth.png){#fig-ga-lesion-growth}\n\nBecause GA progression is slow and varies between patients, clinical trials need\nto enroll many  patients and follow them over a long period of time. Most recent\nGA trial designs followed patients for 12-18 months for the primary efficacy\nendpoint. However, the within patient noise of measuring GA progression over\ntime is small and GA lesions progress linearly over the duration of the trial,\nmaking this a promising problem for a prediction model. This also allows us to\nuse slopes calculated for each patient as the outcome for the prognostic\nmodeling. Clinical trials often use change from baseline in GA area or slopes as\noutcomes, and sensitivity analyses showed that those are highly correlated and\nresults are very similar for both. For simplicity, we will show the results for\npredicting the individual slopes.\n\n![Examples of GA lesion growth for individual patients](images/ga_indiv_patients.png){#fig-ga-indi-patients}\n\n## Prognostic Model Development\n\nThe modeling and data strategy was pre-specified and allowed a rigorous and fair\ncomparison of a number of models with different levels of complexity and\noperational burden. The goal was to find the least complex model with the best\nperformance.\n\nData from a previous GA development program was harmonized and divided into\ntraining, hold-out, and independent test sets as shown in the figure. The\nindependent tests sets consisted of two studies that were entirely excluded from\nthe training and hold-out evaluations. Within the training data set, models were\ntrained using additional re-sampling techniques for parameter tuning and\nperformance estimation. \n\n![Data Strategy for Model Development](images/ga_data_strategy.png){#fig-ga-data-strategy}\n\nDuring training a model was selected from each of three model groups for\nhold-out and independent test set evaluation. These model groups were defined\nas:\n\n* Model Group 1 (Benchmark): The first group of models was based on a set of\n  standard, pre-specified features that are assessed at baseline. Various types\n  of models were evaluated and compared, from a simple linear model to different\n  types of machine learning models (e.g. support vector machines, gradient\n  boosted machines, random forests, etc.). The features were:\n  + Demographics: age, sex, smoking status\n  + Measures of visual acuity:  best corrected visual acuity (BCVA), low\n    luminance visual acuity (LLVA), low luminance deficit (LLD = BCVA - LLVA)\n  + Anatomical features assessed by a reading center: lesion size, lesion\n    contiguity, lesion location, lesion distance to fovea, reticular\n    pseudodrusen\n* Model Group 2 (Benchmark + Run-in): This group additionally included a\n  \"run-in\" estimate of growth rate, i.e. using the first 6 month to estimate the\n  GA growth rate and predicting the future. Note that this required\n  re-baselining the remaining outcome data to the 6 month time point. Due to the\n  linear growth and low within patient noise, this is a promising approach but\n  operationally complex to include in a trial as it requires high quality\n  historical images from patients, or starting the trial with a pre-treatment\n  run-in period, which may slow trial recruitment and delay the trial readout. \n* Model Group 3 (DL Models): The third group of models consists of end-to-end\n  deep learning (DL) models from FAF and also OCT images at baseline. Three\n  types of models were developed (each exploring different architectures): using\n  FAF only, using OCT only and using FAF and OCT together [@DLpaper]. The FAF\n  only model would be the operationally preferred model.\n\nOne model was selected from each model group using the relevant performance\nmetrics ($r^2$ and ESSI) estimated using re-sampling techniques. The three\nselected models were then tested and compared on the holdout and independent\ntest sets.\n\n## Results\n\nThe selected models were:\n\n* Model Group 1 (Benchmark): A simple linear model with four features (lesion\n  size, lesion contiguity, lesion distance to fovea and LLD) gave the best\n  performance. More complex models like tree based models or added features did\n  not add performance.\n* Model Group 2 (Benchmark + Run-in): A simple linear model with the four\n  features from model group 1 and an estimate of growth rate as an additional\n  fifth feature.\n* Model Group 3 (DL Models): A multi-task DL (CNN) model that uses a single FAF\n  image as input to predict GA lesion size (same image) and GA growth rate\n  [@DLpaper].\n\nThe results were quite impressive; although operationally complex to implement,\nthe run-in model appears to effectively increase the sample size by $1/3$\ncompared to an analysis using the benchmark model. If no DL FAF model were\navailable, the additional complexity of the run-in might be justified by the\n$ESSI$. However, the DL FAF model outperformed the benchmark and even the\nbenchmark+run-in model in the hold-out as well as the two independent test sets\n[@DLpaper]. This implies that the logistical complexities of the run-in model\ncan be avoided with the DL FAF model, which is based on a single baseline time\npoint and is still relatively simple to implement [@DLpaper]. The table shows\nthat the DL FAF model substantially increases the ESSI by at least 90% when\ncompared to an unadjusted analysis and by 40%-80% compared to a simple\nadjustment using the known baseline features.\n\n![Results from selected model per group](images/ga_model_results.png){#fig-ga-model-results}\n\n## Model in Action\n\nThe MAHALO study independent test set shown in the previous section was the\nPhase 2 trial for Lampalizumab. There was an observed treatment effect of ∼20%\nslowing progression in the higher dose (LQ4 - monthly treatment) compared to the\nSham (no treatment) arm. This effect was not confirmed by the two large global\nPhase 3 trials (SPECTRI and CHROMA), so in retrospect we know this was a\nfalse-positive result. When the decision was made to move forward with a Phase 3\ntrial, the DL FAF model was not available. However, it is an interesting\npost-hoc exercise to see how results from the Phase 2 trial would have changed\nhad the DL FAF model been used for covariate adjustment.\n\n@fig-ga-mahalo-ex shows the estimated mean changes in GA lesion size with and\nwithout adjusting for the FAF DL model. Covariate adjustment leads to better\nprecision and hence tighter confidence intervals for the treatment effect\nestimate and also changes the estimate in case of imbalances. In this example,\nthe estimate changes (from 20% to 6%) and becomes non-significant (a p-value\nthreshold of 0.2 is usually used in a Phase 2 trial). \n\n![GA progression over time by treatment arm for unadjusted and adjusted analysis](images/ga_mahalo_example.png){#fig-ga-mahalo-ex}\n\n![Treatment effect estimates](images/ga_mahalo_cis.png){#fig-ga-mahalo-cis}\n\nWhile there were many other considerations in the decision to move into Phase 3\ntrials (e.g. subgroup analyses), if results from the adjusted analysis had been\navailable, Phase 3 decision making may have been different.\n\n# Brief Recap\n\nBelow is a brief outline summarizing how to get the most out of prognostic\nbaseline variables via covariate adjustment:\n\n* Analyze external/historical data\n  + build a score/model\n  + evaluate with ESSIs\n* Determine model budget\n  + use methods developed for prognostic modeling\n* Spend the model budget wisely\n  + choose the most prognostic factors among stratification and other prognostic variables\n  + avoid dichotomania\n  + by default, do not use interactions\n* Estimate ATEs and TAMs\n  + use standardized estimator\n  + also works for non-continuous outcomes\n* Inference for ATEs and TAMs\n  + suggest SE estimates from TMLE literature\n  + re-sampling methods are also an option\n\n\n# Discussion\n\nCovariate adjustment is a statistical free lunch. You can enjoy a rigorous,\nmodel-assisted analysis that increases precision and power, accounts for\nconditional bias and is supported by regulatory authorities at virtually no\ncost. With the growing availability of high-quality data for the identification\nof robust prognostic covariates, the benefits of an adjusted analysis will only\ncontinue to increase. We focused here on continuous endpoints, however the same\nstandardized estimator and variance estimates can be applied to non-continuous\noutcomes (e.g. binary or ordinal) by substituting in the appropriate generalized\nlinear \"working\" model and making minor adaptations in the standard error\nestimates [@TMLE2010]. ESSI formulas for non-continuous outcomes are a\nwork-in-progress. While the concept of covariate adjustment has been around for\ndecades, it seems to be underutilized or sub-optimally implemented (e.g. poor\nchoice of covariates). We hope this guidance makes it clear how to rigorously\nimplement covariate adjustment in randomized trials and encourages statisticians\nto take advantage of all of the benefits of adjusted analyses for randomized\nclinical trials.\n\n# Acknowledgements\n\n**Roche GA Development Team**: Verena Steffen, Neha Anegondi, Daniela Ferrara, Simon Gao, Qi Yang, Julia Cluceru, Lee Honigberg\n\n**Methods, Collaboration, Outreach (MCO)**: Ray Lin, Marcel Wolbers\n\n**Website design**: Doug Kelkhoff \n\n# Appendix\n\n## ESSI Formulas\n\n* $Y$ continuous outcome, $X$ baseline covariate\n* **Constant absolute $\\delta$**: $E(Y_{active}|X)=E(Y_{control}|X)-\\delta$\n* **Constant proportional $\\delta$**: $E(Y_{active}|X)=(1-\\delta)E(Y_{control}|X)$\n* $r_{control}$ and $r_{active}$: correlation between $Y$ and $X$ given control and active treatment, respectively.\n* $k=\\frac{\\sigma_{Y_{active}}}{\\sigma_{Y_{control}}}$\n* $\\pi=Probability\\;of\\;active\\;treatment$. Note, when $\\pi=1/2$ the ESSI corresponding to the additive model will always equal the ESSI corresponding to the interaction model.\n\n\n![ESSI Formulas](images/essi_formulas.png){#fig-essi-formulas}\n\n## Huber White Sandwich Estimator\n\nBelow is a simple simulation demonstrating how the Huber-White robust\n“sandwich” estimator will give standard error estimates that are too small when\ntreatment effects are heterogeneous and a working regression model with\ntreatment-by-covariate interactions is used.\n\n\n::: {.cell hash='index_cache/html/hws_est_superpopulation_17a9076332907831554d36a56fc2c6ab'}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(sandwich)\nlibrary(doParallel)\nlibrary(foreach)\nregisterDoParallel(cores=4)\n\n################### SIMULATION STUDIES\n\n####### Paradigm 1: Superpopulation\n\n# Sample of size N from a population of covariates\n# Each patient's treatment assignment is a Bernoulli(1/2) random variable\n# Then generate a random outcome from the model using the covariate value \n# and treatment for all N patients\n\na <- -3\nb <- 5\nc <- 2\nd  <- 5\n\nvar.eps0 <- var.eps1 <-  .6\n\np <- 1 / 2\nN <- 10000\nnsamp <- 10000\n\np1.estimates <- foreach(i=1:nsamp,.combine = rbind) %dopar% {\n  X.sim <- runif(N,0,1) # sample covariate\n  X.sim.center <- X.sim - mean(X.sim) # center covariate\n\n  # outcome under control\n  y0.sim  <-  a*X.sim + b + rnorm(N,0,sqrt(var.eps0))\n\n  # outcome under treatment\n  y1.sim  <-   c*X.sim + d + rnorm(N,0,sqrt(var.eps1))\n\n  A.sim <- rbinom(N,1,p)\n\n  Y.sim <- y1.sim*A.sim + y0.sim*(1-A.sim)\n\n  df.sim <- data.frame(Y.sim, A.sim, X.sim, X.sim.center) # A=0,1,2,...treatment arm factor\n\n  ## Additive Model\n\n  lm.fit <- lm(Y.sim ~ A.sim + X.sim, data = df.sim)\n\n  lm.se <- summary(lm.fit)[[4]][2,2]\n\n  # set treatment indicator to active treatment for all subjects\n  pred1 <- predict(lm.fit, newdata = df.sim %>% mutate(A.sim=1))\n\n  # set treatment indicator to control for all subjects\n  pred0 <- predict(lm.fit, newdata = df.sim %>% mutate(A.sim=0)) \n\n  ate.add <- mean(pred1) - mean(pred0)\n\n  ate.add.se <- sqrt(mean((A.sim*(Y.sim-pred1)/mean(A.sim==1) + pred1 -\n                mean(pred1)-((1-A.sim)*(Y.sim-pred0)/mean(A.sim==0) + \n                               pred0 - mean(pred0)))^2))/sqrt(N)\n\n  ## Interaction Model\n\n  lm.fit <- lm(Y.sim ~ A.sim*X.sim, data = df.sim)\n\n  # set treatment indicator to active treatment for all subjects\n  pred1 <- predict(lm.fit, newdata = df.sim %>% mutate(A.sim=1))\n\n  # set treatment indicator to control for all subjects\n  pred0 <- predict(lm.fit, newdata = df.sim %>% mutate(A.sim=0))\n\n  ate.int <- mean(pred1) - mean(pred0)\n\n  ate.int.se <- sqrt(mean((A.sim*(Y.sim-pred1)/mean(A.sim==1) + pred1 - \n                             mean(pred1)-((1-A.sim)*(Y.sim-pred0)/mean(A.sim==0) + \n                                            pred0 - mean(pred0)))^2))/sqrt(N)\n\n  ## HW SEs\n\n  lm.fit <- lm(Y.sim ~ A.sim + X.sim, data = df.sim)\n  hw.add <- sqrt(vcovHC(lm.fit, type=\"HC0\")[2,2])\n\n  lm.fit <- lm(Y.sim ~ A.sim*X.sim.center, data = df.sim)\n  hw.int <- sqrt(vcovHC(lm.fit, type=\"HC0\")[2,2]) \n\n  lm.fit <- lm(Y.sim ~ A.sim*X.sim, data = df.sim)\n  ate <- coefficients(lm.fit)[2] + coefficients(lm.fit)[4]*mean(df.sim$X.sim)\n  myvcov <- vcovHC(lm.fit, type=\"HC0\")\n  ate.sd <- sqrt(mean(df.sim$X.sim)^2*myvcov[4,4]+myvcov[2,2] + 2*mean(df.sim$X.sim)*myvcov[2,4])\n\n\n  return(c(ate.add, ate.int, ate.add.se, ate.int.se, hw.add, hw.int, lm.se))\n}\n\np1.se.truth <- sqrt(apply(p1.estimates[,1:2],2,var))\n\ntmle.mean.se <- apply(p1.estimates[,3:4],2,mean)\n\nhw.mean.se <- apply(p1.estimates[,5:6],2,mean)\n\nt <- t(data.frame(truth = p1.se.truth, tmle = tmle.mean.se, HW = hw.mean.se))\n\nrownames(t) <- c(\"True SE\",\"Mean TMLE SE\",\"Mean HW SE\")\n\ncolnames(t) <- c(\"Additive Working Regression Model\",\"Interaction Working Regression Model\")\n\nknitr::kable(t, align = \"cc\", caption = \"Superpopulation Sampling Framework\")\n```\n\n::: {.cell-output-display}\nTable: Superpopulation Sampling Framework\n\n|             | Additive Working Regression Model | Interaction Working Regression Model |\n|:------------|:---------------------------------:|:------------------------------------:|\n|True SE      |             0.0213702             |              0.0213635               |\n|Mean TMLE SE |             0.0211727             |              0.0211716               |\n|Mean HW SE   |             0.0211753             |              0.0154888               |\n:::\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n# References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}