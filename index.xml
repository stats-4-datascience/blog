<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>stats&lt;span class=&#39;fancy-four&#39;&gt;4&lt;/span&gt;datascience</title>
<link>https://github.com/stats-4-datascience/blog/index.html</link>
<atom:link href="https://github.com/stats-4-datascience/blog/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.2.269</generator>
<lastBuildDate>Tue, 31 Oct 2023 00:00:00 GMT</lastBuildDate>
<item>
  <title>Everything you wanted to know about R2 but were afraid to ask</title>
  <dc:creator>Michel Friesenhahn</dc:creator>
  <dc:creator>Christina Rabe</dc:creator>
  <dc:creator>Courtney Schiffman</dc:creator>
  <link>https://github.com/stats-4-datascience/blog/posts/three_metrics_binary/index.html</link>
  <description><![CDATA[ 




<div class="cell">

</div>
<center>
<img src="https://github.com/stats-4-datascience/blog/posts/three_metrics_binary/images/BlogPic.png" id="fig-magnify" class="img-fluid" style="width:60.0%">
</center>
<p><img src="https://latex.codecogs.com/png.latex?%5C%5C%5B.02in%5D"> Below is the second part of a two part post on model performance evaluation and the fundamental decomposition of <img src="https://latex.codecogs.com/png.latex?R%5E2">. <a href="https://schiffmc.pages.roche.com/stats4datascience/posts/three_metrics/">Part 1 of this post</a> discussed how the general quality of predictions is assessed by accuracy, calibration, and discrimination. An approach was proposed that unifies these concepts via a scaled decomposition of accuracy into miscalibration and discrimination, <img src="https://latex.codecogs.com/png.latex?R%5E2%20=%20%5Cmathrm%7BDI%7D%20-%20%5Cmathrm%7BMI%7D">. In Part 1 it was discovered that the three key metrics are <img src="https://latex.codecogs.com/png.latex?R%5E2">, <img src="https://latex.codecogs.com/png.latex?r%5E2">, and <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D">, that they satisfy a key inequality, <img src="https://latex.codecogs.com/png.latex?R%5E2%20%5Cleq%20r%5E2%20%5Cleq%20DI"> and that <img src="https://latex.codecogs.com/png.latex?R%5E2"> can indeed be negative!</p>
<p>We now discuss the binary outcome setting and how the fundamental decompostion can be generalized…</p>
<section id="binary-outcomes" class="level1">
<h1>Binary Outcomes</h1>
<p>When outcomes are binary, <img src="https://latex.codecogs.com/png.latex?Y%5Cin%5C%7B0,1%5C%7D">, <strong>the metrics and decomposition discussed in <a href="https://schiffmc.pages.roche.com/stats4datascience/posts/three_metrics/">Part 1 of this post</a> that were based on quadratic loss still apply without change</strong>. Importantly, the interpretations of the metrics hold for binary outcomes as well:</p>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Interpretations of <img src="https://latex.codecogs.com/png.latex?R%5E2"> and <img src="https://latex.codecogs.com/png.latex?DI"> as Proportions of Explained Uncertainty
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D"> is the proportion of variability in the observed outcomes explained by recalibrated predictions.</p></li>
<li><p>If predictions are calibrated, then <img src="https://latex.codecogs.com/png.latex?R%5E2%20=%20%5Cmathrm%7BDI%7D=Var(C(%5Chat%7BY%7D))%20/%20Var(Y)=%20Var(%5Chat%7BY%7D)%20/%20Var(Y)">, so <img src="https://latex.codecogs.com/png.latex?R%5E2"> is the proportion of variability in the observed outcomes explained by the predictions.</p></li>
<li><p>If predictions are not calibrated, then <img src="https://latex.codecogs.com/png.latex?R%5E2"> cannot be interpreted as the proportion of variability in the observed outcomes explained by the predictions.</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?R%5E2"> is always the proportional reduction of <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMSE%7D"> relative to the best constant prediction, <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D(Y)">.</p></li>
</ul>
<p><strong>These interpretations still apply when outcomes are binary and take values in {0, 1}</strong>!</p>
</div>
</div>
<p>Recall that quadratic loss is a strictly proper scoring rule (defined in more detail below). Remarkably, it has been shown that the decomposition into miscalibration and discrimination applies more generally to all strictly proper scoring rules, not just quadratic loss, and still retains its general interpretation <span class="citation" data-cites="Brocker">(Brocker, 2009)</span>. An alternative strictly proper scoring rule that is commonly used for binary outcomes is the log loss–sometimes referred to as cross entropy.</p>
</section>
<section id="scoring-rules" class="level1 page-columns page-full">
<h1>Scoring Rules</h1>
<p>A <strong>scoring rule</strong> takes a predicted outcome <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D"> and a realized outcome <img src="https://latex.codecogs.com/png.latex?y"> and assigns a real valued score <img src="https://latex.codecogs.com/png.latex?s(%5Chat%7By%7D,%20y)">. Scoring rules can have negative or positive orientation, where negative orientation refers to larger scores representing worse discrepancies between <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D"> and <img src="https://latex.codecogs.com/png.latex?y">. We assume negative orientation so that scores represent a loss. The <strong>score</strong>, <img src="https://latex.codecogs.com/png.latex?S(%5Chat%7BY%7D,%20Y)"> is defined as the expected value of the scoring rule, <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5Bs(%5Chat%7BY%7D,%20Y)%5D">. For example, <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMSE%7D"> is the score based on the quadratic loss scoring rule. In the binary outcome setting, <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMSE%7D"> is often called the Brier Score, first proposed in 1950 for applications in meteorology <span class="citation" data-cites="Brier1950">(Brier, 1950)</span>.</p>
<p>A scoring rule is <strong>proper</strong> if the score based on that scoring rule is optimized at <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5BY%5D">. It is strictly proper if it is uniquely optimized there. In other words, scores that are based on strictly proper scoring rules are optimized when predictions are equal to the true expected values (probabilities in the case of binary outcomes), so strictly proper scoring rules incentivize accurate predictions.</p>
<p>Any score that is based on a strictly proper scoring rule can be decomposed into calibration and discrimination components. To show this, we first define the following terms which are also shown mathematically in Table&nbsp;1. This is simply meant to be a brief sketch of these concepts, and for more details see <span class="citation" data-cites="Brocker">Brocker (2009)</span>:</p>
<ol type="1">
<li><p>Uncertainty: <img src="https://latex.codecogs.com/png.latex?S(%5Cmathbb%7BE%7D(Y),Y)">. Uncertainty is the score evaluated at the constant prediction <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D=%5Cmathbb%7BE%7D(Y)">. In general, uncertainty measures the spread of the outcomes and does not depend on the prediction model. When the scoring rule is quadratic loss, uncertainty is the variance of <img src="https://latex.codecogs.com/png.latex?Y"> which is simply <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D(Y)(1-%5Cmathbb%7BE%7D(Y))=p(1-p)"> for binary outcomes. When the scoring rule is log loss, uncertainty is the same as Shannon’s information entropy. Figure&nbsp;1 shows uncertainty as a function of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D(Y)"> for quadratic and log loss scoring rules. These functions are both unimodal and symmetric about <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D(Y)%20=%20%5Cfrac%7B1%7D%7B2%7D">, with minimum of zero uncertainty at the two extremes of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D(Y)"> equal to 0 or 1.</p></li>
<li><p>Miscalibration: <img src="https://latex.codecogs.com/png.latex?S(%5Chat%7BY%7D,Y)%20-%20S(C(%5Chat%7BY%7D),Y)">. Miscalibration is the difference between scores evaluated at the original and recalibrated predictions. It is also often referred to as reliability. Using language from information geometry, miscalibration is the expected divergence between the predictions, <img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D">, and the recalibrated predictions, <img src="https://latex.codecogs.com/png.latex?C(%5Chat%7BY%7D)">. In information geometry, expected divergence is a type of statistical distance, so miscalibration measures the distance between the original and recalibrated predictions. For quadratic loss, the divergence is simply squared Euclidean distance; for log loss, the divergence is Kullback-Leibler divergence.</p></li>
<li><p>Discrimination: <img src="https://latex.codecogs.com/png.latex?S(E(Y),Y)%20-%20S(C(%5Chat%7BY%7D),Y)">. Discrimination, also often referred to as resolution, measures the distance between the best constant prediction <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D(Y)"> and the recalibrated predictions. Therefore, discrimination is a measure of spread in the recalibrated predictions about <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D(Y)">. For quadratic loss, this measure of spread is simply the variance of the recalibrated predictions; for log loss, it is the Kullback-Leibler divergence between the constant prediction of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D(Y)"> relative to the recalibrated predictions. Note that Uncertainty, <img src="https://latex.codecogs.com/png.latex?S(E(Y),Y)">, which is the first term of the difference, is not affected by the prediction model. The second term of the difference, <img src="https://latex.codecogs.com/png.latex?S(C(%5Chat%7BY%7D),Y)">, is minimized and thus discrimination is maximized when recalibrated predictions are at the extremes of 0 or 1. Intuitively this makes sense since we would want to say that a model with many calibrated predicted probabilities close to the extremes is able to discriminate well between observations.</p></li>
</ol>
<div class="cell">
<div class="cell-output-display">
<div id="fig-uncertainty" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://github.com/stats-4-datascience/blog/posts/three_metrics_binary/index_files/figure-html/fig-uncertainty-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: Uncertainty for quadratic and log loss scoring rules</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="column-page-inset">
<div id="tbl-scorerules" class="anchored">
<table class="table">
<caption>Table&nbsp;1: Miscalibration, discrimination and uncertainty under general, squared error and log loss scoring rules</caption>
<colgroup>
<col style="width: 8%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 59%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">General</th>
<th style="text-align: center;">Quadratic Loss</th>
<th style="text-align: center;">Log Loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>Scoring rule</strong></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?s(%5Chat%7By%7D,y)"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?(%5Chat%7By%7D-y)%5E2"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?-%5By*log(%5Chat%7By%7D)+(1-y)*log(1-%5Chat%7By%7D)%5D"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>Score</strong></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5Bs(%5Chat%7BY%7D,Y)%5D"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5B(%5Chat%7BY%7D-Y)%5E2%5D"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?-%5Cmathbb%7BE%7D%5BY*log(%5Chat%7BY%7D)+(1-Y)*log(1-%5Chat%7BY%7D)%5D"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>Uncertainty</strong></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?S(%5Cmathbb%7BE%7D(Y),Y)"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D(Y)(1-%5Cmathbb%7BE%7D(Y))"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?-%5B%5Cmathbb%7BE%7D(Y)*log(%5Cmathbb%7BE%7D(Y))+(1-%5Cmathbb%7BE%7D(Y))*log(1-%5Cmathbb%7BE%7D(Y))%5D"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>Miscalibration</strong></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?S(%5Chat%7BY%7D,Y)-S(C(%5Chat%7BY%7D),Y)"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5B(%5Chat%7BY%7D-C(%5Chat%7BY%7D))%5E2%5D"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?-%5Cmathbb%7BE%7D%5CBig%5BC(%5Chat%7BY%7D)*log%5CBig(%5Cfrac%7B%5Chat%7BY%7D%7D%7BC(%5Chat%7BY%7D)%7D%5CBig)+(1-C(%5Chat%7BY%7D))*log%5CBig(%5Cfrac%7B1-%5Chat%7BY%7D%7D%7B1-C(%5Chat%7BY%7D)%7D%5CBig)%5CBig%5D"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>Discrimination</strong></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?S(%5Cmathbb%7BE%7D(Y),Y)-S(C(%5Chat%7BY%7D),Y)"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?Var(C(%5Chat%7BY%7D))"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?-%5Cmathbb%7BE%7D%5BC(%5Chat%7BY%7D)*log%5CBig(%5Cfrac%7B%5Cmathbb%7BE%7D(Y)%7D%7BC(%5Chat%7BY%7D)%7D%5CBig)+(1-C(%5Chat%7BY%7D))*log%5CBig(%5Cfrac%7B1-%5Cmathbb%7BE%7D(Y)%7D%7B1-C(%5Chat%7BY%7D)%7D%5CBig)%5D"></td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="generalized-fundamental-decomposition" class="level1">
<h1>Generalized Fundamental Decomposition</h1>
<p>For strictly proper scoring rules, the Fundamental Decomposition <a href="https://schiffmc.pages.roche.com/stats4datascience/posts/three_metrics/">discussed in Part 1 of this post</a> can then be generalized to:</p>
<div class="callout-definition callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Generalized Fundamental Decomposition
</div>
</div>
<div class="callout-body-container callout-body">
<p><span id="eq-gendecomp"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AR%5E2%20=%20DI%20-%20MI%0A%5Cend%7Balign%7D%0A%5Ctag%7B1%7D"></span> where</p>
<p><img src="https://latex.codecogs.com/png.latex?R%5E2%20=%201%20-%20%5Cfrac%7BS(%5Chat%7BY%7D,Y)%7D%7BS(%5Cmathbb%7BE%7D(Y),Y)%7D"></p>
<p><img src="https://latex.codecogs.com/png.latex?DI=%5Cfrac%7BS(E(Y),Y)%20-%20S(C(%5Chat%7BY%7D),Y)%7D%7BS(%5Cmathbb%7BE%7D(Y),Y)%7D"></p>
<p><img src="https://latex.codecogs.com/png.latex?MI=%5Cfrac%7BS(%5Chat%7BY%7D,Y)%20-%20S(C(%5Chat%7BY%7D),Y)%7D%7BS(%5Cmathbb%7BE%7D(Y),Y)%7D"></p>
</div>
</div>
<p>Note that <img src="https://latex.codecogs.com/png.latex?R%5E2"> is simply the score expressed as a <strong>skill score</strong> for the prediction model relative to the optimal null model <span class="citation" data-cites="Fissler">(Fissler et al., 2022)</span>.</p>
<p>With the metrics and decomposition now defined under a general proper scoring rule, we re-emphasize that all properties described in the preceding sections still apply for log loss scoring. In addition, the interpretations of the metrics <a href="https://schiffmc.pages.roche.com/stats4datascience/posts/three_metrics/">discussed previously in Part 1 of this post</a> also apply with suitable modifications:</p>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Interpretations of <img src="https://latex.codecogs.com/png.latex?R%5E2"> and <img src="https://latex.codecogs.com/png.latex?DI"> for any proper scoring rule
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D"> is the spread of the recalibrated predictions divided by the spread of the original outcomes. Therefore, it can be interpreted as the proportion of uncertainty explained by the recalibrated predictions</p></li>
<li><p>If predictions are calibrated, then <img src="https://latex.codecogs.com/png.latex?R%5E2%20=%20%5Cmathrm%7BDI%7D">, so <img src="https://latex.codecogs.com/png.latex?R%5E2"> is the proportion of uncertainty explained by the predictions.</p></li>
<li><p>If predictions are not calibrated, then <img src="https://latex.codecogs.com/png.latex?R%5E2"> cannot be interpreted as the proportion of uncertainty explained by the predictions.</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?R%5E2"> is always the proportional reduction in score relative to the best constant prediction, <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D(Y)">.</p></li>
</ul>
</div>
</div>
<p>We note that in practice these metrics can all be estimated using straightforward empirical estimators similar to those described for quadratic loss (see section on estimation <a href="https://schiffmc.pages.roche.com/stats4datascience/posts/three_metrics/">discussed in Part 1 of this post</a>).</p>
</section>
<section id="comments-on-log-loss" class="level1">
<h1>Comments on log loss</h1>
<p>Because of the connections to maximum likelihood estimation and information theory, some researchers prefer log loss to quadratic loss for binary outcomes. We note two issues with log loss scoring. The first is that log loss is not defined if <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D%20=%200"> and <img src="https://latex.codecogs.com/png.latex?y%20=%201">, even in a large data set. So log loss should be restricted to the evaluation of models that will never make predictions that are exactly 0. The second issue is that metrics based on quadratic loss are arguably more interpretable or at least familiar to many researchers and data scientists. Both quadratic and log loss are frequently used for binary outcomes and we recommend either (better yet, both if possible) for performance evaluation.</p>
<p>Since Brier scores are equivalent to MSE, the definition of a calibration line for quadratic loss scoring rules is the same as defined in the preceding sections. However, note that since the outcomes are binary, the calibration line is not guaranteed to have predictions between 0 and 1. Therefore we recommend being cautious about the use of calibration lines when using quadratic loss.</p>
<p>Recall that the calibration line was <a href="https://schiffmc.pages.roche.com/stats4datascience/posts/three_metrics/">previously defined</a> as the linear transformation of the predictions that maximizes the quadratic loss-based <img src="https://latex.codecogs.com/png.latex?R%5E2">. If log loss is used instead, the calibration line is then defined as the population logistic regression of outcome versus <img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D"> and in this case <img src="https://latex.codecogs.com/png.latex?logit(%5Cmathbb%7BE%7D(Y%7C%5Chat%7BY%7D))"> would be some linear function of <img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D">. However, this is problematic, since if the model is calibrated then <img src="https://latex.codecogs.com/png.latex?logit(%5Cmathbb%7BE%7D(Y%7C%5Chat%7BY%7D))=logit(%5Chat%7BY%7D)">, which is not a linear function of <img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D">, let alone one with intercept 0 and slope 1. This issue is solved by performing a population logistic regression of outcome on <img src="https://latex.codecogs.com/png.latex?logit(%5Chat%7BY%7D)"> rather than on <img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D">, which is usually how an appropriate calibration line is defined for log loss.</p>
<p>Figure&nbsp;2 shows that when <img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D"> is logit transformed prior to fitting the logistic regression (i.e.&nbsp;both conditional outcome probabilities <img src="https://latex.codecogs.com/png.latex?C(%5Chat%7BY%7D)"> and <img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D"> are logit transformed), calibration lines will actually be lines. However, on the original probability scale calibration lines are actually curves (except when the predictions are perfectly calibrated).</p>
<div id="fig-logit" class="cell quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-logit-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://github.com/stats-4-datascience/blog/posts/three_metrics_binary/index_files/figure-html/fig-logit-1.png" class="img-fluid figure-img" data-ref-parent="fig-logit" width="768"></p>
<p></p><figcaption class="figure-caption">(a) Logit-logit calibration line</figcaption><p></p>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-logit-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://github.com/stats-4-datascience/blog/posts/three_metrics_binary/index_files/figure-html/fig-logit-2.png" class="img-fluid figure-img" data-ref-parent="fig-logit" width="768"></p>
<p></p><figcaption class="figure-caption">(b) Corresponding calibration line on original scales</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;2: Calibration lines for log loss</figcaption><p></p>
</figure>
</div>
</section>
<section id="comments-on-misclassification-error" class="level1">
<h1>Comments on misclassification error</h1>
<p>Finally, we note that misclassification error is also a commonly used metric for probabilistic predictions of binary outcomes. It has been noted that this metric is a score based on a proper scoring rule <span class="citation" data-cites="Dimitriadis">(Dimitriadis et al., 2021)</span>: <img src="https://latex.codecogs.com/png.latex?I(%5Chat%7By%7D%3C1/2,y=1)+I(%5Chat%7By%7D%3E1/2,y=0)+%20%5Cfrac%7B1%7D%7B2%7D%20I(%5Chat%7By%7D=1/2)">.</p>
<p>This scoring rule dichotomizes the probabilistic predictions via a threshold at 1/2 and the proportion of discrepancies with the outcome is the misclassification error metric. However it is not strictly proper since it only matters if the predicted probability is above or below the decision threshold. Misclassification error will not change if the probabilistic predictions are adjusted arbitrarily, as long as those adjustments do not cross the decision threshold. Consequently misclassification error is not uniquely optimized at the true probabilities. Since misclassification error is not a strictly proper scoring rule, a decomposition into calibration and discrimination cannot meaningfully be used for that scoring rule.</p>
<p>More importantly, in most applications it is the predicted probabilities that are of interest so that individual users of the model can make their own decisions on what actions to take. In such settings it is premature for the builder of the prediction model to use an arbitrary cut point, such as 1/2, as a decision threshold. An exception to this would be for prediction models that have nearly perfect accuracy for the binary outcomes and where costs of the few misclassifications are roughly the same for predicted events vs predicted non events. An example of this would be modern optical character recognition systems. However, most applications do not satisfy such prerequisites and we do not recommend the use of misclassification error as a performance metric. Frank Harrell’s insightful <a href="https://www.fharrell.com/">blog</a> on statistical thinking has further discussion of this topic.</p>
</section>
<section id="pseudo-r2" class="level1">
<h1>Pseudo <img src="https://latex.codecogs.com/png.latex?R%5E2"></h1>
<p>In parallel with the development of metrics based on scoring rules, there has been considerable work to develop pseudo-<img src="https://latex.codecogs.com/png.latex?R%5E2">s that are applicable to binary outcomes (see <span class="citation" data-cites="Mittlbock">MITTLBOCK et al. (1996)</span> for a nice summary). We review some of them below and place them in context of what we have learned about quadratic and log loss-based metrics. Pseudo-<img src="https://latex.codecogs.com/png.latex?R%5E2"> metrics are usually calculated in-sample. However, we will comment on the more relevant population performance when these are applied out of sample as we have done in the rest of the blog:</p>
<ul>
<li><p>Sum of Squares <img src="https://latex.codecogs.com/png.latex?R%5E2"> (<img src="https://latex.codecogs.com/png.latex?R%5E2_%7BSS%7D">): This is equivalent to the <img src="https://latex.codecogs.com/png.latex?R%5E2"> based on quadratic loss.</p></li>
<li><p>Entropy <img src="https://latex.codecogs.com/png.latex?R%5E2"> (<img src="https://latex.codecogs.com/png.latex?R%5E2_E">): This is the same as the <img src="https://latex.codecogs.com/png.latex?R%5E2"> based on the log loss scoring rule. It was proposed by Theil (1970) and is equivalent to McFadden’s <img src="https://latex.codecogs.com/png.latex?R%5E2"> <span class="citation" data-cites="Theil McFadden">(McFadden, 1974; Theil, 1970)</span>.</p></li>
<li><p>Gini’s Concentration Measure <img src="https://latex.codecogs.com/png.latex?R%5E2"> (<img src="https://latex.codecogs.com/png.latex?R%5E2_G">): This is the expected nominal variance of the conditional Bernoulli outcomes. If predictions are calibrated, this is the same as <img src="https://latex.codecogs.com/png.latex?R%5E2%20=%20DI"> for quadratic loss. If predictions are not calibrated there is no guarantee <img src="https://latex.codecogs.com/png.latex?R%5E2_G"> represents anything meaningful.</p></li>
<li><p>Tjur’s Coefficient of Discrimination <img src="https://latex.codecogs.com/png.latex?R%5E2"> (<img src="https://latex.codecogs.com/png.latex?R%5E2_T">) <span class="citation" data-cites="Tjur">(Tjur, 2009)</span>: This is the mean of the predictions for all events minus the mean of the predictions for all non events: <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D(%5Chat%7BY%7D%20%7C%20Y%20=%201)%20-%20%5Cmathbb%7BE%7D(%5Chat%7BY%7D%20%7C%20Y%20=%200)">. We note it is also the same as discrimination slope <span class="citation" data-cites="Pencina2008">(Pencina et al., 2008)</span>. Surprisingly, if the predictions are calibrated then this is the same as <img src="https://latex.codecogs.com/png.latex?R%5E2%20=%20DI"> for the quadratic loss scoring rule <span class="citation" data-cites="Huang2007">(Pepe et al., 2008)</span>. If predictions are not calibrated, there is no guarantee this represents anything meaningful. For example, all predictions can be multiplied by a small positive scalar, <img src="https://latex.codecogs.com/png.latex?k">. These transformed scores should have the same discrimination as the original predictions, but <img src="https://latex.codecogs.com/png.latex?R%5E2_T"> will be multiplied by that same factor, <img src="https://latex.codecogs.com/png.latex?k">.</p></li>
<li><p>Cox and Snell <img src="https://latex.codecogs.com/png.latex?R%5E2"> (<img src="https://latex.codecogs.com/png.latex?R%5E2_%7BCS%7D">): This is <img src="https://latex.codecogs.com/png.latex?1%20-%20%5Cbig(%5Cfrac%7B%5Cmathbb%7BL%7D_%7Bnull%7D%7D%7B%5Cmathbb%7BL%7D_%7Bmodel%7D%7D%5Cbig)%5E%7B(2/n)%7D">, also known as the Maximum Likelihood <img src="https://latex.codecogs.com/png.latex?R%5E2">. If the normal theory based likelihood is used for regression, then this is equivalent to the usual in-sample OLS <img src="https://latex.codecogs.com/png.latex?R%5E2">. It is therefore viewed as a generalization of <img src="https://latex.codecogs.com/png.latex?R%5E2"> applicable to a variety of distributions. However, for binary outcomes, the upper limit of this pseudo-<img src="https://latex.codecogs.com/png.latex?R%5E2"> is strictly less than 1, even with perfect predictions. It also doesn’t correspond to the skill score of a strictly proper scoring rule so the fundamental decomposition and interpretations do not apply to it.</p></li>
<li><p>Nagelkerke <img src="https://latex.codecogs.com/png.latex?R%5E2"> (<img src="https://latex.codecogs.com/png.latex?R%5E2_N">): This rescales the Cox and Snell pseudo <img src="https://latex.codecogs.com/png.latex?R%5E2_%7BCS%7D"> by its maximum value, <img src="https://latex.codecogs.com/png.latex?U%20=%201%20-%20%5Cmathbb%7BL%7D_%7Bnull%7D%5E%7B(2/n)%7D">. So <img src="https://latex.codecogs.com/png.latex?R%5E2_N%20=%20R%5E2_%7BCS%7D%20/%20U"> now has a maximum value of 1 which is achieved when all predictions are perfect. However, it still has the disadvantage of the Cox and Snell pseudo-<img src="https://latex.codecogs.com/png.latex?R%5E2"> that it does not correspond to the skill score of a strictly proper scoring rule.</p></li>
</ul>
<p>We prefer the metrics suggested in this blog over both <img src="https://latex.codecogs.com/png.latex?R%5E2_%7BCS%7D"> and <img src="https://latex.codecogs.com/png.latex?R%5E2_N"> due to the greater interpretation provided by the Fundamental Decomposition and interpretation as Proportion of Explained Uncertainty, potentially after suitable recalibration. The other pseudo-<img src="https://latex.codecogs.com/png.latex?R%5E2">s, when viewed as out of sample population parameters represent either <img src="https://latex.codecogs.com/png.latex?R%5E2"> or <img src="https://latex.codecogs.com/png.latex?DI">, potentially after suitable recalibration, for either quadratic or log Loss, so they do not add anything to the metrics we have proposed. We note also that this is not an exhaustive list of pseudo-<img src="https://latex.codecogs.com/png.latex?R%5E2">s, and there are additional variations, for example, to account for over-fitting <span class="citation" data-cites="Pseudo">(Hemmert et al., 2018)</span>. Rather than addressing over-fitting by incorporating explicit adjustments into the metrics, we handle this by out of sample estimation and, in case the same data is being used to develop the predictive models, resampling based on out of sample estimates.</p>
</section>
<section id="summarykey-points" class="level1">
<h1>Summary/Key Points</h1>
<ul>
<li><p>The general quality of predictions is assessed by accuracy, calibration, and discrimination. These three evaluation domains are often assessed completely independently of each other.</p></li>
<li><p>We propose an approach that unifies these concepts via a scaled decomposition of accuracy into miscalibration and discrimination, <img src="https://latex.codecogs.com/png.latex?R%5E2%20=%20%5Cmathrm%7BDI%7D%20-%20%5Cmathrm%7BMI%7D">. This decomposition is unitless and clarifies that <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D"> is the accuracy when there is no miscalibration, equivalently when the predictions are re-calibrated using the calibration curve.</p></li>
<li><p>Interestingly, <img src="https://latex.codecogs.com/png.latex?r%5E2"> can be interpreted as the <img src="https://latex.codecogs.com/png.latex?R%5E2"> for the best linear recalibration. That metric is also inherently of interest for some applications such as covariate adjustment.</p></li>
<li><p>The three key metrics are <img src="https://latex.codecogs.com/png.latex?R%5E2">, <img src="https://latex.codecogs.com/png.latex?r%5E2">, and <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D"> and they satisfy a key inequality, <img src="https://latex.codecogs.com/png.latex?R%5E2%20%5Cleq%20r%5E2%20%5Cleq%20DI">. The remaining metrics of <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMI%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BNI%7D"> are derived from these.</p></li>
<li><p>Discrimination can never be improved via recalibration, but miscalibration and accuracy can. These metrics are very informative for assessing how much accuracy can be improved via linear recalibration and recalibration via the calibration curve.</p></li>
<li><p>For binary outcomes, both Brier Scores and Log Loss can be used to obtain meaningful performance metrics. For both, the decomposition and interpretations (with suitable modifications) all hold. In general, we recommend use of either score.</p></li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-line-spacing="2">
<div id="ref-Brier1950" class="csl-entry">
Brier, G. W. (1950). VERIFICATION OF FORECASTS EXPRESSED IN TERMS OF PROBABILITY. <em>U. S. Weather Bureau, Monthly Weather Review</em>.
</div>
<div id="ref-Brocker" class="csl-entry">
Brocker, J. (2009). Stable reliability diagrams for probabilistic classifiers. <em>The Quarterly Journal of the Royal Meteorological Society</em>.
</div>
<div id="ref-Dimitriadis" class="csl-entry">
Dimitriadis, T., Gneitingb, T., &amp; Jordan, A. I. (2021). Stable reliability diagrams for probabilistic classifiers. <em>PNAS</em>.
</div>
<div id="ref-Fissler" class="csl-entry">
Fissler, T., Lorentzen, C., &amp; Mayer, M. (2022). Model comparison and calibration assessment. <em>arXiv</em>.
</div>
<div id="ref-Pseudo" class="csl-entry">
Hemmert, G. A. J., Schons, L. M., Wieseke, J., &amp; Schimmelpfennig, H. (2018). Log-likelihood-based pseudo-R2 in logistic regression: Deriving sample-sensitive benchmarks. <em>Sociological Methods &amp; Research</em>.
</div>
<div id="ref-McFadden" class="csl-entry">
McFadden, D. (1974). The measurement of urban travel demand. <em>Journal of Public Economics</em>.
</div>
<div id="ref-Mittlbock" class="csl-entry">
MITTLBOCK, M., &amp; SCHEMPER, M. (1996). EXPLAINED VARIATION FOR LOGISTIC REGRESSION. <em>Statistics in Medicine</em>.
</div>
<div id="ref-Pencina2008" class="csl-entry">
Pencina, M. J., Sr, R. B. D., Jr, R. B. D., &amp; Vasan, R. S. (2008). Evaluating the added predictive ability of a new marker: From area under the ROC curve to reclassification and beyond. <em>Statist. Med.</em>
</div>
<div id="ref-Huang2007" class="csl-entry">
Pepe, M. S., Feng, Z., &amp; Gu, J. W. (2008). Comments on <span>“evaluating the added predictive ability of a new marker: From area under the ROC curve to reclassification and beyond”</span> by m. J. Pencina et al., Statistics in medicine (DOI: 10.1002/sim.2929). <em>STATISTICS IN MEDICINE</em>.
</div>
<div id="ref-Theil" class="csl-entry">
Theil, H. (1970). On the estimation of relationships involving qualitative variables. <em>American Journal of Sociology</em>.
</div>
<div id="ref-Tjur" class="csl-entry">
Tjur, T. (2009). Coefficients of determination in logistic regression models—a new proposal: The coefficient of discrimination. <em>The American Statistician</em>.
</div>
</div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{friesenhahnandchristinarabeandcourtneyschiffman2023,
  author = {Michel Friesenhahn and Christina Rabe and Courtney
    Schiffman},
  title = {Everything You Wanted to Know about {R2} but Were Afraid to
    Ask. {Part} 2, {Binary} Outcomes and Generalizing the Fundamental
    Decomposition.},
  date = {2023-10-31},
  url = {stats4datascience.com},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-friesenhahnandchristinarabeandcourtneyschiffman2023" class="csl-entry quarto-appendix-citeas">
Michel Friesenhahn and Christina Rabe and Courtney Schiffman. (2023,
October 31). <em>Everything you wanted to know about R2 but were afraid
to ask. Part 2, Binary outcomes and generalizing the fundamental
decomposition.</em> Retrieved from <a href="https://stats4datascience.com">stats4datascience.com</a>
</div></div></section></div> ]]></description>
  <category>machine learning</category>
  <category>model perormance</category>
  <guid>https://github.com/stats-4-datascience/blog/posts/three_metrics_binary/index.html</guid>
  <pubDate>Tue, 31 Oct 2023 00:00:00 GMT</pubDate>
  <media:content url="https://github.com/stats-4-datascience/blog/posts/three_metrics_binary/images/BlogPic.png" medium="image" type="image/png" height="120" width="144"/>
</item>
<item>
  <title>Everything you wanted to know about R2 but were afraid to ask</title>
  <dc:creator>Michel Friesenhahn</dc:creator>
  <dc:creator>Christina Rabe</dc:creator>
  <dc:creator>Courtney Schiffman</dc:creator>
  <link>https://github.com/stats-4-datascience/blog/posts/three_metrics/index.html</link>
  <description><![CDATA[ 




<div class="cell">

</div>
<div class="cell" data-layout-align="center">

</div>
<center>
<img src="https://github.com/stats-4-datascience/blog/posts/three_metrics/images/Confusion.PNG" id="fig-confusion" class="img-fluid" style="width:90.0%">
</center>
<p><img src="https://latex.codecogs.com/png.latex?%5C%5C%5B.02in%5D"></p>
<p>There are two steps to state-of-the-art clinical prediction model performance evaluation. The first step, often referred to as overall model performance evaluation, involves interrogating the general quality of predictions in terms of how close they are to observed outcomes. Overall model performance is assessed via three general concepts that are widely applicable: accuracy, discrimination and calibration <span class="citation" data-cites="Steyerberg2010 Steyerberg2014">(Steyerberg et al., 2010, 2014)</span>.</p>
<p>There is some confusion around the relationship between accuracy, discrimination and calibration and what the best approach is to evaluate these concepts. This post aims to provide some clarity via a remarkable decomposition of <img src="https://latex.codecogs.com/png.latex?R%5E2"> that beautifully unifies these three concepts. While similar decompositions have existed for decades in meteorological forecasting <span class="citation" data-cites="Murphy1973">(Murphy, 1973)</span>, we wish to call greater attention to this underutilized evaluation tool and provide additional insights based on our own scaled version of the decomposition. Part 1 of this post provides details around which existing and newly proposed performance metrics to use for an overall assessment of prediction model quality. We will reveal how the suggested metrics are affected by transformations of the predicted values, explain the difference between <img src="https://latex.codecogs.com/png.latex?R%5E2"> and <img src="https://latex.codecogs.com/png.latex?r%5E2">, demonstrate that <img src="https://latex.codecogs.com/png.latex?R%5E2"> can indeed be negative, and give practical guidance on how to compare the metrics to gain information about model performance. <a href="https://schiffmc.pages.roche.com/stats4datascience/posts/three_metrics_binary/">Part 2 of the post</a> explains how the metrics and decompositions discussed here are applicable to binary outcomes as well and can also be generalized beyond the familiar squared error loss.</p>
<p>While accuracy, calibration, and discrimination provide important insights into the overall quality of the predictions, they are not direct metrics of the model’s fitness for use in a particular application. Therefore, for a complete performance evaluation, the second step is to evaluate utility using tailored metrics and visualizations. For example, if a model’s predictions are intended to be used as a covariate for a trial’s adjusted primary endpoint analysis, the effective sample size increase metric evaluates the expected precision gains from the adjustment <span class="citation" data-cites="schiffman">(Schiffman et al., 2023)</span>. Another example is net benefit decision curves, an elegant decision-analytic metric for assessing the utility in clinical practice of prognostic and diagnostic prediction models <span class="citation" data-cites="Vickers Calster2018">(Calster et al., 2018; Vickers et al., 2006)</span>. The focus of this post, however, will be only on the first step of general model performance evaluation, and further discussion around assessing a model’s clinical utility is beyond its scope.</p>
<p>This blog mostly focuses on defining and characterizing population level performance metrics. However, how to estimate metrics of these performance characteristics is covered in a later section.</p>
<section id="accuracy" class="level1">
<h1>Accuracy</h1>
<p>The central desirable property of a prediction model is accuracy. An accurate model is one for which predicted outcomes <img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D"> are close to observed outcomes <img src="https://latex.codecogs.com/png.latex?Y">, as measured with a scoring rule. A scoring rule is a function of <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D"> and <img src="https://latex.codecogs.com/png.latex?y"> that assigns a real value score <img src="https://latex.codecogs.com/png.latex?s(%5Chat%7By%7D,y)">. Without loss of generality, we follow the convention of using scoring rules with negative orientation, meaning larger scores represent greater discrepancies between <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D"> and <img src="https://latex.codecogs.com/png.latex?y">. One such example is the quadratic loss (or squared error) scoring rule, <img src="https://latex.codecogs.com/png.latex?s(%5Chat%7By%7D,y)=(%5Chat%7By%7D-y)%5E2">. Quadratic loss is one of the most common scoring rules because of its simplicity, interpretability and because it is a strictly proper scoring rule (see <a href="https://schiffmc.pages.roche.com/stats4datascience/posts/three_metrics_binary/">Part 2 of this post</a> for more details on proper scoring rules) <span class="citation" data-cites="Fissler">(Fissler et al., 2022)</span>.</p>
<p>The <em>mean squared error</em> (<img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMSE%7D">) of a prediction model is its expected quadratic loss,</p>
<p><span id="eq-mse-sel"><img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMSE%7D%20=%20%5Cmathbb%7BE%7D%5Cbig%5B(Y-%5Chat%7BY%7D)%5E2%5Cbig%5D%20%5Ctag%7B1%7D"></span></p>
<p>We assume that the evaluation of prediction models is done on an independent test data set not used in the development of the model. Therefore, the expectation in Equation&nbsp;1 would be taken over an independent test set and the error would be referred to as generalization error, as opposed to in-sample error <span class="citation" data-cites="Fissler">(Fissler et al., 2022)</span>. We make a quick note here that it is often convenient to work with <img src="https://latex.codecogs.com/png.latex?R%5E2=1-%5Cfrac%7BMSE%7D%7BVar(Y)%7D">, a scaled version of <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMSE%7D">. <img src="https://latex.codecogs.com/png.latex?R%5E2"> will be revisited later on in this post.</p>
<p>Figure&nbsp;1 is a visual aid for understanding <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMSE%7D"> as a metric of accuracy and what performance qualities affect it. Predicted outcomes <img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D"> are shown on the x-axis vs.&nbsp;observed outcomes <img src="https://latex.codecogs.com/png.latex?Y"> on the y-axis for three different models (panels (a)-(c)). Red vertical lines show the distance between <img src="https://latex.codecogs.com/png.latex?Y"> and <img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D">. <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMSE%7D"> is the average of the squared lengths of these red lines.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-mse" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://github.com/stats-4-datascience/blog/posts/three_metrics/index_files/figure-html/fig-mse-1.png" class="img-fluid figure-img" width="1152"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: Visualizing the Effects of Calibration and Discrimination on Mean Squared Error</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>For Model 1 in panel (a) of Figure&nbsp;1, data points are relatively close to the trend line which in this case is the identity line (<img src="https://latex.codecogs.com/png.latex?Y=%5Chat%7BY%7D">), so there is little difference between predicted and observed outcomes. The closer data points are to the identity line, the smaller the average length of the red lines and the smaller the <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMSE%7D">.</p>
<p>What would increase the distance between <img src="https://latex.codecogs.com/png.latex?Y"> and <img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D">? For Model 2 in Figure&nbsp;1 (b), data points still have little variability around the trend line, but now the trend line is no longer equal to the identity line. The predictions from Model 2 are systematically biased. This increases the <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMSE%7D">, illustrated by the increase in the average length of the red lines for Model 2 compared to Model 1.</p>
<p>Like Model 1, the trend line for Model 3 in Figure&nbsp;1 (c) is the identity line and predictions are not systematically biased. However, there is more variability around the trend line. In other words, the predictions from Model 3 explain less of the variability in the outcome. The average length of the red lines increases as less of the outcome variability is explained by the predictions.</p>
<p>Figure&nbsp;1 illustrates how <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMSE%7D"> is yet another quantity that is impacted by the two concepts of bias and variance. In the context of predictive modeling, these two concepts are referred to as calibration and discrimination, respectively, and will be formally defined in the upcoming sections. Later we will derive a mathematical decomposition of accuracy into these two components to obtain a simple quantitative relationship between accuracy, calibration, and discrimination.</p>
</section>
<section id="calibration" class="level1">
<h1>Calibration</h1>
<p><em>Calibration</em> is the assessment of systematic bias in a model’s predictions. The <em>average</em> of the observed outcomes for data points whose predictions are all <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D"> should be close to <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D">. If that is not the case, there is systematic bias in the model’s predictions and it is miscalibrated. Visually, when plotting predicted outcomes on the x-axis vs.&nbsp;observed outcomes on the y-axis, the model is calibrated if points are centered around the identity line <img src="https://latex.codecogs.com/png.latex?Y=%5Chat%7BY%7D">, as they are in Figure&nbsp;2 panel (b).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-cal" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://github.com/stats-4-datascience/blog/posts/three_metrics/index_files/figure-html/fig-cal-1.png" class="img-fluid figure-img" width="1152"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2: Example of two models, one calibrated and one miscalibrated, with the same discrimination</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>More formally, define the <em>calibration curve</em> to be the function <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BC%7D(%5Chat%7By%7D)%20=%20%5Cmathbb%7BE%7D%5BY%20%5Cmid%20%5Chat%7BY%7D%20=%20%5Chat%7By%7D%5D">. A model is calibrated if and only if <img src="https://latex.codecogs.com/png.latex?C(%5Chat%7BY%7D)%20=%20%5Cmathbb%7BE%7D%5BY%20%5Cmid%20%5Chat%7BY%7D%20%5D%20=%20%5Chat%7BY%7D">. As illustrated in Figure&nbsp;3, miscalibration is numerically defined to be the expected squared difference between the predictions and the calibration curve, <img src="https://latex.codecogs.com/png.latex?E%5Cbig%5B(C(%5Chat%7BY%7D)%20-%20%5Chat%7BY%7D)%5E2%5Cbig%5D">. Using the calibration curve, we define the miscalibration index (<img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMI%7D">) as miscalibration normalized by the variance of <img src="https://latex.codecogs.com/png.latex?Y">. Clearly <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMI%7D%5Cgeq0"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMI%7D=0"> if and only if the model is perfectly calibrated. A model can be <strong>recalibrated</strong> by transforming its predictions via the calibration curve <img src="https://latex.codecogs.com/png.latex?C(%5Chat%7BY%7D)"> (proof in the Appendix), in other words by applying the function <img src="https://latex.codecogs.com/png.latex?C(%5Chat%7BY%7D)%20=%20%5Cmathbb%7BE%7D%5BY%20%5Cmid%20%5Chat%7BY%7D%5D"> to the model’s predictions.</p>
<div class="callout-definition callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Miscalibration Index (MI)
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Miscalibration Index (MI) is the <img src="https://latex.codecogs.com/png.latex?MSE"> between <img src="https://latex.codecogs.com/png.latex?C(%5Chat%7BY%7D)"> and <img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D"> normalized by <img src="https://latex.codecogs.com/png.latex?Var(Y)"></p>
<p><span id="eq-MI"><img src="https://latex.codecogs.com/png.latex?%0AMI%20=%20%5Cfrac%7BE%5Cbig%5B(C(%5Chat%7BY%7D)%20-%20%5Chat%7BY%7D)%5E2%5Cbig%5D%7D%7BVar(Y)%7D%0A%5Ctag%7B2%7D"></span> <img src="https://latex.codecogs.com/png.latex?MI%5Cgeq0"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMI%7D=0"> iff the model is perfectly calibrated</p>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-MI" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://github.com/stats-4-datascience/blog/posts/three_metrics/index_files/figure-html/fig-MI-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3: The miscalibration Index (MI) is the MSE between <img src="https://latex.codecogs.com/png.latex?C(%5Chat%7BY%7D)"> and <img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D">, scaled by the variance of <img src="https://latex.codecogs.com/png.latex?Y"></figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>A growing number of researchers are calling for calibration evaluation to become a standard part of model performance evaluation <span class="citation" data-cites="Calster2019">(Calster et al., 2019)</span>. This is because for many use cases calibration is an important component of prediction quality that impacts utility. For example, calibration is very important when the prediction model is intended to be used in medical practice to manage patients. Since decisions in such scenarios are based on absolute risks, systematic deviations from true risks can lead to suboptimal care. While the importance of calibration varies depending on the use case, it is simple enough to examine that it should be a routine part of model performance evaluation <span class="citation" data-cites="Calster2019">(Calster et al., 2019)</span>.</p>
<p>As demonstrated in Figure&nbsp;1 (c), while calibration is important, it does not assess the spread of the expected outcome values. For example, a constant prediction model <img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D=%5Cmathbb%7BE%7D(Y)"> is calibrated (since <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5BY%7C%5Chat%7BY%7D=%5Cmathbb%7BE%7D(Y)%5D=%5Cmathbb%7BE%7D(Y)">), but it has zero variance of the expected outcome values and thus no ability to discriminate between observations.</p>
</section>
<section id="discrimination" class="level1">
<h1>Discrimination</h1>
<p>The two models shown in Figure&nbsp;4 are both calibrated, but the model in panel (b) has better <em>discrimination</em> than the model in panel (a). Compared to the model in panel (a), the model in panel (b) explains the true outcome <img src="https://latex.codecogs.com/png.latex?Y"> with less spread around the calibration curve <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BC%7D(%5Chat%7BY%7D)">, which in this instance is the identity line. For a given outcome distribution <img src="https://latex.codecogs.com/png.latex?Y">, less spread around <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BC%7D(%5Chat%7BY%7D)"> translates into a larger variation in <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BC%7D(%5Chat%7BY%7D)">. Thus there are two equivalent ways to view discrimination, either as the amount of explained variability in the outcome or as the variance of the calibration curve.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-dis" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://github.com/stats-4-datascience/blog/posts/three_metrics/index_files/figure-html/fig-dis-1.png" class="img-fluid figure-img" width="1152"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;4: Visualizing the effect of discrimination on MSE</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>To formalize this concept, define discrimination as the variance of the calibration curve, <img src="https://latex.codecogs.com/png.latex?Var(C(%5Chat%7BY%7D))">. We then define the Discrimination Index <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7B(DI)%7D"> as discrimination scaled by <img src="https://latex.codecogs.com/png.latex?Var(Y)">.</p>
<div class="callout-definition callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Discrimination Index (DI)
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Discrimination Index (DI) is the variance of the calibration curve <img src="https://latex.codecogs.com/png.latex?C(%5Chat%7BY%7D)"> scaled by <img src="https://latex.codecogs.com/png.latex?Var(Y)"></p>
<p><span id="eq-DI"><img src="https://latex.codecogs.com/png.latex?%0ADI%20=%20%5Cfrac%7BVar(C(%5Chat%7BY%7D))%7D%7BVar(Y)%7D%0A%5Ctag%7B3%7D"></span></p>
<p><img src="https://latex.codecogs.com/png.latex?0%5Cleq%20DI%5Cleq1"></p>
</div>
</div>
<p>The more variability in the observed outcomes explained by the predictions, the larger the variance in <img src="https://latex.codecogs.com/png.latex?C(%5Chat%7BY%7D)"> and the larger the <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D">. For example, the model in Figure&nbsp;4 panel (b) has a larger <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D"> than the model in panel (a). In general, <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D"> is a non-negative number that can be at most <img src="https://latex.codecogs.com/png.latex?1"> (proof in Appendix). If a model’s predictions explain virtually all of the variability in the observed outcomes, then <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D%5Capprox1">. Unlike <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMI%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D"> is unchanged by calibrating the model (proof in Appendix).</p>
</section>
<section id="a-fundamental-decomposition-of-r2" class="level1">
<h1>A Fundamental Decomposition of <img src="https://latex.codecogs.com/png.latex?R%5E2"></h1>
<p>Now that calibration and discrimination are well-defined, we return to model accuracy. Recall that <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMSE%7D">, the expected quadratic loss over an independent test set, is an accuracy metric. We noted earlier that <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMSE%7D"> depends on the scale of <img src="https://latex.codecogs.com/png.latex?Y">. It is therefore sensible to normalize the <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMSE%7D"> by the total variance, <img src="https://latex.codecogs.com/png.latex?Var(Y)">. Finally, subtracting from 1 gives another well-known accuracy metric, <img src="https://latex.codecogs.com/png.latex?R%5E2">:</p>
<p><span id="eq-simple-r2"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AR%5E2%20=%201-%5Cfrac%7B%5Cmathrm%7BMSE%7D%7D%7BVar(Y)%7D%0A%5Cend%7Balign%7D%0A%5Ctag%7B4%7D"></span></p>
<p>Equation&nbsp;5 shows that <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMSE%7D"> can be decomposed into the total variation of the observed outcomes, the variance of the calibration curve, and the mean squared error between the calibration curve and predictions (see proof in the Appendix):</p>
<p><span id="eq-mse-decomp"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%20%20%5Cmathrm%7BMSE%7D%20&amp;=%20Var(Y)%20-%20Var(%5Cmathrm%7BC%7D(%5Chat%7BY%7D))%20+%20%5Cmathbb%7BE%7D%5Cbig%5B(%5Cmathrm%7BC%7D(%5Chat%7BY%7D)%20-%20%5Chat%7BY%7D)%5E2%20%5Cbig%5D%20%5C%5C%0A%5Cend%7Balign%7D%0A%5Ctag%7B5%7D"></span></p>
<p>Replacing the numerator in Equation&nbsp;4 with the <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMSE%7D"> decomposition from Equation&nbsp;5, it can be easily shown how <img src="https://latex.codecogs.com/png.latex?R%5E2"> is affected by miscalibration and discrimination.</p>
<div class="callout-definition callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
The Fundamental Decomposition of <img src="https://latex.codecogs.com/png.latex?R%5E2">
</div>
</div>
<div class="callout-body-container callout-body">
<p><span id="eq-R2-decomp"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%20%20R%5E2&amp;%20=%20DI%20-%20MI%0A%5Cend%7Balign%7D%0A%5Ctag%7B6%7D"></span></p>
</div>
</div>
<p>This wonderfully simple decomposition of <img src="https://latex.codecogs.com/png.latex?R%5E2"> into the difference between <img src="https://latex.codecogs.com/png.latex?DI"> and <img src="https://latex.codecogs.com/png.latex?MI"> disentangles the concepts of accuracy, calibration and discrimination. The decomposition reveals that <img src="https://latex.codecogs.com/png.latex?R%5E2"> holds valuable information on model performance.</p>
<p>Some very useful observations follow directly from this decomposition:</p>
<ul>
<li>Since <img src="https://latex.codecogs.com/png.latex?MI%20%5Cgeq%200"> and <img src="https://latex.codecogs.com/png.latex?0%20%5Cleq%20DI%5Cleq%201">, <img src="https://latex.codecogs.com/png.latex?R%5E2%20%5Cleq%201">. In fact, <img src="https://latex.codecogs.com/png.latex?R%5E2"> can be negative due to miscalibration!</li>
<li>Calibrating a model or increasing its discriminatory power increases accuracy.</li>
<li>If <img src="https://latex.codecogs.com/png.latex?R%5E2%20%3C%20DI">, accuracy can be improved by calibrating the model. <img src="https://latex.codecogs.com/png.latex?R%5E2"> will equal <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D"> for the calibrated model.</li>
</ul>
<center>
<div id="fig-quad" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://github.com/stats-4-datascience/blog/posts/three_metrics/images/quadrants_numbers_v2.PNG" class="img-fluid figure-img" style="width:75.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5: Visualizing the effects of calibration and discrimination on overall accuracy</figcaption><p></p>
</figure>
</div>
</center>
<p>Figure&nbsp;5 demonstrates four examples of the fundamental decomposition. The models in the bottom row of Figure&nbsp;5 have imperfect calibration (<img src="https://latex.codecogs.com/png.latex?MI%3E0">),and thus accuracy can be improved via calibration. The top row of Figure&nbsp;5 is the result of calibrating the bottom row models; <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D"> remains the same but <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMI%7D=0"> after calibration and thus <img src="https://latex.codecogs.com/png.latex?R%5E2"> increases. Although both models in the top row of Figure&nbsp;5 have perfect calibration, the model on the right has better discrimination and thus greater <img src="https://latex.codecogs.com/png.latex?R%5E2">.</p>
<p>Similar concepts and decompositions have been discussed since the 70’s and 80’s, in many cases developed with a focus on binary outcomes <span class="citation" data-cites="Murphy1973 DeGroot Dimitriadis">(DeGroot et al., 1983; Dimitriadis et al., 2021; Murphy, 1973)</span>. Part of the appeal of these decompositions, including our proposed Fundamental Decomposition, is that they apply to both continuous and binary outcomes, assuming that binary outcomes are coded with values in {0,1}. In historical and recent literature, the terms <em>reliability</em>, <em>resolution</em> and <em>uncertainty</em> are sometimes used instead of <em>calibration</em>, <em>discrimination</em> and <em>variance of the observed outcome</em>.</p>
</section>
<section id="linear-recalibration" class="level1">
<h1>Linear Recalibration</h1>
<p>We have already introduced recalibration via the calibration curve <img src="https://latex.codecogs.com/png.latex?C(%5Chat%7BY%7D)">, but there is also linear recalibration. Linear transformations are easier to estimate than unconstrained calibration curves, and may be more appropriate for smaller sample sizes.</p>
<p>As illustrated in Figure&nbsp;6, the linear transformation of predictions that maximizes <img src="https://latex.codecogs.com/png.latex?R%5E2"> is referred to as the <em>calibration line</em>:</p>
<div class="callout-definition callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
<img src="https://latex.codecogs.com/png.latex?L(%5Chat%7BY%7D)=%5Calpha_%7Bopt%7D%20+%20%5Cbeta_%7Bopt%7D%5Chat%7BY%7D"> is referred to as the <em>calibration line</em> where
</div>
</div>
<div class="callout-body-container callout-body">
<p><span id="eq-cal-line"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A(%5Calpha_%7Bopt%7D%20,%20%5Cbeta_%7Bopt%7D)%20=argmin_%7B%5Calpha,%5Cbeta%7D%20%5Cmathbb%7BE%7D%5Cbig%5B%5Cbig(Y-(%5Calpha%20+%20%5Cbeta%5Chat%7BY%7D)%5Cbig)%5E2%5Cbig%5D%20%5C%5C%0A%5Cbeta_%7Bopt%7D=%5Cfrac%7BCov(Y,%5Chat%7BY%7D)%7D%7BVar(%5Chat%7BY%7D)%7D%20%5C%5C%0A%5Calpha_%7Bopt%7D%20=%20%5Cmathbb%7BE%7D%5BY%5D%20-%20%5Cbeta_%7Bopt%7D%5Cmathbb%7BE%7D%5B%5Chat%7BY%7D%5D%20%5Cspace%20%5C%5C%0A%5Cend%7Balign%7D%0A%5Ctag%7B7%7D"></span></p>
<p><img src="https://latex.codecogs.com/png.latex?L(%5Chat%7BY%7D)"> is the linear transformation that maximizes <img src="https://latex.codecogs.com/png.latex?R%5E2">.</p>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-LX-CX" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://github.com/stats-4-datascience/blog/posts/three_metrics/index_files/figure-html/fig-LX-CX-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6: The calibration line and calibration curve.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Not surprisingly, <img src="https://latex.codecogs.com/png.latex?%5Calpha_%7Bopt%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta_%7Bopt%7D"> are the familiar population least squares linear regression coefficients for a linear model with an intercept and single predictor (proof in Appendix). <img src="https://latex.codecogs.com/png.latex?%5Calpha_%7Bopt%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta_%7Bopt%7D"> are also referred to as the calibration intercept and slope, respectively.</p>
<p>When predictions are transformed via the calibration line, <img src="https://latex.codecogs.com/png.latex?R%5E2"> is equal to the squared Pearson correlation between the predictions and the observed outcomes, <img src="https://latex.codecogs.com/png.latex?R%5E2=r%5E2%20=%20cor%5E2(Y,%5Chat%7BY%7D)"> (see Appendix for proof). Note that, in general, <img src="https://latex.codecogs.com/png.latex?R%5E2"> and <img src="https://latex.codecogs.com/png.latex?r%5E2"> are not equal, easily seen by the fact that <img src="https://latex.codecogs.com/png.latex?R%5E2"> can be negative whereas <img src="https://latex.codecogs.com/png.latex?0%20%5Cleq%20r%5E2%20%5Cleq1">. <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D"> remains unchanged by linear recalibration, but <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMI%7D"> becomes <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMI%7D%20=%20%5Cmathrm%7BDI%7D%20-%20R%5E2%20=%20%5Cmathrm%7BDI%7D%20-%20r%5E2">. We refer to <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D%20-%20r%5E2"> as the <strong>nonlinearity index</strong> <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BNI%7D">, since any residual miscalibration after linear recalibration is a measure of nonlinearity in the calibration curve.</p>
</section>
<section id="what-about-r2" class="level1">
<h1>What about <img src="https://latex.codecogs.com/png.latex?r%5E2"> ?</h1>
<p>In the previous section it was noted that after linear recalibration the <img src="https://latex.codecogs.com/png.latex?R%5E2"> accuracy metric equals <img src="https://latex.codecogs.com/png.latex?r%5E2">, the squared Pearson correlation coefficient, and in general <img src="https://latex.codecogs.com/png.latex?R%5E2"> need not equal <img src="https://latex.codecogs.com/png.latex?r%5E2">. Here we note that in some settings the <img src="https://latex.codecogs.com/png.latex?r%5E2"> metric is of interest in its own right. An important example is for covariate adjustment in clinical trials (<span class="citation" data-cites="schiffman">Schiffman et al. (2023)</span>). There, treatment effects are adjusted using predictions from a model developed based on data external to the clinical trial. <img src="https://latex.codecogs.com/png.latex?r%5E2"> is a useful metric for this application because it can be used to quantify the expected precision and power gains from adjustment when the outcome is continuous. Our <a href="https://schiffmc.pages.roche.com/stats4datascience/posts/covariate_adjustment/">previous post</a> on covariate adjustment reveals how the effective sample size increase (<img src="https://latex.codecogs.com/png.latex?ESSI">) from an adjusted analysis of a continuous outcome is a simple function of the squared correlations between the outcome and the covariate in the various treatment arms. For example, a squared correlation of <img src="https://latex.codecogs.com/png.latex?r%5E2%20=%200.3"> between the outcome and the predicted outcome from a prognostic model would translate into an <img src="https://latex.codecogs.com/png.latex?ESSI"> of <img src="https://latex.codecogs.com/png.latex?43%5C%25"> if the predictions were adjusted for in the primary analysis. Knowing <img src="https://latex.codecogs.com/png.latex?r%5E2"> allows study teams to be able to easily estimate the expected gains from covariate adjustment and to decide which covariates to adjust for.</p>
<p>Note that covariate adjustment is an application of prediction models where calibration is not critical. Adjusting implicitly performs linear calibration, as needed, so only very marked nonlinearity in the calibration curve will impact the effectiveness of covariate adjustment. Since such nonlinearity is not common, calibration plays little to no role for this use case and discrimination is much more important.</p>
</section>
<section id="a-key-inequality" class="level1">
<h1>A Key Inequality</h1>
<p>Table&nbsp;1 summarizes the impact of transforming predictions via the calibration line and curve on <img src="https://latex.codecogs.com/png.latex?R%5E2">, <img src="https://latex.codecogs.com/png.latex?r%5E2">, <img src="https://latex.codecogs.com/png.latex?DI"> and <img src="https://latex.codecogs.com/png.latex?MI">.</p>
<div id="tbl-3metrics" class="anchored">
<table class="table">
<caption>Table&nbsp;1: Relationship between <img src="https://latex.codecogs.com/png.latex?R%5E2">, <img src="https://latex.codecogs.com/png.latex?r%5E2"> and <img src="https://latex.codecogs.com/png.latex?DI"></caption>
<colgroup>
<col style="width: 20%">
<col style="width: 16%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Predictor</th>
<th style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?R%5E2"></th>
<th style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?r%5E2"></th>
<th style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?DI"></th>
<th style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?MI"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?R%5E2"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?r%5E2"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?DI"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?DI-R%5E2"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?L(%5Chat%7BY%7D)"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?r%5E2"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?r%5E2"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?DI"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?DI-r%5E2"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?C(%5Chat%7BY%7D)"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?DI"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?DI"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?DI"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?0"></td>
</tr>
</tbody>
</table>
</div>
<p>An extremely useful inequality for evaluating model performance follows from the first column of Table&nbsp;1, and the fact that the calibration curve is the transformation that maximizes <img src="https://latex.codecogs.com/png.latex?R%5E2"> (proof in Appendix).</p>
<div class="callout-definition callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
<em>Key Inequality</em>
</div>
</div>
<div class="callout-body-container callout-body">
<p><span id="eq-inequality"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AR%5E2%20%5Cleq%20r%5E2%20%5Cleq%20DI%0A%5Cend%7Balign%7D%0A%5Ctag%7B8%7D"></span></p>
</div>
</div>
<p>This key inequality has several useful implications:</p>
<ul>
<li><p>If <strong><img src="https://latex.codecogs.com/png.latex?R%5E2=r%5E2=DI"></strong> then <img src="https://latex.codecogs.com/png.latex?MI=0"> and the model is calibrated.</p></li>
<li><p>If <strong><img src="https://latex.codecogs.com/png.latex?R%5E2%20%3C%20r%5E2=DI"></strong>, then the calibration curve is identical to the calibration line, but this line is not the identity. After linear recalibration using the calibration line, <img src="https://latex.codecogs.com/png.latex?R%5E2"> is increased to the <img src="https://latex.codecogs.com/png.latex?r%5E2"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D"> of the original predictions, <img src="https://latex.codecogs.com/png.latex?Y">.</p></li>
<li><p>If <strong><img src="https://latex.codecogs.com/png.latex?R%5E2%20=%20r%5E2%20%3C%20DI"></strong>, then the calibration line is the identity line, but the calibration curve is not linear. So linear recalibration will not change the predictions and recalibration using the calibration curve will increase both <img src="https://latex.codecogs.com/png.latex?R%5E2"> and <img src="https://latex.codecogs.com/png.latex?r%5E2"> to the <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D"> of the original predictions, <img src="https://latex.codecogs.com/png.latex?Y">.</p></li>
<li><p>If <strong><img src="https://latex.codecogs.com/png.latex?R%5E2%20%3C%20r%5E2%20%3C%20DI"></strong>, then the calibration line is not the identity line and the calibration curve is not linear. Linear recalibration will increase <img src="https://latex.codecogs.com/png.latex?R%5E2"> to the <img src="https://latex.codecogs.com/png.latex?r%5E2"> of the original predictions; calibration using the calibration curve will increase <img src="https://latex.codecogs.com/png.latex?R%5E2"> and <img src="https://latex.codecogs.com/png.latex?r%5E2"> to the <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D"> of the original predictions.</p></li>
</ul>
<p>By estimating and comparing these simple metrics, one can gain a comprehensive understanding of a model’s overall performance. Comparing <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BR%7D%5E2">, <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7Br%7D%5E2"> and <img src="https://latex.codecogs.com/png.latex?DI"> provides valuable information about whether the model is calibrated, whether the calibration curve is linear or non linear, whether the calibration line is the identity line, and what happens to the <img src="https://latex.codecogs.com/png.latex?R%5E2"> and <img src="https://latex.codecogs.com/png.latex?r%5E2"> metrics with linear recalibration or recalibration using the calibration curve.</p>
</section>
<section id="the-interpretation-youve-been-looking-for" class="level1">
<h1>The Interpretation You’ve Been Looking For</h1>
<p>Based on these results we have the following simple interpretations for <img src="https://latex.codecogs.com/png.latex?R%5E2"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D">. Note the important role calibration plays in interpreting <img src="https://latex.codecogs.com/png.latex?R%5E2">:</p>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Interpretations of <img src="https://latex.codecogs.com/png.latex?R%5E2"> and <img src="https://latex.codecogs.com/png.latex?DI"> as Proportions of Explained Uncertainty
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D"> is the proportion of variability in the observed outcomes explained by recalibrated predictions.</p></li>
<li><p>If predictions are calibrated, then <img src="https://latex.codecogs.com/png.latex?R%5E2%20=%20%5Cmathrm%7BDI%7D=Var(C(%5Chat%7BY%7D))%20/%20Var(Y)=%20Var(%5Chat%7BY%7D)%20/%20Var(Y)">, so <img src="https://latex.codecogs.com/png.latex?R%5E2"> is the proportion of variability in the observed outcomes explained by the predictions.</p></li>
<li><p>If predictions are not calibrated, then <img src="https://latex.codecogs.com/png.latex?R%5E2"> cannot be interpreted as the proportion of variability in the observed outcomes explained by the predictions.</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?R%5E2"> is always the proportional reduction of <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMSE%7D"> relative to the best constant prediction, <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D(Y)">.</p></li>
</ul>
</div>
</div>
<p>The above interpretations of <img src="https://latex.codecogs.com/png.latex?R%5E2"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D"> may be confusing since they appear at odds with the traditional interpretation of <img src="https://latex.codecogs.com/png.latex?R%5E2"> as the proportion of variance explained by the predictions when outcomes are continuous. Furthermore, it is often stated that <img src="https://latex.codecogs.com/png.latex?R%5E2%20=%20r%5E2">, whereas we have stated that <img src="https://latex.codecogs.com/png.latex?R%5E2%20%5Cleq%20r%5E2">, with equality if and only if the calibration line is the identity line.</p>
<p>The reason for the discrepancy between the traditional interpretation of <img src="https://latex.codecogs.com/png.latex?R%5E2"> and what is stated here is that in the traditional setting <img src="https://latex.codecogs.com/png.latex?R%5E2"> is an in-sample estimator and the predictions come from ordinary least squares regression, so <img src="https://latex.codecogs.com/png.latex?R%5E2%20=%20%5Cfrac%7BVar(%5Chat%7BY%7D)%7D%7BVar(Y)%7D"> and <img src="https://latex.codecogs.com/png.latex?R%5E2%20=%20r%5E2"> necessarily. However, we do not think this traditional setting is particularly relevant and what is more, the interpretation of <img src="https://latex.codecogs.com/png.latex?R%5E2"> as the proportion of explained variability does not hold out of sample. Instead, we prefer to focus on out of sample performance and metrics that are applicable to any prediction model, not just models of continuous outcomes. For out of sample interpretation of performance metrics in any setting, use the simple interpretations listed in the callout block above which highlights the impact of calibration on the metrics’ interpretations.</p>
</section>
<section id="estimating-performance-metrics" class="level1">
<h1>Estimating Performance Metrics</h1>
<p>We assume that <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMI%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BR%5E2%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7Br%7D%5E2"> are estimated using independent data that were not involved in the training or selection of the model being evaluated. If an independent data set is not available, performance metrics can be estimated using an appropriate resampling method to avoid overly optimistic estimates.</p>
<section id="estimating-mathrmdi" class="level3">
<h3 class="anchored" data-anchor-id="estimating-mathrmdi">Estimating <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D"></h3>
<p>Recall that <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D"> is discrimination normalized by the variance of the observed outcomes:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0ADI%20=%20%5Cfrac%7BVar(C(%5Chat%7BY%7D))%7D%7BVar(Y)%7D%0A"></p>
<p>To estimate discrimination, first estimate the calibration curve using the observed and predicted outcomes in the independent holdout data. Generalized additive models offer a nice method for estimating flexible yet smooth calibration curves. The ‘gam’ function in the r package ‘mgcv’, can be used to fit generalized additive models with regression splines. Note that observed outcomes are the dependent variable and predictions are the independent variable.</p>
<p>An alternative to using generalized additive models is to use nonparametric isotonic regression and the pool-adjacent-violators algorithm (PAVA) to estimate calibration curves <span class="citation" data-cites="Dimitriadis Ayer">(Ayer et al., 1955; Dimitriadis et al., 2021)</span>. PAVA is an appealing bin-and-count method because it is fast, non-parametric, has a monotonicity constraint for regularization, has automatic bin determination, and is a maximum likelihood estimator <span class="citation" data-cites="Ayer Leeuw">(Ayer et al., 1955; Leeuw et al., 2009)</span>. We find that this method is helpful for confirming the shape of the calibration curve in a robust, non-parametric manner. However, we note that PAVA is more data-hungry than generalized additive models, and may not be appropriate for smaller data sets. Furthermore, we have observed that <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BR%5E2%7D"> estimates based on PAVA tend to be systematically higher than direct estimates of <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BR%5E2%7D"> or estimates of <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BR%5E2%7D"> based on generalized additive models, even when the estimated calibration curves appear to be visually very similar.</p>
<p>For smaller sample sizes, it may be necessary to estimate only the calibration line and assume the calibration curve is equal to the calibration line. However, note that in this case the <img src="https://latex.codecogs.com/png.latex?r%5E2"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D"> estimates will necessarily be equal and curvature will not be evaluated, so we must assume that departures from linearity are minor for the performance metrics to be meaningful. We note that a popular use of calibration lines is examination of their intercept and slope. While this may be helpful for some descriptive purposes, we do not recommend these to be the sole metrics used for evaluation of calibration. For further details see the Appendix.</p>
<p>Once the calibration curve is fit, calibrate the original predictions by transforming them via the estimated calibration curve, <img src="https://latex.codecogs.com/png.latex?%5Chat%7BC%7D(%5Chat%7BY%7D)">. Discrimination can then be estimated as the empirical variance of the calibrated predictions. Estimating <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D"> is then as simple as dividing the estimated discrimination by the empirical variance of the observed outcomes:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathrm%7B%5Cwidehat%7BDI%7D%7D%20=%20%5Cfrac%7B%5Csum%5Cbigg(%5Chat%7BC%7D(%5Chat%7BY%7D)%20-%20%5Coverline%7B%5Chat%7BC%7D(%5Chat%7BY%7D)%7D%5Cbigg)%5E2%7D%7B%5Csum%5Cbig(Y-%5Coverline%7BY%7D%5Cbig)%5E2%7D%0A"></p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"></span>
<span id="cb1-2"><span class="co" style="color: #5E5E5E;"># Fit calibration curve</span></span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="fu" style="color: #4758AB;">library</span>(mgcv)</span>
<span id="cb1-5"></span>
<span id="cb1-6">cal_fun <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">gam</span>(observed_outcomes <span class="sc" style="color: #5E5E5E;">~</span> <span class="fu" style="color: #4758AB;">s</span>(original_predictions, <span class="at" style="color: #657422;">k=</span><span class="dv" style="color: #AD0000;">3</span>), <span class="at" style="color: #657422;">na.action =</span> <span class="st" style="color: #20794D;">"na.omit"</span>)</span>
<span id="cb1-7"></span>
<span id="cb1-8"><span class="co" style="color: #5E5E5E;"># Obtain calibrated predictions</span></span>
<span id="cb1-9"></span>
<span id="cb1-10">calibrated_predictions <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">predict</span>(cal_fun, <span class="fu" style="color: #4758AB;">data.frame</span>(<span class="at" style="color: #657422;">original_predictions =</span> original_predictions))</span>
<span id="cb1-11"></span>
<span id="cb1-12"><span class="co" style="color: #5E5E5E;"># Estimate DI</span></span>
<span id="cb1-13"></span>
<span id="cb1-14">DI <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">var</span>(calibrated_predictions)<span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">var</span>(observed_outcomes)</span></code></pre></div>
</section>
<section id="estimating-mathrmmi" class="level3">
<h3 class="anchored" data-anchor-id="estimating-mathrmmi">Estimating <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMI%7D"></h3>
<p>The calibrated predictions will also be used to estimate the miscalibration index. Recall that <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMI%7D"> is miscalibration normalized by the variance of the observed outcomes:</p>
<p><img src="https://latex.codecogs.com/png.latex?MI%20=%20%5Cfrac%7BE%5Cbig%5B(C(%5Chat%7BY%7D)%20-%20%5Chat%7BY%7D)%5E2%5Cbig%5D%7D%7BVar(Y)%7D"></p>
<p>To estimate miscalibration, simply take the empirical mean of the squared differences between the original predictions and the predictions transformed by the calibration curve. Divide by the empirical variance of the observed outcomes to estimate <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMI%7D">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7BMI%7D%20=%20%5Cfrac%7B%5Csum%5Cbig(%5Chat%7BC%7D(%5Chat%7BY%7D)%20-%20%5Chat%7BY%7D%5Cbig)%5E2%7D%7B%5Csum%5Cbig(Y-%5Coverline%7BY%7D%5Cbig)%5E2%7D"></p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"></span>
<span id="cb2-2"><span class="co" style="color: #5E5E5E;"># Estimate MI</span></span>
<span id="cb2-3"></span>
<span id="cb2-4">MI <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">mean</span>((calibrated_predictions <span class="sc" style="color: #5E5E5E;">-</span> original_predictions)<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span>)<span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">var</span>(observed_outcomes)</span>
<span id="cb2-5"></span></code></pre></div>
</section>
<section id="estimating-mathrmr2" class="level3">
<h3 class="anchored" data-anchor-id="estimating-mathrmr2">Estimating <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BR%5E2%7D"></h3>
<p>Once <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathrm%7BDI%7D%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathrm%7BMI%7D%7D"> are calculated using the calibration curve, an estimate of <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BR%5E2%7D"> based on the calibration curve can then easily be obtained by subtracting <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathrm%7BMI%7D%7D"> from <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathrm%7BDI%7D%7D">. Denote this estimate of <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BR%5E2%7D"> based on the calibration curve with a subscript ‘C’ for ‘calibration’:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathrm%7BR_%7BC%7D%5E2%7D%7D%20=%20%5Cwidehat%7B%5Cmathrm%7BDI%7D%7D%20-%20%5Cwidehat%7B%5Cmathrm%7BMI%7D%7D"></p>
<p>Note that <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BR%7D%5E2"> can also be estimated directly, without needing to first estimate a calibration curve, by calculating one minus the empirical mean squared error between the original predictions and the observed outcomes divided by the variance of the observed outcome. Denote this direct estimate of <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BR%5E2%7D"> with a subscript ‘D’ for ‘direct’. In our experience, <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathrm%7BR_%7BC%7D%5E2%7D%7D"> is nearly identical to <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathrm%7BR_%7BD%7D%5E2%7D%7D"> when the calibration curve has been estimated using the default smoother in the ‘gam’ function from the ‘mgcv’ R package.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathrm%7BR_%7BD%7D%5E2%7D%7D%20=%201-%5Cfrac%7B%5Csum%5Cbig(Y-%5Chat%7BY%7D%5Cbig)%5E2%7D%7B%5Csum%5Cbig(Y-%5Coverline%7BY%7D%5Cbig)%5E2%7D"></p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"></span>
<span id="cb3-2"><span class="co" style="color: #5E5E5E;"># Estimate of R^2 based on the calibration curve</span></span>
<span id="cb3-3"></span>
<span id="cb3-4">R2 <span class="ot" style="color: #003B4F;">&lt;-</span> DI <span class="sc" style="color: #5E5E5E;">-</span> MI</span>
<span id="cb3-5"></span>
<span id="cb3-6"><span class="co" style="color: #5E5E5E;"># Direct estimate of R^2</span></span>
<span id="cb3-7"></span>
<span id="cb3-8">R2_direct <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">-</span> (<span class="fu" style="color: #4758AB;">sum</span>((observed_outcomes <span class="sc" style="color: #5E5E5E;">-</span> original_predictions)<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span>) <span class="sc" style="color: #5E5E5E;">/</span> ((<span class="fu" style="color: #4758AB;">length</span>(original_predictions) <span class="sc" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span>) <span class="sc" style="color: #5E5E5E;">*</span> <span class="fu" style="color: #4758AB;">var</span>(observed_outcomes)))</span>
<span id="cb3-9"></span></code></pre></div>
</section>
<section id="estimating-mathrmr2-1" class="level3">
<h3 class="anchored" data-anchor-id="estimating-mathrmr2-1">Estimating <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7Br%5E2%7D"></h3>
<p>To estimate <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7Br%7D%5E2">, simply calculate the squared Pearson correlation coefficient between the observed outcomes and the original predictions:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Br%5E2%7D=cor(Y,%5Chat%7BY%7D_%7Boriginal%7D)%5E2"></p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"></span>
<span id="cb4-2"></span>
<span id="cb4-3"><span class="co" style="color: #5E5E5E;"># Direct estimate of r^2</span></span>
<span id="cb4-4"></span>
<span id="cb4-5">r2_direct <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">cor</span>(original_predictions, observed_outcomes)<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb4-6"></span></code></pre></div>
<div class="cell">

</div>
</section>
<section id="example" class="level2">
<h2 class="anchored" data-anchor-id="example">Example</h2>
<p>Geographic atrophy (GA) is an advanced form of age-related macular degeneration (AMD) that leads to vision loss. GA progression can be assessed by the change in GA lesion area (<img src="https://latex.codecogs.com/png.latex?mm%5E2">) over time using Fundus Autofluorescence (FAF) images. Salvi et al.&nbsp;use data from several clinical trials and observational studies to develop deep learning (DL) models that can predict the future region of growth (ROG) of GA lesions at l-year using FAF images <span class="citation" data-cites="GAmanuscript">(Salvi et al., 2023)</span>.</p>
<p>To develop and evaluate the models, the data were split into a development set (<img src="https://latex.codecogs.com/png.latex?n=388">) and test set (<img src="https://latex.codecogs.com/png.latex?n=209">). The development set was further split into a training (<img src="https://latex.codecogs.com/png.latex?n=310">) and validation (<img src="https://latex.codecogs.com/png.latex?n=78">) set. Models were built using the training set, selected using the validation set and evaluated using the test set. See Salvi et al.&nbsp;for further details around the development of the various DL models.</p>
<center>
<p><img src="https://github.com/stats-4-datascience/blog/posts/three_metrics/images/GA_progression.png" class="img-fluid"></p>
</center>
<p>To demonstrate how to use the previously discussed metrics to evaluate model performance, we estimate the metrics for one of the DL models from this publication (referred to as model #5 multiclass whole lesion in the manuscript), before and after recalibration based on the test set <span class="citation" data-cites="GAmanuscript">(Salvi et al., 2023)</span>.</p>
<p>Table&nbsp;2 shows the metric estimates for the model in the test set. The first row of Table&nbsp;2 shows the metric estimates prior to recalibration. The model is imperfectly calibrated, demonstrated visually with the calibration plot in Figure&nbsp;7 and with the relatively high estimate of the miscalibration index (<img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathrm%7BMI%7D%7D=0.338">). However, the estimated discrimination index is relatively high, <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathrm%7BDI%7D%7D=0.678">. The fundamental decomposition tells us that <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathrm%7BR%5E2%7D%7D"> could reach this value if the model were calibrated. The shape of the calibration curve and the low estimate of the nonlinearity index (<img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathrm%7BNI%7D%7D=0.030">) indicate that linear recalibration would go a long way in improving the model’s accuracy</p>
<center>
<div id="fig-calplot" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://github.com/stats-4-datascience/blog/posts/three_metrics/images/blog_plot1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;7: Predicted vs observed outcomes in the test set for the whole lesion model from Salvi et al.</figcaption><p></p>
</figure>
</div>
</center>
<div id="tbl-gametrics" class="anchored">
<table class="table">
<caption>Table&nbsp;2: Performance metrics estimated on the test set based on original, linearly recalibrated and recalibrated results</caption>
<colgroup>
<col style="width: 9%">
<col style="width: 17%">
<col style="width: 17%">
<col style="width: 17%">
<col style="width: 17%">
<col style="width: 17%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Predictor</th>
<th style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathrm%7BR%5E2%7D%7D"></th>
<th style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathrm%7Br%5E2%7D%7D"></th>
<th style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathrm%7BDI%7D%7D"></th>
<th style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathrm%7BMI%7D%7D"></th>
<th style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathrm%7BNI%7D%7D"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?0.338"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?0.648"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?0.678"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?0.338"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?0.030"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?L(%5Chat%7BY%7D)"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?0.648"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?0.648"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?0.678"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?0.030"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?0.030"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?C(%5Chat%7BY%7D)"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?0.678"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?0.678"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?0.678"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?0.000"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?0.000"></td>
</tr>
</tbody>
</table>
</div>
<p>The second row of Table&nbsp;2 shows the metric estimates after linearly recalibrating the model. The miscalibration index decreases to <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathrm%7BMI%7D%7D=0.030"> and <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathrm%7BR%5E2%7D%7D"> increases to <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathrm%7BR%5E2%7D%7D=%5Cwidehat%7B%5Cmathrm%7Br%5E2%7D%7D=0.648">, as expected. Since recalibration does not affect discrimination, <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathrm%7BDI%7D%7D"> remains the same. Finally, the bottom row of Table&nbsp;2 shows the metric estimates after recalibrating the model with the estimated calibration curve (the red gam curve in Figure&nbsp;7). As expected, after transforming the predictions via <img src="https://latex.codecogs.com/png.latex?C(%5Chat%7BY%7D)">, <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathrm%7BMI%7D%7D=0"> and <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathrm%7BR%5E2%7D%7D"> has increased to <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathrm%7BDI%7D%7D">.</p>
</section>
</section>
<section id="summarykey-points" class="level1">
<h1>Summary/Key Points</h1>
<ul>
<li><p>The general quality of predictions is assessed by accuracy, calibration, and discrimination. These three evaluation domains are often assessed completely independently of each other.</p></li>
<li><p>We propose an approach that unifies these concepts via a scaled decomposition of accuracy into miscalibration and discrimination, <img src="https://latex.codecogs.com/png.latex?R%5E2%20=%20%5Cmathrm%7BDI%7D%20-%20%5Cmathrm%7BMI%7D">. This decomposition is unitless and clarifies that <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D"> is the accuracy when there is no miscalibration, equivalently when the predictions are re-calibrated using the calibration curve.</p></li>
<li><p>Interestingly, <img src="https://latex.codecogs.com/png.latex?r%5E2"> can be interpreted as the <img src="https://latex.codecogs.com/png.latex?R%5E2"> for the best linear recalibration. That metric is also inherently of interest for some applications such as covariate adjustment.</p></li>
<li><p>The three key metrics are <img src="https://latex.codecogs.com/png.latex?R%5E2">, <img src="https://latex.codecogs.com/png.latex?r%5E2"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D"> and they satisfy a key inequality, <img src="https://latex.codecogs.com/png.latex?R%5E2%20%5Cleq%20r%5E2%20%5Cleq%20DI">. The remaining metrics of <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMI%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BNI%7D"> are derived from these.</p></li>
<li><p>Discrimination can never be improved via recalibration, but miscalibration and accuracy can. These metrics are very informative for assessing how much accuracy can be improved via linear recalibration and recalibration via the calibration curve.</p></li>
</ul>
<p>To understand how the metrics and decompositions apply to binary outcomes and generalize beyond quadratic loss, see <a href="https://schiffmc.pages.roche.com/stats4datascience/posts/three_metrics_binary/">Part 2 of this post</a>.</p>
</section>
<section id="appendix" class="level1">
<h1>Appendix</h1>
<section id="appendix-a" class="level2">
<h2 class="anchored" data-anchor-id="appendix-a">Appendix A</h2>
<section id="a-note-on-the-intercept-and-slope-of-lhaty" class="level4">
<h4 class="anchored" data-anchor-id="a-note-on-the-intercept-and-slope-of-lhaty"><em>A Note on the Intercept and Slope of <img src="https://latex.codecogs.com/png.latex?L(%5Chat%7BY%7D)"></em></h4>
<p>Calibration is often assessed by reporting the intercept and slope of the calibration line <img src="https://latex.codecogs.com/png.latex?L(%5Chat%7BY%7D)"> <span class="citation" data-cites="Steyerberg2014 CalSlope calhierarchy">(Calster et al., 2016; Stevensa et al., 2020; Steyerberg et al., 2014)</span>. An intercept of 0 and slope of 1 correspond to perfect calibration if the calibration curve is linear. The intercept of <img src="https://latex.codecogs.com/png.latex?L(%5Chat%7BY%7D)"> is generally interpreted as a measure of “calibration in the large”, i.e.&nbsp;how different the mean outcome is from the mean prediction. The slope of <img src="https://latex.codecogs.com/png.latex?L(%5Chat%7BY%7D)"> can be interpreted as a measure of overfitting (slope &lt; 1) or underfitting (slope &gt; 1) during model development and when performing internal validation.</p>
<p>However, as metrics for miscalibration, the intercept and slope of <img src="https://latex.codecogs.com/png.latex?L(%5Chat%7BY%7D)"> have limitations. If the calibration function is nonlinear, calibration performance may not be well assessed by the calibration line. The <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMI%7D"> metric discussed above, however, is appropriate for any shape the calibration function may have. The presence of nonlinear miscalibration can be assessed via a Nonlinearity Index <img src="https://latex.codecogs.com/png.latex?NI%20=%20DI%20-%20r%5E2">. <img src="https://latex.codecogs.com/png.latex?NI%20%3E=%200"> by the fundamental inequality and <img src="https://latex.codecogs.com/png.latex?NI=0"> if and only if the calibration curve is linear. When <img src="https://latex.codecogs.com/png.latex?NI%3E0">, it indicates how much recalibration via the calibration curve improves accuracy over recalibration via the calibration line.</p>
<p>Furthermore, since there are two parameters, model comparisons for miscalibration using the intercept and slope of <img src="https://latex.codecogs.com/png.latex?L(%5Chat%7BY%7D)"> are only partially ordered since they cannot be compared based on these parameters if one has a better intercept and a worse slope than the other. <img src="https://latex.codecogs.com/png.latex?MI">, on the other hand, is a single metric that enables comparison of any two models.</p>
<p>Even if the calibration curve is linear and the prediction model is calibrated in the large, there is an additional issue: the calibration intercept and slope do not take into account the distribution of the predicted outcomes. Unless the slope of the calibration line is 1, the discrepancies between the predicted outcomes and the calibration line will depend on the predicted outcomes and the distribution of discrepancies will depend on the distribution of predictions. <img src="https://latex.codecogs.com/png.latex?MI"> accounts for this since it captures the expected squared discrepancies over the distribution of predictions, but the intercept and slope of <img src="https://latex.codecogs.com/png.latex?L(%5Chat%7BY%7D)"> do not.</p>
<div class="cell">

</div>
<p>For these reasons, we prefer to do the following to gain a comprehensive understanding of miscalibration:</p>
<ul>
<li>Focus on <img src="https://latex.codecogs.com/png.latex?R%5E2">, <img src="https://latex.codecogs.com/png.latex?r%5E2">, <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMI%7D"> and <img src="https://latex.codecogs.com/png.latex?NI"></li>
<li>If the evaluation data set is too small to estimate a flexible calibration curve, estimate a calibration line and calculate the above metrics using that line as the calibration curve. Since <img src="https://latex.codecogs.com/png.latex?r%5E2%20=%20DI"> and <img src="https://latex.codecogs.com/png.latex?NI%20=%200"> in this case, only a subset of the above metrics needs to be reported. Note, <img src="https://latex.codecogs.com/png.latex?MI"> should still be reported even if recalibration was done with a linear function.</li>
<li>If <img src="https://latex.codecogs.com/png.latex?NI%20%5Capprox%200">, the intercept and slope of <img src="https://latex.codecogs.com/png.latex?L(%5Chat%7BY%7D)"> may also be reported for the purposes of describing or approximating the calibration curve.</li>
</ul>
</section>
</section>
<section id="appendix-b" class="level2">
<h2 class="anchored" data-anchor-id="appendix-b">Appendix B</h2>
<section id="proof-that-predictions-transformed-via-the-calibration-curve-are-calibrated" class="level4">
<h4 class="anchored" data-anchor-id="proof-that-predictions-transformed-via-the-calibration-curve-are-calibrated"><em>Proof that predictions transformed via the calibration curve are calibrated</em></h4>
<p>If a model is miscalibrated, its predictions can be transformed via the calibration curve <img src="https://latex.codecogs.com/png.latex?C_%7BY,%5Chat%7BY%7D%7D">. To show this, note that the calibration curve for the new predictions <img src="https://latex.codecogs.com/png.latex?C_%7BY,%5Chat%7BY%7D%7D(%5Chat%7BY%7D)"> is the identity function:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%20%20%5Cmathrm%7BC%7D_%7BY,%5Cmathrm%7BC%7D_%7BY,%5Chat%7BY%7D%7D(%5Chat%7BY%7D)%7D(%5Cmathrm%7BC%7D_%7BY,%5Chat%7BY%7D%7D(%5Chat%7BY%7D))%20&amp;=%20%5Cmathbb%7BE%7D%5Cbig%5BY%20%5Cmid%20%5Cmathrm%7BC%7D_%7BY,%5Chat%7BY%7D%7D(%5Chat%7BY%7D)%5Cbig%5D%20%5Cnotag%20%5C%5C%0A%20%20&amp;=%20%5Cmathbb%7BE%7D%5Cbig%5BY%20%5Cmid%20%5Cmathbb%7BE%7D%5BY%20%5Cmid%20%5Chat%7BY%7D%5D%5Cbig%5D%20%5Cnotag%20%5C%5C%0A%20%20&amp;=%20%5Cmathbb%7BE%7D%5Cbig%5B%5Cmathbb%7BE%7D%5Cbig%5BY%20%5Cmid%20%5Chat%7BY%7D,%5Cmathbb%7BE%7D%5BY%20%5Cmid%20%5Chat%7BY%7D%5D%5Cbig%5D%20%5Cmid%20%5Cmathbb%7BE%7D%5BY%20%5Cmid%20%5Chat%7BY%7D%5D%5Cbig%5D%20%5Cnotag%20%5C%5C%0A%20%20&amp;=%20%5Cmathbb%7BE%7D%5Cbig%5B%5Cmathbb%7BE%7D%5BY%20%5Cmid%20%5Chat%7BY%7D%5D%20%5Cmid%20%5Cmathbb%7BE%7D%5BY%20%5Cmid%20%5Chat%7BY%7D%5D%5Cbig%5D%20%5Cnotag%20%5C%5C%0A%20%20&amp;=%20%5Cmathbb%7BE%7D%5BY%20%5Cmid%20%5Chat%7BY%7D%5D%20%5Cnotag%20%5C%5C%0A%20%20&amp;=%20%5Cmathrm%7BC%7D_%7BY,%5Chat%7BY%7D%7D(%5Chat%7BY%7D)%0A%5Cend%7Balign%7D%0A"></p>
<p>And therefore <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMI%7D=0"> after transforming the original predictions with <img src="https://latex.codecogs.com/png.latex?C_%7BY,%5Chat%7BY%7D%7D">:</p>
<p><span id="eq-MI-cal"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%20%20%5Cmathrm%7BMI%7D%20&amp;=%20%20%5Cfrac%7B%5Cmathbb%7BE%7D%5CBig%5B%5CBig(%5Cmathrm%7BC%7D_%7BY,%5Cmathrm%7BC%7D_%7BY,%5Chat%7BY%7D%7D(%5Chat%7BY%7D)%7D(%5Cmathrm%7BC%7D_%7BY,%5Chat%7BY%7D%7D(%5Chat%7BY%7D))%20-%20%5Cmathrm%7BC%7D_%7BY,%5Chat%7BY%7D%7D(%5Chat%7BY%7D)%5CBig)%5E2%5Cbig%5D%7D%7BVar(Y)%7D%20%5C%5C%0A%20%20&amp;=%20%5Cfrac%7B%5Cmathbb%7BE%7D%5CBig%5B%5CBig(%5Cmathrm%7BC%7D_%7BY,%5Chat%7BY%7D%7D(%5Chat%7BY%7D)%20-%20%5Cmathrm%7BC%7D_%7BY,%5Chat%7BY%7D%7D(%5Chat%7BY%7D)%5CBig)%5E2%5Cbig%5D%7D%7BVar(Y)%7D%20%5Cnotag%20%5C%5C%0A%20%20&amp;=%200%0A%5Cend%7Balign%7D%0A%5Ctag%7B9%7D"></span></p>
</section>
<section id="proof-that-0leqmathrmdileq1" class="level4">
<h4 class="anchored" data-anchor-id="proof-that-0leqmathrmdileq1"><em>Proof that <img src="https://latex.codecogs.com/png.latex?0%5Cleq%5Cmathrm%7BDI%7D%5Cleq1"></em></h4>
<p>In general, the <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D"> is a non-negative number that can be at most <img src="https://latex.codecogs.com/png.latex?1"> since</p>
<p><span id="eq-DI-leq1"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%20%20%5Cmathrm%7BDI%7D%20&amp;=%20%20%5Cfrac%7BVar%5CBig(%20%5Cmathrm%7BC%7D_%7BY,%5Chat%7BY%7D%7D(%5Chat%7BY%7D)%5CBig)%7D%7BVar(Y)%7D%20%5C%5C%0A%20%20&amp;=%20%5Cfrac%7BVar(%5Cmathbb%7BE%7D%5BY%7C%5Chat%7BY%7D%5D)%7D%7BVar(Y)%7D%20%5C%5C%0A%20%20&amp;=%20%5Cfrac%7BVary(Y)%20-%20%5Cmathbb%7BE%7D(Var%5BY%7C%5Chat%7BY%7D%5D)%7D%7BVar(Y)%7D%20%5C%5C%0A%20%20&amp;%5Cleq%201%0A%5Cend%7Balign%7D%0A%5Ctag%7B10%7D"></span></p>
</section>
<section id="proof-that-transformations-do-not-affect-mathrmdi" class="level4">
<h4 class="anchored" data-anchor-id="proof-that-transformations-do-not-affect-mathrmdi"><em>Proof that transformations do not affect <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BDI%7D"></em></h4>
<p><span id="eq-DI-cal"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%20%20DI%20%5Cspace%20of%20%5Cspace%20calibrated%20%5Cspace%20predictions%20&amp;=%20%5Cfrac%7BVar%5Cbig(%5Cmathrm%7BC%7D_%7BY,%5Cmathrm%7BC%7D_%7BY,%5Chat%7BY%7D%7D(%5Chat%7BY%7D)%7D(%5Cmathrm%7BC%7D_%7BY,%5Chat%7BY%7D%7D(%5Chat%7BY%7D))%5Cbig)%7D%7BVar(Y)%7D%20%5Cnotag%20%5C%5C%0A%20%20&amp;=%20%5Cfrac%7BVar%5Cbig(%5Cmathrm%7BC%7D_%7BY,%5Chat%7BY%7D%7D(%5Chat%7BY%7D)%5Cbig)%7D%7BVar(Y)%7D%20%5Cnotag%20%5C%5C%0A%20%20&amp;=%20DI%20%5Cspace%20of%20%5Cspace%20original%20%5Cspace%20predictions%0A%5Cend%7Balign%7D%0A%5Ctag%7B11%7D"></span></p>
</section>
<section id="proof-of-the-mathrmmse-decomposition" class="level4">
<h4 class="anchored" data-anchor-id="proof-of-the-mathrmmse-decomposition"><em>Proof of the <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BMSE%7D"> decomposition</em></h4>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%20%20%5Cmathrm%7BMSE%7D%20&amp;=%20%5Cmathbb%7BE%7D%5Cbig%5B(Y-%5Chat%7BY%7D)%5E2%5Cbig%5D%20%5C%5C%0A&amp;=%20%5Cmathbb%7BE%7D%5Cbig%5B%5Cmathbb%7BE%7D%5B(Y-%5Chat%7BY%7D)%5E2%20%5Cmid%20%5Chat%7BY%7D%5D%5Cbig%5D%20%5C%5C%0A&amp;=%20%5Cmathbb%7BE%7D%5Cbig%5B%5Cmathbb%7BE%7D%5B(Y-%5Cmathbb%7BE%7D%5BY%20%5Cmid%20%5Chat%7BY%7D%5D%20+%5Cmathbb%7BE%7D%5BY%20%5Cmid%20%5Chat%7BY%7D%5D%20-%20%5Chat%7BY%7D)%5E2%20%5Cmid%20%5Chat%7BY%7D%5D%5Cbig%5D%20%5C%5C%0A&amp;=%20%5Cmathbb%7BE%7D%5Cbig%5B%5Cmathbb%7BE%7D%5B(Y-%5Cmathbb%7BE%7D%5BY%20%5Cmid%20%5Chat%7BY%7D%5D)%5E2%20%5Cmid%20%5Chat%7BY%7D%5D%5Cbig%5D%20+%5Cmathbb%7BE%7D%5Cbig%5B%5Cmathbb%7BE%7D%5B(%5Cmathbb%7BE%7D%5BY%20%5Cmid%20%5Chat%7BY%7D%5D%20-%20%5Chat%7BY%7D)%5E2%20%5Cmid%20%5Chat%7BY%7D%5D%5Cbig%5D%20%5C%5C%0A&amp;=%20%5Cmathbb%7BE%7D%5Cbig%5BVar(Y%20%5Cmid%20%5Chat%7BY%7D)%5Cbig%5D%20+%5Cmathbb%7BE%7D%5Cbig%5B(%5Cmathbb%7BE%7D%5BY%20%5Cmid%20%5Chat%7BY%7D%5D%20-%20%5Chat%7BY%7D)%5E2%20%5Cbig%5D%20%5C%5C%0A&amp;=%20Var(Y)%20-%20Var(%5Cmathbb%7BE%7D%5BY%20%5Cmid%20%5Chat%7BY%7D%5D)%20+%20%5Cmathbb%7BE%7D%5Cbig%5B(%5Cmathbb%7BE%7D%5BY%20%5Cmid%20%5Chat%7BY%7D%5D%20-%20%5Chat%7BY%7D)%5E2%20%5Cbig%5D%20%5C%5C%0A&amp;=%20Var(Y)%20-%20Var(%5Cmathrm%7BC%7D_%7BY,%5Chat%7BY%7D%7D(%5Chat%7BY%7D))%20+%20%5Cmathbb%7BE%7D%5Cbig%5B(%5Cmathrm%7BC%7D_%7BY,%5Chat%7BY%7D%7D(%5Chat%7BY%7D)%20-%20%5Chat%7BY%7D)%5E2%20%5Cbig%5D%20%5C%5C%0A%5Cend%7Balign%7D%0A"></p>
</section>
<section id="proof-of-the-fundamental-decomposition-of-r2" class="level4">
<h4 class="anchored" data-anchor-id="proof-of-the-fundamental-decomposition-of-r2"><em>Proof of the fundamental decomposition of <img src="https://latex.codecogs.com/png.latex?R%5E2"></em></h4>
<p><span id="eq-R2-decomp"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%20%20R%5E2&amp;%20=%201-%5Cfrac%7B%5Cmathrm%7BMSE%7D%7D%7BVar(Y)%7D%20%5C%5C%0A&amp;%20=%201-%5Cfrac%7B%5Cmathrm%7BVar(Y)%20-%20Var(%5Cmathrm%7BC%7D_%7BY,%5Chat%7BY%7D%7D(%5Chat%7BY%7D))%20+%20%5Cmathbb%7BE%7D%5Cbig%5B(%5Cmathrm%7BC%7D_%7BY,%5Chat%7BY%7D%7D(%5Chat%7BY%7D)%20-%20%5Chat%7BY%7D)%5E2%20%5Cbig%5D%7D%7D%7BVar(Y)%7D%20%5C%5C%0A&amp;=%20%5Cfrac%7BVar(%5Cmathrm%7BC%7D_%7BY,%5Chat%7BY%7D%7D(%5Chat%7BY%7D))%7D%7BVar(Y)%7D%20-%20%20%5Cfrac%7B%5Cmathbb%7BE%7D%5Cbig%5B(%5Cmathrm%7BC%7D_%7BY,%5Chat%7BY%7D%7D(%5Chat%7BY%7D)%20-%20%5Chat%7BY%7D)%5E2%20%5Cbig%5D%7D%7BVar(Y)%7D%20%5C%5C%0A&amp;=%20DI%20-%20MI%0A%5Cend%7Balign%7D%0A%5Ctag%7B12%7D"></span></p>
</section>
<section id="proof-that-mathrmc_yhatyhaty-maximizes-r2" class="level4">
<h4 class="anchored" data-anchor-id="proof-that-mathrmc_yhatyhaty-maximizes-r2"><em>Proof that <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BC%7D_%7BY,%5Chat%7BY%7D%7D(%5Chat%7BY%7D)"> maximizes <img src="https://latex.codecogs.com/png.latex?R%5E2"></em></h4>
<div class="callout-definition callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
The calibration curve is the transformation that maximizes <img src="https://latex.codecogs.com/png.latex?R%5E2">
</div>
</div>
<div class="callout-body-container callout-body">
<p><span id="eq-opt-trans"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AC(%5Chat%7BY%7D)%20&amp;=%20argmax_%7Bh%20%5Cin%20H%7D%20%5Cspace%201-%5Cfrac%7B%5Cmathrm%7BMSE%7D%7D%7BVar(Y)%7D%20%5C%5C%0A&amp;=argmin_%7Bh%20%5Cin%20H%7D%20%5Cspace%20%5Cmathbb%7BE%7D%5Cbig%5B%5Cbig(Y-h(%5Chat%7BY%7D)%5Cbig)%5E2%5Cbig%5D%0A%5Cend%7Balign%7D%0A%5Ctag%7B13%7D"></span></p>
</div>
</div>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0Aargmax_%7Bh%20%5Cin%20H%7D%20%5Cspace%201-%5Cfrac%7B%5Cmathrm%7BMSE%7D%7D%7BVar(Y)%7D%20&amp;=argmin_%7Bh%20%5Cin%20H%7D%20%5Cspace%20%5Cmathbb%7BE%7D%5Cbig%5B%5Cbig(Y-h(%5Chat%7BY%7D)%5Cbig)%5E2%5Cbig%5D%20%5C%5C%0A&amp;=%20argmin_%7Bh%20%5Cin%20H%7D%20%5Cspace%20%5Cmathbb%7BE%7D%5Cbig%5B%5Cbig(Y-%5Cmathbb%7BE%7D%5BY%7C%5Chat%7BY%7D%5D+%5Cmathbb%7BE%7D%5BY%7C%5Chat%7BY%7D%5D-h(%5Chat%7BY%7D)%5Cbig)%5E2%5Cbig%5D%20%5C%5C%0A&amp;=%20argmin_%7Bh%20%5Cin%20H%7D%20%5Cspace%20%5Cmathbb%7BE%7D%5Cbig%5B%5Cbig(Y-%5Cmathbb%7BE%7D%5BY%7C%5Chat%7BY%7D%5D%5Cbig)%5E2%20+%20%5C%5C%20%20&amp;2%5Cbig(Y-%5Cmathbb%7BE%7D%5BY%7C%5Chat%7BY%7D%5D%5Cbig)%5Cbig(%5Cmathbb%7BE%7D%5BY%7C%5Chat%7BY%7D%5D-h(%5Chat%7BY%7D)%5Cbig)%20+%20%5Cbig(%5Cmathbb%7BE%7D%5BY%7C%5Chat%7BY%7D%5D-h(%5Chat%7BY%7D)%5Cbig)%5E2%5Cbig%5D%20%5C%5C%0A&amp;=%20argmin_%7Bh%20%5Cin%20H%7D%20%5Cspace%20%5Cmathbb%7BE%7D%5Cbig%5B%5Cbig(%5Cmathbb%7BE%7D%5BY%7C%5Chat%7BY%7D%5D-h(%5Chat%7BY%7D)%5Cbig)%5E2%5Cbig%5D%20%5C%5C%0A&amp;=%20%5Cmathbb%7BE%7D%5BY%7C%5Chat%7BY%7D%5D%0A%5Cend%7Balign%7D%0A"></p>
</section>
<section id="proof-that-r2r2-when-predictions-are-transformed-via-the-calibration-line-lhaty" class="level4">
<h4 class="anchored" data-anchor-id="proof-that-r2r2-when-predictions-are-transformed-via-the-calibration-line-lhaty"><em>Proof that <img src="https://latex.codecogs.com/png.latex?R%5E2=r%5E2"> when predictions are transformed via the calibration line <img src="https://latex.codecogs.com/png.latex?L(%5Chat%7BY%7D)"></em></h4>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AR%5E2%20%5Cspace%20for%20%5Cspace%20L(%5Chat%7BY%7D)%20&amp;=%201-%5Cfrac%7B%5Cmathbb%7BE%7D%5Cbig%5B%5Cbig(Y-L(%5Chat%7BY%7D)%5Cbig)%5E2%5Cbig%5D%7D%7BVar(Y)%7D%20%5C%5C%0A&amp;=%20%5Cfrac%7BVar(Y)%20-%20%5Cmathbb%7BE%7D%5Cbig%5B%5Cbig(Y-%5Cmathbb%7BE%7D(Y)+%5Cfrac%7BCov(Y,%5Chat%7BY%7D)%7D%7BVar(%5Chat%7BY%7D)%7D%5Cmathbb%7BE%7D(%5Chat%7BY%7D)-%5Cfrac%7BCov(Y,%5Chat%7BY%7D)%7D%7BVar(%5Chat%7BY%7D)%7D%5Chat%7BY%7D%5Cbig)%5E2%5Cbig%5D%7D%7BVar(Y)%7D%20%5C%5C%0A&amp;=%20%5Cfrac%7BVar(Y)%20-%20%5Cmathbb%7BE%7D%5Cbig%5BVar(Y)%20-2%5Cfrac%7BCov(Y,%5Chat%7BY%7D)%7D%7BVar(%5Chat%7BY%7D)%7DCov(Y,%5Chat%7BY%7D)+%5Cfrac%7BCov%5E2(Y,%5Chat%7BY%7D)%7D%7BVar%5E2(%5Chat%7BY%7D)%7DVar(%5Chat%7BY%7D)%5Cbig%5D%7D%7BVar(Y)%7D%20%5C%5C%0A&amp;=%20%5Cfrac%7BCov%5E2(Y,%5Chat%7BY%7D)%7D%7BVar(%5Chat%7BY%7D)Var(Y)%7D%20%5C%5C%0A&amp;=%20cor%5E2(Y,%20%5Chat%7BY%7D)%20%5C%5C%0A%5Cend%7Balign%7D%0A"></p>
</section>
<section id="proof-that-lhaty-is-the-population-least-squares-regression-line" class="level4">
<h4 class="anchored" data-anchor-id="proof-that-lhaty-is-the-population-least-squares-regression-line"><em>Proof that <img src="https://latex.codecogs.com/png.latex?L(%5Chat%7BY%7D)"> is the population least squares regression line</em></h4>
<p>Let <img src="https://latex.codecogs.com/png.latex?Y"> be an <img src="https://latex.codecogs.com/png.latex?n%5Ctimes1"> outcome vector and <img src="https://latex.codecogs.com/png.latex?X"> be an <img src="https://latex.codecogs.com/png.latex?n%5Ctimes2"> design matrix with an intercept, and suppose you want to approximate <img src="https://latex.codecogs.com/png.latex?E(Y%7CX)"> with a simple linear function of <img src="https://latex.codecogs.com/png.latex?X">:</p>
<p><img src="https://latex.codecogs.com/png.latex?E(Y%7CX)%20%5Capprox%20Xb"></p>
<p>The coefficients that minimize the expected squared error of the approximation are called the population least squares regression coefficients <img src="https://latex.codecogs.com/png.latex?%5Cbeta">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cbeta%20=%20argmin_%7Bb%20%5Cin%20%5Cmathbb%7BR%7D%5E2%7D%5Cmathbb%7BE%7D%5Cbig(E(Y%7CX)-Xb%5Cbig)%5E2"></p>
<p>Taking the derivative w.r.t <img src="https://latex.codecogs.com/png.latex?b"> and setting equal to zero:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cmathbb%7BE%7D%5Cbig(-2X'E(Y%7CX)%20+%202(X'X)%5Cbeta%20%5Cbig)=0%20%5C%5C%0A%5Cbeta=%5Cmathbb%7BE%7D(X'X)%5E%7B-1%7D%5Cmathbb%7BE%7D(X'Y)%20%5C%5C%20=%5Cbigg(%5Cmathbb%7BE%7D(Y)-%5Cfrac%7BCov(Y,%5Chat%7BY%7D)%7D%7BVar(%5Chat%7BY%7D)%7D%5Cmathbb%7BE%7D(%5Chat%7BY%7D),%5Cfrac%7BCov(Y,%5Chat%7BY%7D)%7D%7BVar(%5Chat%7BY%7D)%7D%5Cbigg)'%20%5C%5C%0A=(%5Calpha_%7Bopt%7D,%5Cbeta_%7Bopt%7D)'%20%5C%5C%0A%5Cend%7Balign%7D%0A"></p>



</section>
</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-line-spacing="2">
<div id="ref-Ayer" class="csl-entry">
Ayer, M., Brunk, H. D., Ewing, G. M., Reid, W. T., &amp; Silverman, E. (1955). An empirical distribution function for sampling with incomplete information. <em>Ann. Math. Statist.</em>
</div>
<div id="ref-Calster2019" class="csl-entry">
Calster, B. V., McLernon, D. J., Smeden, M. van, Wynants, L., &amp; Steyerberg, E. W. (2019). Calibration: The achilles heel of predictive analytics. <em>BMC Medicine</em>.
</div>
<div id="ref-calhierarchy" class="csl-entry">
Calster, B. V., Nieboer, D., Vergouwe, Y., Cock, B. D., Pencina, M. J., &amp; Steyerberg, E. W. (2016). A calibration hierarchy for risk models was defined: From utopia to empirical data. <em>Journal of Clinical Epidemiology</em>.
</div>
<div id="ref-Calster2018" class="csl-entry">
Calster, B. V., Wynants, L., Verbeek, J. F. M., Verbakel, J. Y., Christodoulou, E., Vickers, A. J., Roobol, M. J., &amp; Steyerberg, E. W. (2018). Reporting and interpreting decision curve analysis: A guide for investigators. <em>European Urology</em>.
</div>
<div id="ref-DeGroot" class="csl-entry">
DeGroot, M. H., &amp; Fienberg, S. E. (1983). Comparing probability forecasters: Basic binary concepts and multivariate extensions. <em>Defense Technical Information Center</em>.
</div>
<div id="ref-Dimitriadis" class="csl-entry">
Dimitriadis, T., Gneitingb, T., &amp; Jordan, A. I. (2021). Stable reliability diagrams for probabilistic classifiers. <em>PNAS</em>.
</div>
<div id="ref-Fissler" class="csl-entry">
Fissler, T., Lorentzen, C., &amp; Mayer, M. (2022). Model comparison and calibration assessment. <em>arXiv</em>.
</div>
<div id="ref-Leeuw" class="csl-entry">
Leeuw, J. de, Hornik, K., &amp; Mair, P. (2009). Isotone optimization in r: Pool-adjacent-violators algorithm (PAVA) and active set methods. <em>Journal of Statistical Software</em>.
</div>
<div id="ref-Murphy1973" class="csl-entry">
Murphy, A. H. (1973). A new vector partition of the probability score. <em>Journal of Applied Meteorology and Climatology</em>.
</div>
<div id="ref-GAmanuscript" class="csl-entry">
Salvi, A., Cluceru, J., Gao, S., Rabe, C., Yang, Q., Lee, A., Keane, P., Sadda, S., Holz, F. G., Ferrara, D., &amp; Anegondi, N. (2023). Deep learning to predict the future growth of geographic atrophy from fundus autofluorescence. <em>In Review</em>.
</div>
<div id="ref-schiffman" class="csl-entry">
Schiffman, C., Friesenhahn, M., &amp; Rabe, C. (2023, October 26). <em>How to Get the Most Out of Prognostic Baseline Variables in Clinical Trials</em>. Retrieved from <a href="https://www.stats4datascience.com/posts/covariate_adjustment/">https://www.stats4datascience.com/posts/covariate_adjustment/</a>
</div>
<div id="ref-CalSlope" class="csl-entry">
Stevensa, R. J., &amp; Poppe, K. K. (2020). Validation of clinical prediction models: What does the "calibration slope"" really measure? <em>Journal of Clinical Epidemiology</em>.
</div>
<div id="ref-Steyerberg2014" class="csl-entry">
Steyerberg, E. W., &amp; Vergouwe, Y. (2014). Towards better clinical prediction models: Seven steps for development and an ABCD for validation. <em>European Heart Journal</em>.
</div>
<div id="ref-Steyerberg2010" class="csl-entry">
Steyerberg, E. W., Vickers, A. J., Cook, N. R., Gerds, T., Gonen, M., Obuchowski, N., Pencina, M. J., &amp; Michael W. Kattan, and. (2010). Assessing the performance of prediction models: A framework for some traditional and novel measures. <em>Epidemiology</em>.
</div>
<div id="ref-Vickers" class="csl-entry">
Vickers, A. J., &amp; Elkin, E. B. (2006). Decision curve analysis: A novel method for evaluating prediction models. <em>Med Decis Making</em>.
</div>
</div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{friesenhahnandchristinarabeandcourtneyschiffman2023,
  author = {Michel Friesenhahn and Christina Rabe and Courtney
    Schiffman},
  title = {Everything You Wanted to Know about {R2} but Were Afraid to
    Ask. {Part} 1, {Using} a Fundamental Decomposition to Gain Insights
    into Predictive Model Performance.},
  date = {2023-10-26},
  url = {stats4datascience.com},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-friesenhahnandchristinarabeandcourtneyschiffman2023" class="csl-entry quarto-appendix-citeas">
Michel Friesenhahn and Christina Rabe and Courtney Schiffman. (2023,
October 26). <em>Everything you wanted to know about R2 but were afraid
to ask. Part 1, Using a fundamental decomposition to gain insights into
predictive model performance.</em> Retrieved from <a href="https://stats4datascience.com">stats4datascience.com</a>
</div></div></section></div> ]]></description>
  <category>machine learning</category>
  <category>model perormance</category>
  <guid>https://github.com/stats-4-datascience/blog/posts/three_metrics/index.html</guid>
  <pubDate>Thu, 26 Oct 2023 00:00:00 GMT</pubDate>
  <media:content url="https://github.com/stats-4-datascience/blog/posts/three_metrics/images/quadrants_numbers_v2.PNG" medium="image"/>
</item>
<item>
  <title>How to get the most out of prognostic baseline variables in clinical trials</title>
  <dc:creator>Courtney Schiffman</dc:creator>
  <dc:creator>Christina Rabe</dc:creator>
  <dc:creator>Michel Friesenhahn</dc:creator>
  <link>https://github.com/stats-4-datascience/blog/posts/covariate_adjustment/index.html</link>
  <description><![CDATA[ 




<div class="cell">

</div>
<section id="new-insights-into-an-old-method" class="level1">
<h1>New Insights into an Old Method</h1>
<center>
<p><img src="https://github.com/stats-4-datascience/blog/posts/covariate_adjustment/images/new_insights.png" class="img-fluid" style="width:60.0%"></p>
</center>
<p>In randomized trials, a wide range of baseline covariates are collected prior to randomization. These include demographic variables, baseline outcome assessments, biomarkers from biological samples and medical images. Baseline covariates that are prognostic for trial outcomes are often used for enrichment, stratified randomization, imbalance checking and subgroup analyses. However, it is not widely recognized that one of the most powerful uses of prognostic baseline variables in randomized clinical trials is for covariate adjustment. <em>A pre-specified primary endpoint analysis that adjusts for prognostic baseline covariates is a rigorous and virtually risk-free analysis that increases precision and power.</em></p>
<p>While the concept of covariate adjustment and its potential value for increasing trial power is decades old, historically there has been some controversy around how to ensure the validity of trial analyses without relying on model assumptions. This has been particularly challenging for non-continuous endpoints. Fortunately, there have been advancements that have resolved this controversy. Our first aim for this blog is to build on those advancements and provide concrete guidance on how to perform treatment effect estimation and inference using covariate adjustment that is both rigorous and easy-to-implement. In general, standard ANCOVA-based inference is correct <em>when the regression model is correctly specified</em>, whereas the covariate adjustment methods proposed here are asymptotically robust to model misspecification.</p>
<p>Furthermore, the pre-specified covariates that are currently used in clinical trials can be sub-optimal and strategies are needed to better determine which baseline covariates are best to use for adjustment. These baseline covariates could be a simple collection of individual variables or a single prediction from a complex machine learning model. External data, e.g.&nbsp;data from historical trials and observational studies, are an excellent resource for identifying and evaluating such candidate covariates. Accordingly, our second aim is to provide strategic guidance and relevant performance metrics for these evaluations.</p>
<p>This document will be a practical how-to guide for clinical trial statisticians on effective, trustworthy, and practical use of covariate adjustment in clinical development for maximizing power and minimizing bias. We will also discuss the value of stratified randomization compared to covariate adjustment. We focus here on continuous outcomes, but many of the points discussed apply to binary and ordinal outcomes as well.</p>
</section>
<section id="covariate-adjustment-has-gotten-hip" class="level1">
<h1>Covariate Adjustment Has Gotten Hip</h1>
<center>
<p><img src="https://github.com/stats-4-datascience/blog/posts/covariate_adjustment/images/collage.png" class="img-fluid" style="width:80.0%"></p>
</center>
<p>Covariate adjustment has recently gained popularity in a variety of industries. In drug development, the FDA has provided a <a href="https://www.fda.gov/regulatory-information/search-fda-guidance-documents/adjusting-covariates-randomized-clinical-trials-drugs-and-biological-products">draft guidance document</a> that strongly encourages its use. There are also several start-ups who offer covariate adjustment to optimize analysis of online experiments as a key use case for their prognostic models <span class="citation" data-cites="Netflix CUPED CUPAC MLRATE">(Wong, Lewis, and Wardrop 2019; Deng et al. 2013; Li, Tang, and Bauman 2020; Guo et al. 2022)</span>. Across these different applications, what statisticians and the FDA call covariate adjustment is frequently given alternative names, for example CUPED (controlled-experiment using pre-experiment data), CUPAC (Control Using Predictions As Covariates) and MLRATE (machine learning regression-adjusted treatment effect estimator). Companies like Owkin and Unlearn.AI focus on the use of covariate adjustment in randomized trials, hoping to potentially decrease trial sample size by incorporating machine learning <span class="citation" data-cites="owkin unlearn">(Trower, Balazard, and Patel 2020; Schuler et al. 2020)</span>.</p>
<p>We heartily celebrate this increased enthusiasm for covariate adjustment in a variety of settings and hope the current blog post enables clinical trial statisticians to make the most effective use of baseline prognostic data.</p>
</section>
<section id="whats-the-big-deal-about-covariate-adjustment" class="level1">
<h1>What’s the Big Deal About Covariate Adjustment?</h1>
<section id="covariate-adjustment-increases-precision-and-power" class="level2">
<h2 class="anchored" data-anchor-id="covariate-adjustment-increases-precision-and-power">Covariate Adjustment Increases Precision and Power</h2>
<div id="fig-cov-adj" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://github.com/stats-4-datascience/blog/posts/covariate_adjustment/images/increased_precision.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: Covariate adjustment removes explained variability</figcaption><p></p>
</figure>
</div>
<p>Unexplained variability in the outcome data limits the precision with which you can estimate your marginal average treatment effect (ATE). If baseline covariates are prognostic, subtracting off any reasonable linear combination of those covariates from the observed outcomes, and analyzing the differences instead reduces variability of the treatment effect estimate. The marginal ATE estimate calculated using the residuals (observed-predicted) is still asymptotically unbiased, because the expected value of the predictions is equal across all treatment arms. However, the residuals have less variability than the original outcomes, leading to a more precise estimate.</p>
</section>
<section id="covariate-adjustment-removes-the-impact-of-imbalance" class="level2">
<h2 class="anchored" data-anchor-id="covariate-adjustment-removes-the-impact-of-imbalance">Covariate Adjustment Removes the Impact of Imbalance</h2>
<p>A common concern in randomized trials is that treatment groups, by chance, differ in the distributions of one or more important prognostic baseline covariates. When this occurs, one perspective is that this is not a problem since the estimates are still unbiased and type 1 error is still preserved. However, such statistical properties are unconditional on the observed covariate imbalance. When we do condition on observed imbalance, the treatment effect estimates can be biased and type 1 errors can drastically differ from their nominal levels. A simple, clear and insightful analysis of this issue was provided by Stephen Senn who has amusingly remarked “If you are at 35,000 ft, four engines are on fire and the captain has had a heart-attack can you say: ‘Why worry, on average air travel is very safe?’” <span class="citation" data-cites="Senn Senn2010">(S. J. Senn 1989; S. Senn 2010)</span>.</p>
<div id="fig-imbalence" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://github.com/stats-4-datascience/blog/posts/covariate_adjustment/images/imbalance.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2: Conditional type-1 error depending on imbalance</figcaption><p></p>
</figure>
</div>
<p>How do you address concerns over imbalance? Following the analysis from Stephen Senn, the figure above shows the conditional type 1 error as a function of standardized observed imbalance for a range of assumed correlations between the baseline covariate and outcome. This shows that significance tests for imbalance (a commonly used and misguided practice) can fail to flag imbalance effects even when the conditional probability of a type 1 error well exceeds the nominal level. Therefore, significance tests do not reliably control type 1 error. Instead, Senn showed that covariate adjustment removes conditional bias from treatment effect estimates and leads to correct conditional type 1 errors of constant size.</p>
<blockquote class="blockquote">
<p>“Analysis of covariance can be recommended on two grounds: increased power and constant conditional size. The former should be sufficient to recommend the method to those who consider that the latter is irrelevant but for those who are concerned about conditional size this is an added advantage” <span class="citation" data-cites="Senn">(S. J. Senn 1989)</span>.</p>
</blockquote>
</section>
<section id="covariate-adjustment-is-rigorous" class="level2">
<h2 class="anchored" data-anchor-id="covariate-adjustment-is-rigorous">Covariate Adjustment is Rigorous</h2>
<p>To address the historical confusion and controversy around covariate adjustment, methodological development has clearly specified the sampling framework, target estimand (i.e.&nbsp;population level parameter) and required assumptions. While we focus on continuous outcomes, the rigor of covariate adjustment also applies to non-continuous outcomes.</p>
<section id="sampling-framework" class="level3">
<h3 class="anchored" data-anchor-id="sampling-framework">Sampling framework</h3>
<p>An important but often overlooked component of specifying an estimand and estimator is the assumed sampling framework. A rigorous, adjusted analysis makes the assumed sampling framework transparent. There are several choices for the sampling framework, but we assume here the commonly used super-population framework. Let <img src="https://latex.codecogs.com/png.latex?N"> be the total sample size for a randomized trial with several treatment arms and let <img src="https://latex.codecogs.com/png.latex?A=0,1,%5Cldots,a"> be a random variable denoting treatment arm assignment. In the super-population framework, <img src="https://latex.codecogs.com/png.latex?N"> full data vectors are assumed to be drawn independently from some unknown, joint distribution. Patient <img src="https://latex.codecogs.com/png.latex?i's"> full data vector <img src="https://latex.codecogs.com/png.latex?(X_i,%20Y_i(0),Y_i(1),%5Cldots,Y_i(a))"> contains baseline covariates <img src="https://latex.codecogs.com/png.latex?X_i"> and a potential outcome <img src="https://latex.codecogs.com/png.latex?Y_i(k)"> for each possible treatment assignment <img src="https://latex.codecogs.com/png.latex?k=0,1,%5Cldots,a">. Patient <img src="https://latex.codecogs.com/png.latex?i's"> observed data vector <img src="https://latex.codecogs.com/png.latex?(A_i,%20X_i,%20Y_i)"> consists of their treatment assignment <img src="https://latex.codecogs.com/png.latex?A_i">, baseline covariates <img src="https://latex.codecogs.com/png.latex?X_i"> and <em>observed</em> outcome <img src="https://latex.codecogs.com/png.latex?Y_i=Y_i(0)I(A_i=0)+%5Cldots+Y_i(a)I(A_i=a)">. In the case of simple random sampling, <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?X"> are independent.</p>
<p>It is worth noting that rigorous theory behind covariate adjustment has also been developed under the Neyman framework, which assumes a fixed, finite population where the only source of randomness is treatment assignment <span class="citation" data-cites="lin2013 Peng Ding">(Lin 2013; Ding, Li, and Miratrix 2017, 2019)</span>.</p>
</section>
<section id="target-estimand" class="level3">
<h3 class="anchored" data-anchor-id="target-estimand">Target Estimand</h3>
<p>Confusion often arises since there are various kinds of covariate adjusted analyses targeting different estimands. For example, covariate adjustment is often used in the estimation of conditional average treatment effects, which are contrasts in treatment arm means that are conditional on baseline covariates. A conditional treatment effect is not a single value, but rather a function of baseline covariates, unless one makes the assumption that the conditional treatment effect is constant.</p>
<div class="callout-definition callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Conditional Average Treatment Effect
</div>
</div>
<div class="callout-body-container callout-body">
<p>A contrast (difference, ratio, etc.) between treatment arm means conditional on baseline covariates. Example of a conditional ATE:</p>
<p><span id="eq-cond-avg-trt-eff"><img src="https://latex.codecogs.com/png.latex?%0AE(Y_%7Bactive%7D%7CX)%20-%20E(Y_%7Bcontrol%7D%7CX)%0A%5Ctag%7B1%7D"></span></p>
</div>
</div>
<p>However, in this document, we do not use covariate adjustment for that purpose. Instead, we us <strong>covariate adjustment as a tool to more efficiently estimate the marginal ATE</strong>:</p>
<div class="callout-definition callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Marginal Average Treatment Effect
</div>
</div>
<div class="callout-body-container callout-body">
<p>A contrast (difference, ratio, etc.) between marginal treatment arm means. Example of a marginal ATE:</p>
<p><span id="eq-marg-avg-trt-eff"><img src="https://latex.codecogs.com/png.latex?%0AE(Y_%7Bactive%7D)%20-%20E(Y_%7Bcontrol%7D)%0A%5Ctag%7B2%7D"></span></p>
</div>
</div>
<p>Some researchers suggest focusing on estimating conditional treatment effects and using covariate adjustment for that purpose. While estimating conditional and individualized treatment effects is an important research objective, a discussion about how to do so is beyond the scope of this work. Here, we assume estimating the marginal ATE is the primary objective of a label-enabling clinical trial.</p>
<p>We note that the terms ‘marginal’ and ‘conditional’ are often conflated with ‘unadjusted’ and ‘adjusted’, however these terms are not synonymous <span class="citation" data-cites="Daniel">(Daniel, Zhang, and Farewell 2020)</span>:</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Marginal vs.&nbsp;Conditional
</div>
</div>
<div class="callout-body-container callout-body">
<p>Used to distinguish different kinds of estimands.</p>
</div>
</div>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Unadjusted vs.&nbsp;Adjusted
</div>
</div>
<div class="callout-body-container callout-body">
<p>Used to distinguish different kinds of estimators.</p>
</div>
</div>
</section>
<section id="assumptions" class="level3">
<h3 class="anchored" data-anchor-id="assumptions">Assumptions</h3>
<p>Historically, researchers have debated how to safely estimate marginal ATEs using covariate adjustment. Some have been hesitant to use covariate adjustment over concerns around model misspecification, bias and possible losses in precision <span class="citation" data-cites="Freedman2008a Freedman2008b">(D. A. Freedman 2008a; David A. Freedman 2008b)</span>, while others have enthusiastically used covariate adjustment without a second thought.</p>
<p>Academic research ultimately settled that debate <span class="citation" data-cites="Tsiatis2001 Tsiatis2008 Rosenblum2009 TMLE2010">(Yang and Tsiatis 2001; Tsiatis et al. 2008; Rosenblum and Laan 2009; Rosenblum and Laan 2010)</span>. A number of approaches to covariate adjustment have been characterized that result in consistent, asymptotically normal estimators of marginal ATEs even if the regression model is misspecified. This means that even if important interactions or non-linearities are excluded, prognostic factors are missed, or factors are included that have no prognostic value, covariate adjustment consistently estimates the marginal ATE.</p>
<p>The covariate adjustment approach we use is called a standardized regression estimator, which uses predictions from a working regression model to obtain marginal ATEs. The model is referred to as a working regression model because it does not have to represent the true data generating distribution. Therefore, we refer to covariate adjustment as a <strong>model-assisted, not model-dependent, analysis</strong>. Examples of working regression models include logistic, OLS, and Poisson regression models.</p>
<p>For the standardized regression estimator, it is sufficient to assume that treatment is assigned via simple random sampling, that the full data vectors are i.i.d. and that all variables are bounded <span class="citation" data-cites="Rosenblum2009 TMLE2010">(Rosenblum and Laan 2009; Rosenblum and Laan 2010)</span>. Extensions to other sampling frameworks have also been explored <span class="citation" data-cites="Wang2021">(Wang et al. 2021)</span>.</p>
</section>
</section>
<section id="covariate-adjustment-is-encouraged-by-the-fda" class="level2">
<h2 class="anchored" data-anchor-id="covariate-adjustment-is-encouraged-by-the-fda">Covariate Adjustment is Encouraged by the FDA</h2>
<p>In a 2021 <a href="https://www.fda.gov/regulatory-information/search-fda-guidance-documents/adjusting-covariates-randomized-clinical-trials-drugs-and-biological-products">draft guidance</a>, pre-specified covariate adjustment was fully supported and encouraged by the FDA as a primary endpoint analysis in clinical trials:</p>
<blockquote class="blockquote">
<p>“Although an unadjusted analysis is acceptable for the primary analysis, adjustment for baseline covariates will generally reduce the variability of estimation of treatment effects and thus lead to narrower confidence intervals and more powerful hypothesis testing” <span class="citation" data-cites="FDA">(FDA 2021)</span>.</p>
</blockquote>
<p>Importantly, the guidance states that “[s]ponsors should prospectively specify the covariates and the mathematical form of the covariate adjusted estimator” <span class="citation" data-cites="FDA">(FDA 2021)</span>. We interpret this to mean that the exact working regression model form should be pre-specified before working with the trial data. Independent data from observational cohorts or previous trials can be used to select or develop covariates. These covariates may be existing baseline variables or even the output from a multivariable prediction model which takes as input other baseline covariates.</p>
<p>In contrast to pre-specifying an ANCOVA regression model, there is active research into applying machine learning directly to the unblinded trial data under investigation (as opposed to historical data) <span class="citation" data-cites="covid2 covid Tsiatis2008 Tian2012">(Williams, Rosenblum, and Diaz 2021; Benkeser et al. 2020; Tsiatis et al. 2008; Tian et al. 2012)</span> as part of the covariate adjusted analysis. While the role machine learning procedures can play in covariate adjustment is an interesting and important area of research, we do not yet recommend using them as part of the primary endpoint analysis of a clinical trial due to several considerations. First, the finite sample properties of the estimators are not understood. Second, these methods add complexity and burden to the study yet are unlikely to provide meaningful benefits.</p>
<p>It may not be appreciated that a lot of things need to happen between the unblinding of a randomized clinical trial and the reporting of topline results. Outputs and code are quality controlled, pre-specified sensitivity analyses are performed, and internal reviews are conducted with key decision makers and stakeholders. All of these activities need to occur in a tight time frame for practical considerations such as appropriate control of insider information.</p>
<p>In addition, during the planning phase, many choices have to be made when applying machine learning directly to the trial data such as which covariates to present to the machine learning procedure, the method of cross validation to use, which machines to include if using ensemble learning and how to perform standard error estimation. These choices, many of which are based on simulation studies, add to the difficulty of finalizing the statistical analysis plan.</p>
<p>In summary, applying machine learning to the trial data as part of the primary endpoint analysis adds considerable burden to the study team. <em>This added burden would only be worthwhile if the expected precision gains were meaningful</em>. However, in our experience, complex black-box models rarely outperform simple linear models when working with tabular data and do not support this additional complexity and time.</p>
</section>
</section>
<section id="how-to-perform-covariate-adjustment" class="level1 page-columns page-full">
<h1>How to Perform Covariate Adjustment</h1>
<section id="standardized-regression-estimator" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="standardized-regression-estimator">Standardized Regression Estimator</h2>
<p>Table 1 provides instructions and code for estimating the Treatment Arm Means (TAMs) and the marginal ATE (in this case, the difference in means) using covariate adjustment. The example code shows how to estimate these quantities for trials with two treatment arms, but easily generalizes to more than two arms. This estimator is often referred to as the <strong>standardized regression estimator</strong>, and is also an example of a targeted maximum likelihood estimator. There are other consistent estimators of the marginal ATE that use covariate adjustment (see <span class="citation" data-cites="Colantuoni2015">Colantuoni and Rosenblum (2015)</span> for examples). However, we suggest using the standardized regression estimator because it is easy to use, “statistically reliable” <span class="citation" data-cites="FDA">(FDA 2021)</span>, has comparable power and can also be used to estimate TAMS and marginal ATEs for non-continuous outcomes.</p>
<p>In the notation below, <img src="https://latex.codecogs.com/png.latex?A"> is a two-level factor indicating treatment assignment, <img src="https://latex.codecogs.com/png.latex?Y"> is the observed outcome, <img src="https://latex.codecogs.com/png.latex?X"> is a baseline covariate, <img src="https://latex.codecogs.com/png.latex?E%5BY%7Ca,%20X_i%5D%20=%20%5Cmu(a,X_i)"> is the expected value of <img src="https://latex.codecogs.com/png.latex?Y"> for patient <img src="https://latex.codecogs.com/png.latex?i"> given their treatment assignment <img src="https://latex.codecogs.com/png.latex?A_i=a"> and baseline covariate <img src="https://latex.codecogs.com/png.latex?X_i">, <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cmu%7D(a,X_i)"> is the predicted outcome for patient <img src="https://latex.codecogs.com/png.latex?i"> from the working regression model and an estimate of <img src="https://latex.codecogs.com/png.latex?%5Cmu(a,X_i)">, and <img src="https://latex.codecogs.com/png.latex?N"> is the total trial sample size. The standardized regression estimator for the mean outcome in treatment arm <img src="https://latex.codecogs.com/png.latex?A=a"> is:</p>
<p><span id="eq-std-reg-mean"><img src="https://latex.codecogs.com/png.latex?%0A%5Cwidehat%7BTAM_a%7D=%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi=1%7D%5E%7BN%7D%5Chat%7B%5Cmu%7D(a,X_i)%0A%5Ctag%7B3%7D"></span></p>
<p>The standardized regression estimator for the difference in means for treatment arms <img src="https://latex.codecogs.com/png.latex?A=0"> and <img src="https://latex.codecogs.com/png.latex?A=1"> is:</p>
<p><span id="eq-std-reg-diff"><img src="https://latex.codecogs.com/png.latex?%0A%5Cwidehat%7BATE%7D=%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi=1%7D%5E%7BN%7D%5Chat%7B%5Cmu%7D(1,X_i)-%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi=1%7D%5E%7BN%7D%5Chat%7B%5Cmu%7D(0,X_i)%0A%5Ctag%7B4%7D"></span></p>
<p>Other contrasts of the marginal means (TAMs) may be similarily estimated.</p>
<div class="column-page-inset">
<div id="tbl-estimation" class="anchored">
<table class="table">
<caption>Table&nbsp;1: Standardized regression estimator for TAMs and marginal ATE</caption>
<colgroup>
<col style="width: 5%">
<col style="width: 27%">
<col style="width: 64%">
<col style="width: 0%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Estimation Instructions</th>
<th colspan="2">Example R Code</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Fit a working regression model using data from all treatment arms, regressing the outcome on treatment and prognostic baseline covariates</td>
<td><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="co" style="color: #5E5E5E;"># Additive Working Regression Models</span></span>
<span id="cb1-2">mod.fit <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">lm</span>(Y <span class="sc" style="color: #5E5E5E;">~</span> A <span class="sc" style="color: #5E5E5E;">+</span> X, <span class="at" style="color: #657422;">data =</span> d)  <span class="co" style="color: #5E5E5E;"># continuous</span></span>
<span id="cb1-3">mod.fit <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">glm</span>(Y <span class="sc" style="color: #5E5E5E;">~</span> A <span class="sc" style="color: #5E5E5E;">+</span> X, <span class="at" style="color: #657422;">data =</span> d, <span class="at" style="color: #657422;">family =</span> binomial)  <span class="co" style="color: #5E5E5E;"># binary</span></span>
<span id="cb1-4"></span>
<span id="cb1-5"><span class="co" style="color: #5E5E5E;"># Working Regression Models with Interaction</span></span>
<span id="cb1-6">mod.fit <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">lm</span>(Y <span class="sc" style="color: #5E5E5E;">~</span> A <span class="sc" style="color: #5E5E5E;">*</span> X, <span class="at" style="color: #657422;">data =</span> d)  <span class="co" style="color: #5E5E5E;"># continuous</span></span>
<span id="cb1-7">mod.fit <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">glm</span>(Y <span class="sc" style="color: #5E5E5E;">~</span> A <span class="sc" style="color: #5E5E5E;">*</span> X, <span class="at" style="color: #657422;">data =</span> d, <span class="at" style="color: #657422;">family =</span> binomial)  <span class="co" style="color: #5E5E5E;"># binary</span></span></code></pre></div></td>
<td></td>
</tr>
<tr class="even">
<td>2</td>
<td>For each subject, use the model from step 1 and the subject’s baseline covariates to compute their predicted outcome under EACH treatment of interest (regardless of what their assigned treatment was)</td>
<td><div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><span class="co" style="color: #5E5E5E;"># Continuous outcome</span></span>
<span id="cb2-2">pred0 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">predict</span>(mod.fit, <span class="at" style="color: #657422;">newdata =</span> <span class="fu" style="color: #4758AB;">data.frame</span>(<span class="at" style="color: #657422;">A=</span><span class="dv" style="color: #AD0000;">0</span>, <span class="at" style="color: #657422;">X=</span>d<span class="sc" style="color: #5E5E5E;">$</span>X))</span>
<span id="cb2-3">pred1 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">predict</span>(mod.fit, <span class="at" style="color: #657422;">newdata =</span> <span class="fu" style="color: #4758AB;">data.frame</span>(<span class="at" style="color: #657422;">A=</span><span class="dv" style="color: #AD0000;">1</span>, <span class="at" style="color: #657422;">X=</span>d<span class="sc" style="color: #5E5E5E;">$</span>X))</span>
<span id="cb2-4"></span>
<span id="cb2-5"><span class="co" style="color: #5E5E5E;"># Binary outcome</span></span>
<span id="cb2-6">pred0 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">predict</span>(mod.fit, <span class="at" style="color: #657422;">newdata =</span> <span class="fu" style="color: #4758AB;">data.frame</span>(<span class="at" style="color: #657422;">A=</span><span class="dv" style="color: #AD0000;">0</span>, <span class="at" style="color: #657422;">X=</span>d<span class="sc" style="color: #5E5E5E;">$</span>X), <span class="at" style="color: #657422;">type=</span>‘response’)</span>
<span id="cb2-7">pred1 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">predict</span>(mod.fit, <span class="at" style="color: #657422;">newdata =</span> <span class="fu" style="color: #4758AB;">data.frame</span>(<span class="at" style="color: #657422;">A=</span><span class="dv" style="color: #AD0000;">1</span>, <span class="at" style="color: #657422;">X=</span>d<span class="sc" style="color: #5E5E5E;">$</span>X, <span class="at" style="color: #657422;">type=</span>‘response’)</span></code></pre></div></td>
<td></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Take the average of the predicted outcomes in each treatment group to get estimates of the Treatment Arm Means (TAMs)</td>
<td><div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><span class="co" style="color: #5E5E5E;"># Mean of predictions in treatment arm A=0</span></span>
<span id="cb3-2">TAM0 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">mean</span>(pred0)</span>
<span id="cb3-3"></span>
<span id="cb3-4"><span class="co" style="color: #5E5E5E;"># Mean of predictions in treatment arm A=1</span></span>
<span id="cb3-5">TAM1 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">mean</span>(pred1)</span></code></pre></div></td>
<td></td>
</tr>
<tr class="even">
<td>4</td>
<td>Compute the desired contrast of the TAMs to get an estimate of the marginal ATE</td>
<td><div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><span class="co" style="color: #5E5E5E;"># Estimate the desired contrast, for example, the</span></span>
<span id="cb4-2"><span class="co" style="color: #5E5E5E;"># difference between treatment arm A=1 and A=0</span></span>
<span id="cb4-3">ATE <span class="ot" style="color: #003B4F;">&lt;-</span> TAM1 <span class="sc" style="color: #5E5E5E;">-</span> TAM0</span></code></pre></div></td>
<td></td>
</tr>
</tbody>
</table>
</div>
</div>
<p>In the example code in Table 1, <img src="https://latex.codecogs.com/png.latex?X"> is a single covariate, but <img src="https://latex.codecogs.com/png.latex?X"> can also be a matrix containing a set of individual covariates, a prediction from an independent prognostic model, or a combination thereof. When the working regression model is an additive OLS regression, the standardized regression estimate of the difference in means marginal ATE equals the estimated coefficient for <img src="https://latex.codecogs.com/png.latex?A">. This is also true for an interaction model when the covariates are centered. Note that, unlike OLS regression, in generalized linear models the estimated coefficient for treatment does not translate into a marginal ATE.</p>
<p>Theoretically, it is possible to lose precision with covariate adjustment when using an additive model compared to an unadjusted analysis. In contrast, a standardized regression estimator that uses an interaction working regression model is asymptotically guaranteed to be at least as precise as an unadjusted estimator or adjustment with an additive model. However, it is only under unrealistic conditions that covariate adjustment with an additive model will be less precise than an unadjusted estimator. What is more, if randomization is 1:1 or the covariances between covariates and outcome are equal across treatment arms, then covariate adjustment with an additive model will be as efficient as with an interaction model. In our experience, interaction models do not lead to considerable gains in precision over additive models. Likewise, Tsiatis et al. (2001) performed simulations comparing precision when using an additive vs.&nbsp;an interaction model and “found the loss of efficiency to be trivial in all cases” <span class="citation" data-cites="Tsiatis2001">(Yang and Tsiatis 2001)</span>. Therefore, <strong>additive models are a good default choice</strong> especially if your trial sample size is too small to support treatment-by-covariate interactions (see model budget section).</p>
<p>In case of missing values in baseline covariates, the FDA draft guidance suggests that “covariate adjustment is generally robust to the handling of subjects with missing baseline covariates. Missing baseline covariate values can be singly or multiply imputed, or missingness indicators (Groenwold et al.&nbsp;2012) can be added to the model used for covariate adjustment. Sponsors should not perform imputation separately for different treatment groups, and sponsors should ensure that imputed baseline values are not dependent on any post-baseline variables, including the outcome” <span class="citation" data-cites="FDA">(FDA 2021)</span>. A baseline covariate that has too many missing values should not be used for adjustment, but if the number of missing values is small, we recommend simple, single imputation as opposed to using indicator variables in order to spend the model budget wisely.</p>
</section>
<section id="inference-for-tams-and-ates" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="inference-for-tams-and-ates">Inference for TAMs and ATEs</h2>
<p>We suggest using robust standard error estimators from the targeted maximum likelihood estimation (TMLE) literature because they are robust to model misspecification, are fast and easy to implement with a few lines of code, are valid under any randomization ratio and data generating distribution, provide standard error estimates for both ATEs and treatment arm means, are aligned with the superpopulation framework and are deterministic.</p>
<p>Let <img src="https://latex.codecogs.com/png.latex?I(A_i=a)"> be an indicator of patient <img src="https://latex.codecogs.com/png.latex?i"> receiving treatment <img src="https://latex.codecogs.com/png.latex?A=a"> and <img src="https://latex.codecogs.com/png.latex?p(A=a)"> be the probability of being assigned to treatment arm <img src="https://latex.codecogs.com/png.latex?A=a">. The TMLE <strong>standard error estimate for the mean of treatment arm <img src="https://latex.codecogs.com/png.latex?A=a"></strong> is:</p>
<p><span id="eq-std-err-est-mean"><img src="https://latex.codecogs.com/png.latex?%0A%5Csqrt%7B%5Cfrac%7B%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi=1%7D%5E%7BN%7D%5CBigg(%5Cfrac%7BI(A_i=a)(Y_i-%5Chat%7B%5Cmu%7D(a,X_i))%7D%7Bp(A=a)%7D+%5Chat%7B%5Cmu%7D(a,X_i)-%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi=1%7D%5E%7BN%7D%5Chat%7B%5Cmu%7D(a,X_i)%5CBigg)%5E2%7D%7BN%7D%7D%0A%5Ctag%7B5%7D"></span></p>
<p>The standard error estimate for the <strong>difference in Means for Treatment arms <img src="https://latex.codecogs.com/png.latex?A=1"> and <img src="https://latex.codecogs.com/png.latex?A=0"></strong> is:</p>
<div class="column-body-outset">
<p><span id="eq-std-err-est-diff"><img src="https://latex.codecogs.com/png.latex?%0A%5Csqrt%7B%5Cfrac%7B%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi=1%7D%5E%7BN%7D%5CBigg(%5Cfrac%7BI(A_i=1)(Y_i-%5Chat%7B%5Cmu%7D(1,X_i))%7D%7Bp(A=1)%7D+%5Chat%7B%5Cmu%7D(1,X_i)-%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi=1%7D%5E%7BN%7D%5Chat%7B%5Cmu%7D(1,X_i)%20-%20%5CBig(%5Cfrac%7BI(A_i=0)(Y_i-%5Chat%7B%5Cmu%7D(0,X_i))%7D%7Bp(A=0)%7D+%5Chat%7B%5Cmu%7D(0,X_i)-%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi=1%7D%5E%7BN%7D%5Chat%7B%5Cmu%7D(0,X_i)%5CBig)%20%5CBigg)%5E2%7D%7BN%7D%7D%0A%5Ctag%7B6%7D"></span></p>
</div>
<p>The code for the above SE estimators is shown below.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><span class="do" style="color: #5E5E5E;
font-style: italic;">## SE estimate for treatment arm mean A=0</span></span>
<span id="cb5-2"><span class="fu" style="color: #4758AB;">sqrt</span>(<span class="fu" style="color: #4758AB;">mean</span>(((A <span class="sc" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">0</span>) <span class="sc" style="color: #5E5E5E;">*</span> (Y <span class="sc" style="color: #5E5E5E;">-</span> pred0) <span class="sc" style="color: #5E5E5E;">/</span> <span class="fu" style="color: #4758AB;">mean</span>(A <span class="sc" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">0</span>) <span class="sc" style="color: #5E5E5E;">+</span> pred0 <span class="sc" style="color: #5E5E5E;">-</span> <span class="fu" style="color: #4758AB;">mean</span>(pred0)) <span class="sc" style="color: #5E5E5E;">^</span> <span class="dv" style="color: #AD0000;">2</span>) <span class="sc" style="color: #5E5E5E;">/</span> <span class="fu" style="color: #4758AB;">nrow</span>(d))</span>
<span id="cb5-3"></span>
<span id="cb5-4"><span class="do" style="color: #5E5E5E;
font-style: italic;">## SE estimate for treatment arm mean A=1</span></span>
<span id="cb5-5"><span class="fu" style="color: #4758AB;">sqrt</span>(<span class="fu" style="color: #4758AB;">mean</span>(((A <span class="sc" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">1</span>) <span class="sc" style="color: #5E5E5E;">*</span> (Y <span class="sc" style="color: #5E5E5E;">-</span> pred1) <span class="sc" style="color: #5E5E5E;">/</span> <span class="fu" style="color: #4758AB;">mean</span>(A <span class="sc" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">1</span>) <span class="sc" style="color: #5E5E5E;">+</span> pred1 <span class="sc" style="color: #5E5E5E;">-</span> <span class="fu" style="color: #4758AB;">mean</span>(pred1)) <span class="sc" style="color: #5E5E5E;">^</span> <span class="dv" style="color: #AD0000;">2</span>) <span class="sc" style="color: #5E5E5E;">/</span> <span class="fu" style="color: #4758AB;">nrow</span>(d))</span>
<span id="cb5-6"></span>
<span id="cb5-7"><span class="do" style="color: #5E5E5E;
font-style: italic;">## SE estimate for the marginal ATE, the difference between treatment arm mean A=1 and A=0</span></span>
<span id="cb5-8"><span class="fu" style="color: #4758AB;">sqrt</span>(<span class="fu" style="color: #4758AB;">mean</span>(((A <span class="sc" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">1</span>) <span class="sc" style="color: #5E5E5E;">*</span> (Y <span class="sc" style="color: #5E5E5E;">-</span> pred1) <span class="sc" style="color: #5E5E5E;">/</span> <span class="fu" style="color: #4758AB;">mean</span>(A <span class="sc" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">1</span>) <span class="sc" style="color: #5E5E5E;">+</span> pred1 <span class="sc" style="color: #5E5E5E;">-</span> <span class="fu" style="color: #4758AB;">mean</span>(pred1) <span class="sc" style="color: #5E5E5E;">-</span></span>
<span id="cb5-9">        ((A <span class="sc" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">0</span>) <span class="sc" style="color: #5E5E5E;">*</span> (Y <span class="sc" style="color: #5E5E5E;">-</span> pred0) <span class="sc" style="color: #5E5E5E;">/</span> <span class="fu" style="color: #4758AB;">mean</span>(A <span class="sc" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">0</span>) <span class="sc" style="color: #5E5E5E;">+</span> pred0 <span class="sc" style="color: #5E5E5E;">-</span> <span class="fu" style="color: #4758AB;">mean</span>(pred0))) <span class="sc" style="color: #5E5E5E;">^</span> <span class="dv" style="color: #AD0000;">2</span>) <span class="sc" style="color: #5E5E5E;">/</span> <span class="fu" style="color: #4758AB;">nrow</span>(d))</span></code></pre></div>
</div>
<p>Using the estimated standard errors and the asymptotic normality of the standardized regression estimator, one can then easily obtain <img src="https://latex.codecogs.com/png.latex?p">-values and <img src="https://latex.codecogs.com/png.latex?95%5C%25"> confidence intervals for both the treatment arm means and the marginal ATE.</p>
<p>We note that the above standard error estimates assume observations are i.i.d. (i.e.&nbsp;randomization is not stratified), and we address this assumption in a later section. For standard error estimates corresponding to marginal ATEs using different contrasts (e.g.&nbsp;ratio instead of difference) see Rosenblum and van der Laan (2010).</p>
<p>Alternative standard error estimators include the Huber-White estimator and re-sampling based methods like the non-parametric bootstrap. In their recent guidance, the FDA recommends using the Huber-White robust “sandwich” estimator instead of nominal standard error estimates from statistical software <span class="citation" data-cites="FDA">(FDA 2021)</span>. However, if using a working regression model with treatment-by-covariate interactions, the expected sandwich standard error estimates may be too small when treatment effects are heterogeneous (see Appendix and <span class="citation" data-cites="imbens">Imbens and Wooldridge (2009)</span> , <span class="citation" data-cites="lin2013">Lin (2013)</span>). Bootstrap methods are a reasonable alternative, but we prefer TMLE estimators since they are easier to implement and less computationally intensive. Furthermore, bootstrap methods lead to confidence intervals and p-values that are not deterministic. An intriguing alternative to the bootstrap that is deterministic is the jackknife <span class="citation" data-cites="jackknife">(Wolbers et al. 2022)</span>.</p>
</section>
<section id="determining-your-model-budget" class="level2">
<h2 class="anchored" data-anchor-id="determining-your-model-budget">Determining your Model Budget</h2>
<p>Is there such a thing as adjusting for too many prognostic covariates? Yes! The marvelous properties of covariate adjustment discussed thus far (consistency, robustness to model misspecification, precision gains, etc.) are <strong>large sample properties</strong>. Thus, adjusting for too many covariates in your working model relative to your sample size may invalidate the analysis <span class="citation" data-cites="ML Colantuoni2015 Steingrimsson2016">(Rosenblum 2020; Colantuoni and Rosenblum 2015; Rosenblum and Steingrimsson 2016)</span>, and the FDA advises against adjusting for too many covariates:</p>
<blockquote class="blockquote">
<p>“The statistical properties of covariate adjustment are best understood when the number of covariates adjusted for in the study is small relative to the sample size.”<span class="citation" data-cites="FDA">(FDA 2021)</span></p>
</blockquote>
<p>Explicit guidance on how many covariates to adjust for is not offered, so we borrow from prognostic modeling practices <span class="citation" data-cites="rms">(Harrell 2011)</span>.</p>
<div class="callout-definition callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Model Budget
</div>
</div>
<div class="callout-body-container callout-body">
<p>The number of allowable terms in the working regression model, excluding the overall intercept. Suggestions for a model budget based on classic events per variable considerations are <span class="citation" data-cites="rms">(Harrell 2011)</span>:</p>
<table class="table">
<colgroup>
<col style="width: 46%">
<col style="width: 7%">
<col style="width: 46%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: right;">terms (% of total sample size)</th>
<th style="text-align: center;"></th>
<th style="text-align: left;">guidance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;"><img src="https://latex.codecogs.com/png.latex?%5Cleq%205%5C%25"></td>
<td style="text-align: center;"><i class="bi bi-check-circle-fill text-success"></i></td>
<td style="text-align: left;">likely a safe choice</td>
</tr>
<tr class="even">
<td style="text-align: right;"><img src="https://latex.codecogs.com/png.latex?%5Capprox7.5%5C%25"></td>
<td style="text-align: center;"><i class="bi bi-exclamation-triangle-fill text-warning"></i></td>
<td style="text-align: left;">probably reasonable</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><img src="https://latex.codecogs.com/png.latex?%5Cgt%2010%5C%25"></td>
<td style="text-align: center;"><i class="bi bi-question-octagon-fill text-danger"></i></td>
<td style="text-align: left;">potentially unsafe</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Pre-specifying a set of covariates and a working regression model form that satisfies the model budget maintains the rigor and integrity of the statistical analysis. We note that this is a rough guidance and should be considered a useful starting point. Recently, model budget calculations for general prognostic modeling have been refined and in future work it would be helpful to adapt such an approach to covariate adjustment (see <span class="citation" data-cites="Riley2019">R. D. Riley et al. (2019)</span> and <span class="citation" data-cites="Riley2020">R. Riley et al. (2020)</span>).</p>
</section>
<section id="spending-your-model-budget-wisely" class="level2">
<h2 class="anchored" data-anchor-id="spending-your-model-budget-wisely">Spending your Model Budget Wisely</h2>
<center>
<p><img src="https://github.com/stats-4-datascience/blog/posts/covariate_adjustment/images/model_budget.png" class="img-fluid" style="width:60.0%"></p>
</center>
<p>The model budget tells you how complex the working regression model can be while ensuring treatment effect estimates are unbiased, confidence intervals have good coverage, and type 1 error rates for hypothesis testing do not exceed nominal levels. However, there are many options when pre-specifying the working regression model and it is easy to spend beyond one’s model budget, so strategic choices will need to be made prior to looking at the data.</p>
<p>We suggest determining the model form and spending the model budget according to the following four principles:</p>
<ol type="1">
<li><p><strong>Leverage external/historical data</strong><br>
Historically, pre-specification of a regression model was guided by ad hoc analyses reported in the literature, input from subject matter experts, and precedent set by previous trials. While these continue to be important considerations, big data collection and curation and the introduction of machine learning methods to drug development significantly improve the process of pre-specifying the regression model form.</p></li>
<li><p><strong>Prioritize maximally prognostic covariates</strong><br>
The list of candidate prognostic covariates may be too large according to the model budget, in which case a subset of prognostic covariates will need to be selected. We use external/historical data to guide that selection (see upcoming section on Performance Metrics).</p></li>
</ol>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Pro Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>If the model budget is tight and several covariates are expected to meaningfully add prognostic value, consider combining them into a single score and using that in the model instead of the individual covariates.</p>
</div>
</div>
<div id="fig-prog-mod" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://github.com/stats-4-datascience/blog/posts/covariate_adjustment/images/prognostic_modeling.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3: Leverage independent data to find the most promising prognostic factors/models for covariate adjustment</figcaption><p></p>
</figure>
</div>
<ol start="3" type="1">
<li><p><strong>Avoid Dichotomania!</strong><br>
Usually the relationship between a continuous variable and outcome varies gradually and abrupt dichotomization loses prognostic information <span class="citation" data-cites="Ge2011">Stephen J. Senn (2005)</span>. We recommend that, unless there is evidence to the contrary, prognostic covariates that have been discretized be replaced with their underlying continuous forms. Note that, in particular, this will impact how stratification factors enter into the model.</p></li>
<li><p><strong>By default, do not include interactions between covariates</strong><br>
There is a large body of evidence showing that for tabular data, purely additive models are very rarely, if ever, meaningfully improved by including interactions <span class="citation" data-cites="Rudin">(Rudin 2019)</span>. This has also been our experience with many prognostic modeling projects. We do not recommend including interactions between prognostic covariates in the model since these are usually wasteful expenses in the model budget, unless evidence to the contrary is available.</p></li>
<li><p><strong>By default, do not include interactions between covariates and treatment arm</strong><br>
As discussed above, such interactions rarely provide meaningfully improved precision and power under realistic assumptions.</p></li>
</ol>
<p>Use the above five principles to pre-specify a working regression model and to spend your model budget wisely. Then, use this model in the previously described standardized estimator to estimate the treatment arm means and marginal ATEs and to perform inference.</p>
</section>
<section id="accounting-for-stratified-randomization" class="level2">
<h2 class="anchored" data-anchor-id="accounting-for-stratified-randomization">Accounting for Stratified Randomization</h2>
<p>The estimation and inference procedures described in the previous sections assume <em>simple randomization</em>, where each patient’s treatment assignment is an independent and identically distributed Bernoulli random variable for a trial with two treatment arms and a categorical random variable for a trial with more than two treatment arms. However, randomization in clinical trials is usually stratified by selected baseline covariates, for example, using permuted block randomization. In this case, the FDA recommends that the standard error estimation <em>“account for the stratified randomization (Bugni et al.&nbsp;2018) with or without strata variables in an adjustment model. Otherwise, the standard error is likely to be overestimated and interval estimation and hypothesis testing can become unduly conservative”</em> <span class="citation" data-cites="FDA">(FDA 2021)</span>.</p>
<p><strong>However, we recommend that it is NOT necessary to modify estimation and inference to account for stratified randomization.</strong> Instead, we propose that the previously described strategy for spending the model budget wisely be used and applied to all prognostic candidates, making no distinction between stratification factors and other baseline covariates. Then use covariate adjustment as described in the previous sections for estimation and inference of the marginal ATEs, <em>as if treatment were allocated using simple random sampling</em>.</p>
<p>Note that, as a consequence:</p>
<ol type="1">
<li>By default, if stratification variables do enter into the working regression model, they will do so as additive terms.</li>
<li>Dichotomized stratification factors will be replaced by their underlying continuous variables.</li>
<li>Stratification variables not expected to be correlated with outcome in any treatment group may be omitted entirely.</li>
</ol>
<p>Such an approach still maintains type 1 error and leads to consistent estimators for marginal ATEs. At worst, inference is conservative (p-values may be too large and confidence intervals too wide) <span class="citation" data-cites="Wang2021">(Wang et al. 2021)</span>. However, if the model budget is spent wisely to obtain a sensible working regression model, conservatism will be negligible.</p>
</section>
</section>
<section id="performance-metrics" class="level1 page-columns page-full">
<h1>Performance Metrics</h1>
<center>
<p><img src="https://github.com/stats-4-datascience/blog/posts/covariate_adjustment/images/performance.png" class="img-fluid" style="width:50.0%"></p>
</center>
<section id="intuitive-metrics-for-quantifying-the-expected-benefit-of-adjustment" class="level2">
<h2 class="anchored" data-anchor-id="intuitive-metrics-for-quantifying-the-expected-benefit-of-adjustment">Intuitive Metrics for Quantifying the Expected Benefit of Adjustment</h2>
<p>When analyzing external/historical data in preparation for pre-specifying a covariate-adjusted primary endpoint analysis, it’s important to understand the following:</p>
<ol type="1">
<li>Which covariates to adjust for</li>
<li>The expected precision gains from adjustment</li>
</ol>
<p>Below, we provide performance metrics that allow study statisticians to easily select which covariates to adjust for, to understand the impact of heterogeneous treatment effects (HTEs) on precision gains and to estimate the expected benefit from adjustment <strong>without having to perform simulations</strong>.</p>
<p>A great way to derive a single, high-quality covariate is to use external data to construct a prognostic model that combines multiple covariates into a single prognostic score. Thus, we focus here on how to evaluate a single prognostic covariate. However, if the model budget allows and the prognostic model is originally linear in form, the individual covariates can be expanded out in the working regression model and re-fit, usually with no loss in performance.</p>
<p>For practical use in a trial, prognostic models should achieve <strong>maximum performance with minimal complexity.</strong> Complexity is determined by the:</p>
<ol type="1">
<li>Cost, incovenience, and invasiveness of covariates.</li>
<li>Ease of deployment and implementation of the prognostic model.</li>
<li>Algorithmic complexity (e.g.&nbsp;linear models vs deep learners).</li>
</ol>
<p>We find that linear prognostic models are hard to beat when working with tabular data. Deep learning models are more likely to provide a boost in performance over linear models when working with imaging data. Deep learners are natural candidates when working with non-tabular data, such as raw image files. Later we will show an example where considerable gains are achieved using a deep learner trained to predict progression in an ophthalmologic disease from raw images.</p>
<p>An intuitive metric for the expected benefit of a prognostic score is the amount by which it effectively increases the study’s sample size via the gain in precision, i.e.&nbsp;the <strong>Effective Sample Size Increase (ESSI)</strong>. For example, suppose adjusting for a particular covariate is associated with an estimated ESSI of <img src="https://latex.codecogs.com/png.latex?40%5C%25">. That means, asymptotically, the following two analyses have the same power:</p>
<ul>
<li>A covariate-adjusted analysis with sample size <img src="https://latex.codecogs.com/png.latex?N"></li>
<li>An unadjusted analysis but with <img src="https://latex.codecogs.com/png.latex?40%5C%25"> more patients, <img src="https://latex.codecogs.com/png.latex?1.4%20*%20N"></li>
</ul>
<p>ESSI estimates can be used to compare sets of covariates or prognostic models and make transparent the expected benefits of adjustment.</p>
</section>
<section id="essi-formulas" class="level2">
<h2 class="anchored" data-anchor-id="essi-formulas">ESSI Formulas</h2>
<p>For simplicity, we assume 1:1 randomization and equal, marginal variances across the treatment arms. We also provide ESSI formulas that relax these assumptions.</p>
<p>It turns out that the <strong>ESSI from covariate adjustment for continuous outcomes</strong> can be easily estimated since it only depends on two parameters:</p>
<div class="callout-definition callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
General ESSI
</div>
</div>
<div class="callout-body-container callout-body">
<p><span id="eq-essi-gen"><img src="https://latex.codecogs.com/png.latex?%0AGeneral%20%5Cspace%20ESSI=%5CBigg(%5Cfrac%7B1%7D%7B1%20-%20%5Cbig(%5Cfrac%7Br_%7Bcontrol%7D%20%5Cspace%20+%20%5Cspace%20r_%7Bactive%7D%7D%7B2%7D%5Cbig)%5E2%7D-1%5CBigg)%20*%20100%5C%25%0A%5Ctag%7B7%7D"></span></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%5Ctext%7Bwhere%7D%20%5C%5C%0A%20%20r_%7Bactive%7D%20&amp;=%20%5Ctext%7BCorrelation%20between%20the%20outcome%20and%20covariate%20in%20the%20active,%7D%20%5C%5C%0A%20%20r_%7Bcontrol%7D%20&amp;=%20%5Ctext%7BCorrelation%20between%20the%20outcome%20and%20covariate%20in%20the%20control%20arm%7D%0A%5Cend%7Balign*%7D%0A"></p>
</div>
</div>
<p>It makes intuitive sense that the ESSI depends on the squared average of the correlations in the two treatment arms, increasing as the correlations increase. The correlation between the covariate and the outcome among untreated patients (<img src="https://latex.codecogs.com/png.latex?r_%7Bcontrol%7D">) can be estimated using historical trial data, observational cohorts, etc.</p>
<p>This leaves <img src="https://latex.codecogs.com/png.latex?r_%7Bactive%7D"> as the only quantity left to estimate. To help estimate <img src="https://latex.codecogs.com/png.latex?r_%7Bactive%7D">, note that the correlation between the outcome and the covariate in the treatment arm depends on the “treatment effect scenario.” Two plausible treatment effect scenarios are discussed in the following sections.</p>
<section id="constant-absolute-treatment-effect" class="level3">
<h3 class="anchored" data-anchor-id="constant-absolute-treatment-effect">Constant absolute treatment effect</h3>
<p>One commonly assumed treatment effect scenario is that the <em>absolute</em> treatment effect is constant across the baseline covariate:</p>
<div class="callout-definition callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Constant absolute treatment effect:
</div>
</div>
<div class="callout-body-container callout-body">
<p><img src="https://latex.codecogs.com/png.latex?E(Y_%7Bactive%7D%7CX)=E(Y_%7Bcontrol%7D%7CX)-%5Cdelta">, for some constant <img src="https://latex.codecogs.com/png.latex?%5Cdelta">. With a constant absolute treatment effect, <img src="https://latex.codecogs.com/png.latex?r_%7Bcontrol%7D=r_%7Bactive%7D">. Plugging this into the general ESSI formula we get</p>
<p><span id="eq-essi-const-abs-delta"><img src="https://latex.codecogs.com/png.latex?%0AESSI%20%5Cspace%20Assuming%20%5Cspace%20Constant%20%5Cspace%20Absolute%20%5Cspace%20%5Cdelta%20=%5CBigg(%5Cfrac%7B1%7D%7B1-r%5E2_%7Bcontrol%7D%7D-1%5CBigg)%20*%20100%5C%25%0A%5Ctag%7B8%7D"></span></p>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-constant-ate" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://github.com/stats-4-datascience/blog/posts/covariate_adjustment/index_files/figure-html/fig-constant-ate-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;4: Constant Absolute Treatment Effect</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="constant-proportional-treatment-effect" class="level3">
<h3 class="anchored" data-anchor-id="constant-proportional-treatment-effect">Constant proportional treatment effect</h3>
<p>A second plausible treatment effect scenario is that the <em>relative</em> treatment effect is constant across the baseline covariate:</p>
<div class="callout-definition callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Constant proportional treatment effect
</div>
</div>
<div class="callout-body-container callout-body">
<p><img src="https://latex.codecogs.com/png.latex?E(Y_%7Bactive%7D%7CX)=(1-%5Cdelta)E(Y_%7Bcontrol%7D%7CX)">, for some constant <img src="https://latex.codecogs.com/png.latex?%5Cdelta">. With a constant proportional treatment effect, <img src="https://latex.codecogs.com/png.latex?r_%7Bactive%7D=(1-%5Cdelta)r_%7Bcontrol%7D">. Plugging this into the general ESSI formula we get</p>
<p><span id="eq-essi-const-prop-delta"><img src="https://latex.codecogs.com/png.latex?%0AESSI%5Cspace%20Assuming%20%5Cspace%20Constant%20%5Cspace%20Proportional%20%5Cspace%20%5Cdelta=%5CBigg(%5Cfrac%7B1%7D%7B1-r%5E2_%7Bcontrol%7D%5Cbig(1-%5Cfrac%7B%5Cdelta%7D%7B2%7D%5Cbig)%5E2%7D-1%5CBigg)%20*%20100%5C%25%0A%5Ctag%7B9%7D"></span></p>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-constant-pte" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://github.com/stats-4-datascience/blog/posts/covariate_adjustment/index_files/figure-html/fig-constant-pte-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5: Constant Proportional Treatment Effect</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="impact-of-htes-on-gains-from-covariate-adjustment" class="level2">
<h2 class="anchored" data-anchor-id="impact-of-htes-on-gains-from-covariate-adjustment">Impact of HTEs on gains from covariate adjustment</h2>
<p>A constant proportional treatment effect is an example of HTEs. A treatment effect is heterogeneous if the magnitude of the treatment effect (on the absolute scale) depends on the value of the baseline covariate. <strong>HTEs decrease the precision gains from adjustment</strong>. For example, notice how, in the constant proportional treatment effect formula, the ESSI decreases as <img src="https://latex.codecogs.com/png.latex?%5Cdelta"> increases (i.e.&nbsp;as heterogeneity increases). It is important to recognize the impact of HTEs when making decisions about reducing sample size in anticipation of increased power due to covariate adjustment.</p>
<div class="cell" data-layout-align="center">

</div>
</section>
<section id="a-hypothetical-example" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="a-hypothetical-example">A Hypothetical Example</h2>
<p>Suppose you are working on pre-specifying an adjusted analysis for an upcoming phase 2 trial. Assume that the targeted effect size is a <img src="https://latex.codecogs.com/png.latex?25%5C%25"> reduction in mean decline in the treatment arm compared to the mean decline in the placebo arm for a progressive disease that has a continuous outcome measure. Furthermore, assume that you have developed a prognostic model using external/historical data and that the estimated correlation between the prognostic model predictions and the outcome was <img src="https://latex.codecogs.com/png.latex?0.45"> (<img src="https://latex.codecogs.com/png.latex?r%5E2_%7Bcontrol%7D%5Capprox0.2">). The following is an example of how to calculate ESSIs from covariate adjustment under two plausible treatment effect scenarios that are consistent with the targeted effect size:</p>
<div class="column-body-outset">

<table class="table tg">
<thead>
<tr>
<th class="tg-baqh">
Treatment Effect Scenario
</th>
<th class="tg-baqh">
<img src="https://latex.codecogs.com/png.latex?r_%7Bcontrol%7D">
</th>
<th class="tg-baqh">
<img src="https://latex.codecogs.com/png.latex?r_%7Bactive%7D">
</th>
<th class="tg-baqh">
ESSI
</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-baqh">
Constant Absolute Delta<br><br><img src="https://latex.codecogs.com/png.latex?E(Y_%7Bactive%7D%7CX)=E(Y_%7Bcontrol%7D%7CX)-%5Cdelta">
</td>
<td class="tg-baqh">
<img src="https://latex.codecogs.com/png.latex?0.45">
</td>
<td class="tg-baqh">
<span style="font-weight:400;font-style:normal"><img src="https://latex.codecogs.com/png.latex?0.45"></span>
</td>
<td class="tg-baqh">
<img src="https://latex.codecogs.com/png.latex?%5CBig(%5Cfrac%7B1%7D%7B1-r%5E2_%7Bcontrol%7D%7D-1%5CBig)*100%5C%25="><br><img src="https://latex.codecogs.com/png.latex?%5CBig(%5Cfrac%7B1%7D%7B1-%7B0.2%7D%7D-1%5CBig)*100%5C%25%20="> <em><img src="https://latex.codecogs.com/png.latex?25%5C%25"></em>
</td>
</tr>
<tr>
<td class="tg-baqh">
Constant Proportional Delta <br><br><img src="https://latex.codecogs.com/png.latex?E(Y_%7Bactive%7D%7CX)=(1-%5Cdelta)E(Y_%7Bcontrol%7D%7CX)">
</td>
<td class="tg-baqh">
<span style="font-weight:400;font-style:normal"><img src="https://latex.codecogs.com/png.latex?0.45"></span>
</td>
<td class="tg-baqh">
<img src="https://latex.codecogs.com/png.latex?(1-%5Cdelta)r_%7Bcontrol%7D%20="><br><img src="https://latex.codecogs.com/png.latex?(1-.25)*0.45">
</td>
<td class="tg-baqh">
<img src="https://latex.codecogs.com/png.latex?%5CBigg(%5Cfrac%7B1%7D%7B1-r%5E2_%7Bcontrol%7D%20%5C;%20%5C;%20%5Cbig%20(%201-%5Cfrac%7B%5Cdelta%7D%7B2%7D%5Cbig)%5E2%7D-1%5CBigg)*100%5C%25="><br><img src="https://latex.codecogs.com/png.latex?%5CBigg(%5Cfrac%7B1%7D%7B1-0.2%5Cbig(1-%5Cfrac%7B%5Cdelta%7D%7B2%7D%5Cbig)%5E2%7D-1%5CBigg)*100%5C%25%20="> <em><img src="https://latex.codecogs.com/png.latex?18%5C%25"></em>
</td>
</tr>
<tr>
<td class="tg-baqh">
No Correlation in Treatment Arm
</td>
<td class="tg-baqh">
<span style="font-weight:400;font-style:normal"><img src="https://latex.codecogs.com/png.latex?0.45"></span>
</td>
<td class="tg-baqh">
<img src="https://latex.codecogs.com/png.latex?0">
</td>
<td class="tg-baqh">
<img src="https://latex.codecogs.com/png.latex?%5CBigg(%5Cfrac%7B1%7D%7B1-%5Cfrac%7Br%5E2_%7Bcontrol%7D%7D%7B4%7D%7D-1%5CBigg)*100%5C%25="><br><img src="https://latex.codecogs.com/png.latex?%5CBigg(%5Cfrac%7B1%7D%7B1-%5Cfrac%7B0.2%7D%7B4%7D%7D-1%5CBigg)*100%5C%25%20="> <em><img src="https://latex.codecogs.com/png.latex?5%5C%25"></em>
</td>
</tr>
</tbody>

</table>
</div>
<p>Assuming a constant absolute delta, the effective sample size increase from covariate adjustment is expected to be <img src="https://latex.codecogs.com/png.latex?25%5C%25">. However, <em>with a constant proportional treatment effect (which implies HTEs), the ESSI decreases to <img src="https://latex.codecogs.com/png.latex?18%5C%25"></em>. A third, not entirely unrealistic worst-case scenario of no correlation between the covariate and outcome in the active treatment arm reduces the ESSI to just <img src="https://latex.codecogs.com/png.latex?5%5C%25">. The drop in ESSI with HTEs would be an important caveat to communicate if there is a temptation to decrease trial sample size.</p>
<p>All ESSI formulas assume a particular pattern of HTEs. Due to the risk of under-powering, if no prior data is available on HTEs, we do not routinely recommend reducing sample sizes for Phase 2 trials. Usually when designing a Phase 3 trial, there are data from earlier studies on HTEs that can be used to get direct estimates of HTEs and improve ESSI estimates. These improved ESSI estimates could be used to inform sample size reduction, if other trial requirements allow for it (e.g.&nbsp;safety and subgroup analyses). Whenever sample size is reduced, be sure to communicate these risks, consider the noise in ESSI estimates and study-to-study effects and tailor sample size reductions accordingly.</p>
</section>
</section>
<section id="application-to-geographic-atrophy-ga" class="level1">
<h1>Application to Geographic Atrophy (GA)</h1>
<p>Geographic atrophy (GA) is an advanced form of age-related macular degeneration (AMD) and is a major cause of blindness in developed countries worldwide. GA progression is assessed by GA lesion growth from baseline over time using Fundus Autofluorescence (FAF) images. This is usually the primary endpoint in clinical trials that evaluate treatment effects on progression.</p>
<div id="fig-ga-lesion-growth" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://github.com/stats-4-datascience/blog/posts/covariate_adjustment/images/ga_lesion_growth.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6: GA lesion progression over time on an FAF image</figcaption><p></p>
</figure>
</div>
<p>Because GA progression is slow and varies between patients, clinical trials need to enroll many patients and follow them over a long period of time. Most recent GA trial designs followed patients for 12-18 months for the primary efficacy endpoint. However, the within patient noise of measuring GA progression over time is small and GA lesions progress linearly over the duration of the trial, making this a promising problem for a prediction model. This also allows us to use slopes calculated for each patient as the outcome for the prognostic modeling. Clinical trials often use change from baseline in GA area or slopes as outcomes, and sensitivity analyses showed that those are highly correlated and results are very similar for both. For simplicity, we will show the results for predicting the individual slopes.</p>
<div id="fig-ga-indi-patients" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://github.com/stats-4-datascience/blog/posts/covariate_adjustment/images/ga_indiv_patients.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;7: Examples of GA lesion growth for individual patients</figcaption><p></p>
</figure>
</div>
<section id="prognostic-model-development" class="level2">
<h2 class="anchored" data-anchor-id="prognostic-model-development">Prognostic Model Development</h2>
<p>The modeling and data strategy was pre-specified and allowed a rigorous and fair comparison of a number of models with different levels of complexity and operational burden. The goal was to find the least complex model with the best performance.</p>
<p>Data from a previous GA development program was harmonized and divided into training, hold-out, and independent test sets as shown in the figure. The independent tests sets consisted of two studies that were entirely excluded from the training and hold-out evaluations. Within the training data set, models were trained using additional re-sampling techniques for parameter tuning and performance estimation.</p>
<div id="fig-ga-data-strategy" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://github.com/stats-4-datascience/blog/posts/covariate_adjustment/images/ga_data_strategy.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;8: Data Strategy for Model Development</figcaption><p></p>
</figure>
</div>
<p>During training a model was selected from each of three model groups for hold-out and independent test set evaluation. These model groups were defined as:</p>
<ul>
<li>Model Group 1 (Benchmark): The first group of models was based on a set of standard, pre-specified features that are assessed at baseline. Various types of models were evaluated and compared, from a simple linear model to different types of machine learning models (e.g.&nbsp;support vector machines, gradient boosted machines, random forests, etc.). The features were:
<ul>
<li>Demographics: age, sex, smoking status</li>
<li>Measures of visual acuity: best corrected visual acuity (BCVA), low luminance visual acuity (LLVA), low luminance deficit (LLD = BCVA - LLVA)</li>
<li>Anatomical features assessed by a reading center: lesion size, lesion contiguity, lesion location, lesion distance to fovea, reticular pseudodrusen</li>
</ul></li>
<li>Model Group 2 (Benchmark + Run-in): This group additionally included a “run-in” estimate of growth rate, i.e.&nbsp;using the first 6 month to estimate the GA growth rate and predicting the future. Note that this required re-baselining the remaining outcome data to the 6 month time point. Due to the linear growth and low within patient noise, this is a promising approach but operationally complex to include in a trial as it requires high quality historical images from patients, or starting the trial with a pre-treatment run-in period, which may slow trial recruitment and delay the trial readout.</li>
<li>Model Group 3 (DL Models): The third group of models consists of end-to-end deep learning (DL) models from FAF and also OCT images at baseline. Three types of models were developed (each exploring different architectures): using FAF only, using OCT only and using FAF and OCT together <span class="citation" data-cites="DLpaper">(Anegondi et al. 2022)</span>. The FAF only model would be the operationally preferred model.</li>
</ul>
<p>One model was selected from each model group using the relevant performance metrics (<img src="https://latex.codecogs.com/png.latex?r%5E2"> and ESSI) estimated using re-sampling techniques. The three selected models were then tested and compared on the holdout and independent test sets.</p>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<p>The selected models were:</p>
<ul>
<li>Model Group 1 (Benchmark): A simple linear model with four features (lesion size, lesion contiguity, lesion distance to fovea and LLD) gave the best performance. More complex models like tree based models or added features did not add performance.</li>
<li>Model Group 2 (Benchmark + Run-in): A simple linear model with the four features from model group 1 and an estimate of growth rate as an additional fifth feature.</li>
<li>Model Group 3 (DL Models): A multi-task DL (CNN) model that uses a single FAF image as input to predict GA lesion size (same image) and GA growth rate <span class="citation" data-cites="DLpaper">(Anegondi et al. 2022)</span>.</li>
</ul>
<p>The results were quite impressive; although operationally complex to implement, the run-in model appears to effectively increase the sample size by <img src="https://latex.codecogs.com/png.latex?1/3"> compared to an analysis using the benchmark model. If no DL FAF model were available, the additional complexity of the run-in might be justified by the <img src="https://latex.codecogs.com/png.latex?ESSI">. However, the DL FAF model outperformed the benchmark and even the benchmark+run-in model in the hold-out as well as the two independent test sets <span class="citation" data-cites="DLpaper">(Anegondi et al. 2022)</span>. This implies that the logistical complexities of the run-in model can be avoided with the DL FAF model, which is based on a single baseline time point and is still relatively simple to implement <span class="citation" data-cites="DLpaper">(Anegondi et al. 2022)</span>. The table shows that the DL FAF model substantially increases the ESSI by at least 90% when compared to an unadjusted analysis and by 40%-80% compared to a simple adjustment using the known baseline features.</p>
<div id="fig-ga-model-results" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://github.com/stats-4-datascience/blog/posts/covariate_adjustment/images/ga_model_results.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;9: Results from selected model per group</figcaption><p></p>
</figure>
</div>
</section>
<section id="model-in-action" class="level2">
<h2 class="anchored" data-anchor-id="model-in-action">Model in Action</h2>
<p>The MAHALO study independent test set shown in the previous section was the Phase 2 trial for Lampalizumab. There was an observed treatment effect of ∼20% slowing progression in the higher dose (LQ4 - monthly treatment) compared to the Sham (no treatment) arm. This effect was not confirmed by the two large global Phase 3 trials (SPECTRI and CHROMA), so in retrospect we know this was a false-positive result. When the decision was made to move forward with a Phase 3 trial, the DL FAF model was not available. However, it is an interesting post-hoc exercise to see how results from the Phase 2 trial would have changed had the DL FAF model been used for covariate adjustment.</p>
<p>The figure below shows the estimated mean changes in GA lesion size with and without adjusting for the FAF DL model. Covariate adjustment leads to better precision and hence tighter confidence intervals for the treatment effect estimate and also changes the estimate in case of imbalances. In this example, the estimate changes (from 20% to 6%) and becomes non-significant (a p-value threshold of 0.2 is usually used in a Phase 2 trial).</p>
<div id="fig-ga-mahalo-ex" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://github.com/stats-4-datascience/blog/posts/covariate_adjustment/images/ga_mahalo_example.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;10: GA progression over time by treatment arm for unadjusted and adjusted analysis</figcaption><p></p>
</figure>
</div>
<div id="fig-ga-mahalo-cis" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://github.com/stats-4-datascience/blog/posts/covariate_adjustment/images/ga_mahalo_cis.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;11: Treatment effect estimates</figcaption><p></p>
</figure>
</div>
<p>While there were many other considerations in the decision to move into Phase 3 trials (e.g.&nbsp;subgroup analyses), if results from the adjusted analysis had been available, Phase 3 decision making may have been different.</p>
</section>
</section>
<section id="brief-recap" class="level1">
<h1>Brief Recap</h1>
<p>Below is a brief outline summarizing how to get the most out of prognostic baseline variables via covariate adjustment:</p>
<ul>
<li>Analyze external/historical data
<ul>
<li>build a score/model</li>
<li>evaluate with ESSIs</li>
</ul></li>
<li>Determine model budget
<ul>
<li>use methods developed for prognostic modeling</li>
</ul></li>
<li>Spend the model budget wisely
<ul>
<li>choose the most prognostic factors among stratification and other prognostic variables</li>
<li>avoid dichotomania</li>
<li>by default, do not use interactions</li>
</ul></li>
<li>Estimate ATEs and TAMs
<ul>
<li>use standardized estimator</li>
<li>also works for non-continuous outcomes</li>
</ul></li>
<li>Inference for ATEs and TAMs
<ul>
<li>suggest SE estimates from TMLE literature</li>
<li>re-sampling methods are also an option</li>
</ul></li>
</ul>
</section>
<section id="discussion" class="level1">
<h1>Discussion</h1>
<p>Covariate adjustment is a statistical free lunch. You can enjoy a rigorous, model-assisted analysis that increases precision and power, accounts for conditional bias and is supported by regulatory authorities at virtually no cost. With the growing availability of high-quality data for the identification of robust prognostic covariates, the benefits of an adjusted analysis will only continue to increase. We focused here on continuous endpoints, however the same standardized estimator and variance estimates can be applied to non-continuous outcomes (e.g.&nbsp;binary or ordinal) by substituting in the appropriate generalized linear “working” model and making minor adaptations in the standard error estimates <span class="citation" data-cites="TMLE2010">(Rosenblum and Laan 2010)</span>. ESSI formulas for non-continuous outcomes are a work-in-progress. While the concept of covariate adjustment has been around for decades, it seems to be underutilized or sub-optimally implemented (e.g.&nbsp;poor choice of covariates). We hope this guidance makes it clear how to rigorously implement covariate adjustment in randomized trials and encourages statisticians to take advantage of all of the benefits of adjusted analyses for randomized clinical trials.</p>
</section>
<section id="acknowledgements" class="level1">
<h1>Acknowledgements</h1>
<p><strong>Roche GA Development Team</strong>: Neha Anegondi, Lee Honigberg, Simon Gao, Qi Yang, Daniela Ferrara, Julia Cluceru, Verena Steffen</p>
<p><strong>Methods, Collaboration, Outreach (MCO)</strong>: Ray Lin, Marcel Wolbers</p>
<p><strong>Website Development</strong>: Doug Kelkhoff</p>
</section>
<section id="appendix" class="level1">
<h1>Appendix</h1>
<section id="essi-formulas-1" class="level2">
<h2 class="anchored" data-anchor-id="essi-formulas-1">ESSI Formulas</h2>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?Y"> continuous outcome, <img src="https://latex.codecogs.com/png.latex?X"> baseline covariate</li>
<li><strong>Constant absolute <img src="https://latex.codecogs.com/png.latex?%5Cdelta"></strong>: <img src="https://latex.codecogs.com/png.latex?E(Y_%7Bactive%7D%7CX)=E(Y_%7Bcontrol%7D%7CX)-%5Cdelta"></li>
<li><strong>Constant proportional <img src="https://latex.codecogs.com/png.latex?%5Cdelta"></strong>: <img src="https://latex.codecogs.com/png.latex?E(Y_%7Bactive%7D%7CX)=(1-%5Cdelta)E(Y_%7Bcontrol%7D%7CX)"></li>
<li><img src="https://latex.codecogs.com/png.latex?r_%7Bcontrol%7D"> and <img src="https://latex.codecogs.com/png.latex?r_%7Bactive%7D">: correlation between <img src="https://latex.codecogs.com/png.latex?Y"> and <img src="https://latex.codecogs.com/png.latex?X"> given control and active treatment, respectively.</li>
<li><img src="https://latex.codecogs.com/png.latex?k=%5Cfrac%7B%5Csigma_%7BY_%7Bactive%7D%7D%7D%7B%5Csigma_%7BY_%7Bcontrol%7D%7D%7D"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cpi=Probability%5C;of%5C;active%5C;treatment">. Note, when <img src="https://latex.codecogs.com/png.latex?%5Cpi=1/2"> the ESSI corresponding to the additive model will always equal the ESSI corresponding to the interaction model.</li>
</ul>
<div id="fig-essi-formulas" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://github.com/stats-4-datascience/blog/posts/covariate_adjustment/images/essi_formulas.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;12: ESSI Formulas</figcaption><p></p>
</figure>
</div>
</section>
<section id="huber-white-sandwich-estimator" class="level2">
<h2 class="anchored" data-anchor-id="huber-white-sandwich-estimator">Huber White Sandwich Estimator</h2>
<p>Below is a simple simulation demonstrating how the Huber-White robust “sandwich” estimator will give standard error estimates that are too small when treatment effects are heterogeneous and a working regression model with treatment-by-covariate interactions is used.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><span class="fu" style="color: #4758AB;">library</span>(dplyr)</span>
<span id="cb6-2"><span class="fu" style="color: #4758AB;">library</span>(ggplot2)</span>
<span id="cb6-3"><span class="fu" style="color: #4758AB;">library</span>(sandwich)</span>
<span id="cb6-4"><span class="fu" style="color: #4758AB;">library</span>(doParallel)</span>
<span id="cb6-5"><span class="fu" style="color: #4758AB;">library</span>(foreach)</span>
<span id="cb6-6"><span class="fu" style="color: #4758AB;">registerDoParallel</span>(<span class="at" style="color: #657422;">cores=</span><span class="dv" style="color: #AD0000;">4</span>)</span>
<span id="cb6-7"></span>
<span id="cb6-8"><span class="do" style="color: #5E5E5E;
font-style: italic;">################### SIMULATION STUDIES</span></span>
<span id="cb6-9"></span>
<span id="cb6-10"><span class="do" style="color: #5E5E5E;
font-style: italic;">####### Paradigm 1: Superpopulation</span></span>
<span id="cb6-11"></span>
<span id="cb6-12"><span class="co" style="color: #5E5E5E;"># Sample of size N from a population of covariates</span></span>
<span id="cb6-13"><span class="co" style="color: #5E5E5E;"># Each patient's treatment assignment is a Bernoulli(1/2) random variable</span></span>
<span id="cb6-14"><span class="co" style="color: #5E5E5E;"># Then generate a random outcome from the model using the covariate value </span></span>
<span id="cb6-15"><span class="co" style="color: #5E5E5E;"># and treatment for all N patients</span></span>
<span id="cb6-16"></span>
<span id="cb6-17">a <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">3</span></span>
<span id="cb6-18">b <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">5</span></span>
<span id="cb6-19">c <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb6-20">d  <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">5</span></span>
<span id="cb6-21"></span>
<span id="cb6-22">var.eps0 <span class="ot" style="color: #003B4F;">&lt;-</span> var.eps1 <span class="ot" style="color: #003B4F;">&lt;-</span>  .<span class="dv" style="color: #AD0000;">6</span></span>
<span id="cb6-23"></span>
<span id="cb6-24">p <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">/</span> <span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb6-25">N <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">10000</span></span>
<span id="cb6-26">nsamp <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">10000</span></span>
<span id="cb6-27"></span>
<span id="cb6-28">p1.estimates <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">foreach</span>(<span class="at" style="color: #657422;">i=</span><span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span>nsamp,<span class="at" style="color: #657422;">.combine =</span> rbind) <span class="sc" style="color: #5E5E5E;">%dopar%</span> {</span>
<span id="cb6-29">  X.sim <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">runif</span>(N,<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">1</span>) <span class="co" style="color: #5E5E5E;"># sample covariate</span></span>
<span id="cb6-30">  X.sim.center <span class="ot" style="color: #003B4F;">&lt;-</span> X.sim <span class="sc" style="color: #5E5E5E;">-</span> <span class="fu" style="color: #4758AB;">mean</span>(X.sim) <span class="co" style="color: #5E5E5E;"># center covariate</span></span>
<span id="cb6-31"></span>
<span id="cb6-32">  <span class="co" style="color: #5E5E5E;"># outcome under control</span></span>
<span id="cb6-33">  y0.sim  <span class="ot" style="color: #003B4F;">&lt;-</span>  a<span class="sc" style="color: #5E5E5E;">*</span>X.sim <span class="sc" style="color: #5E5E5E;">+</span> b <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">rnorm</span>(N,<span class="dv" style="color: #AD0000;">0</span>,<span class="fu" style="color: #4758AB;">sqrt</span>(var.eps0))</span>
<span id="cb6-34"></span>
<span id="cb6-35">  <span class="co" style="color: #5E5E5E;"># outcome under treatment</span></span>
<span id="cb6-36">  y1.sim  <span class="ot" style="color: #003B4F;">&lt;-</span>   c<span class="sc" style="color: #5E5E5E;">*</span>X.sim <span class="sc" style="color: #5E5E5E;">+</span> d <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">rnorm</span>(N,<span class="dv" style="color: #AD0000;">0</span>,<span class="fu" style="color: #4758AB;">sqrt</span>(var.eps1))</span>
<span id="cb6-37"></span>
<span id="cb6-38">  A.sim <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">rbinom</span>(N,<span class="dv" style="color: #AD0000;">1</span>,p)</span>
<span id="cb6-39"></span>
<span id="cb6-40">  Y.sim <span class="ot" style="color: #003B4F;">&lt;-</span> y1.sim<span class="sc" style="color: #5E5E5E;">*</span>A.sim <span class="sc" style="color: #5E5E5E;">+</span> y0.sim<span class="sc" style="color: #5E5E5E;">*</span>(<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">-</span>A.sim)</span>
<span id="cb6-41"></span>
<span id="cb6-42">  df.sim <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">data.frame</span>(Y.sim, A.sim, X.sim, X.sim.center) <span class="co" style="color: #5E5E5E;"># A=0,1,2,...treatment arm factor</span></span>
<span id="cb6-43"></span>
<span id="cb6-44">  <span class="do" style="color: #5E5E5E;
font-style: italic;">## Additive Model</span></span>
<span id="cb6-45"></span>
<span id="cb6-46">  lm.fit <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">lm</span>(Y.sim <span class="sc" style="color: #5E5E5E;">~</span> A.sim <span class="sc" style="color: #5E5E5E;">+</span> X.sim, <span class="at" style="color: #657422;">data =</span> df.sim)</span>
<span id="cb6-47"></span>
<span id="cb6-48">  lm.se <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">summary</span>(lm.fit)[[<span class="dv" style="color: #AD0000;">4</span>]][<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">2</span>]</span>
<span id="cb6-49"></span>
<span id="cb6-50">  <span class="co" style="color: #5E5E5E;"># set treatment indicator to active treatment for all subjects</span></span>
<span id="cb6-51">  pred1 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">predict</span>(lm.fit, <span class="at" style="color: #657422;">newdata =</span> df.sim <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">A.sim=</span><span class="dv" style="color: #AD0000;">1</span>))</span>
<span id="cb6-52"></span>
<span id="cb6-53">  <span class="co" style="color: #5E5E5E;"># set treatment indicator to control for all subjects</span></span>
<span id="cb6-54">  pred0 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">predict</span>(lm.fit, <span class="at" style="color: #657422;">newdata =</span> df.sim <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">A.sim=</span><span class="dv" style="color: #AD0000;">0</span>)) </span>
<span id="cb6-55"></span>
<span id="cb6-56">  ate.add <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">mean</span>(pred1) <span class="sc" style="color: #5E5E5E;">-</span> <span class="fu" style="color: #4758AB;">mean</span>(pred0)</span>
<span id="cb6-57"></span>
<span id="cb6-58">  ate.add.se <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">sqrt</span>(<span class="fu" style="color: #4758AB;">mean</span>((A.sim<span class="sc" style="color: #5E5E5E;">*</span>(Y.sim<span class="sc" style="color: #5E5E5E;">-</span>pred1)<span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">mean</span>(A.sim<span class="sc" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">1</span>) <span class="sc" style="color: #5E5E5E;">+</span> pred1 <span class="sc" style="color: #5E5E5E;">-</span></span>
<span id="cb6-59">                <span class="fu" style="color: #4758AB;">mean</span>(pred1)<span class="sc" style="color: #5E5E5E;">-</span>((<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">-</span>A.sim)<span class="sc" style="color: #5E5E5E;">*</span>(Y.sim<span class="sc" style="color: #5E5E5E;">-</span>pred0)<span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">mean</span>(A.sim<span class="sc" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">0</span>) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb6-60">                               pred0 <span class="sc" style="color: #5E5E5E;">-</span> <span class="fu" style="color: #4758AB;">mean</span>(pred0)))<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span>))<span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">sqrt</span>(N)</span>
<span id="cb6-61"></span>
<span id="cb6-62">  <span class="do" style="color: #5E5E5E;
font-style: italic;">## Interaction Model</span></span>
<span id="cb6-63"></span>
<span id="cb6-64">  lm.fit <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">lm</span>(Y.sim <span class="sc" style="color: #5E5E5E;">~</span> A.sim<span class="sc" style="color: #5E5E5E;">*</span>X.sim, <span class="at" style="color: #657422;">data =</span> df.sim)</span>
<span id="cb6-65"></span>
<span id="cb6-66">  <span class="co" style="color: #5E5E5E;"># set treatment indicator to active treatment for all subjects</span></span>
<span id="cb6-67">  pred1 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">predict</span>(lm.fit, <span class="at" style="color: #657422;">newdata =</span> df.sim <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">A.sim=</span><span class="dv" style="color: #AD0000;">1</span>))</span>
<span id="cb6-68"></span>
<span id="cb6-69">  <span class="co" style="color: #5E5E5E;"># set treatment indicator to control for all subjects</span></span>
<span id="cb6-70">  pred0 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">predict</span>(lm.fit, <span class="at" style="color: #657422;">newdata =</span> df.sim <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">A.sim=</span><span class="dv" style="color: #AD0000;">0</span>))</span>
<span id="cb6-71"></span>
<span id="cb6-72">  ate.int <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">mean</span>(pred1) <span class="sc" style="color: #5E5E5E;">-</span> <span class="fu" style="color: #4758AB;">mean</span>(pred0)</span>
<span id="cb6-73"></span>
<span id="cb6-74">  ate.int.se <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">sqrt</span>(<span class="fu" style="color: #4758AB;">mean</span>((A.sim<span class="sc" style="color: #5E5E5E;">*</span>(Y.sim<span class="sc" style="color: #5E5E5E;">-</span>pred1)<span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">mean</span>(A.sim<span class="sc" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">1</span>) <span class="sc" style="color: #5E5E5E;">+</span> pred1 <span class="sc" style="color: #5E5E5E;">-</span> </span>
<span id="cb6-75">                             <span class="fu" style="color: #4758AB;">mean</span>(pred1)<span class="sc" style="color: #5E5E5E;">-</span>((<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">-</span>A.sim)<span class="sc" style="color: #5E5E5E;">*</span>(Y.sim<span class="sc" style="color: #5E5E5E;">-</span>pred0)<span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">mean</span>(A.sim<span class="sc" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">0</span>) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb6-76">                                            pred0 <span class="sc" style="color: #5E5E5E;">-</span> <span class="fu" style="color: #4758AB;">mean</span>(pred0)))<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span>))<span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">sqrt</span>(N)</span>
<span id="cb6-77"></span>
<span id="cb6-78">  <span class="do" style="color: #5E5E5E;
font-style: italic;">## HW SEs</span></span>
<span id="cb6-79"></span>
<span id="cb6-80">  lm.fit <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">lm</span>(Y.sim <span class="sc" style="color: #5E5E5E;">~</span> A.sim <span class="sc" style="color: #5E5E5E;">+</span> X.sim, <span class="at" style="color: #657422;">data =</span> df.sim)</span>
<span id="cb6-81">  hw.add <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">sqrt</span>(<span class="fu" style="color: #4758AB;">vcovHC</span>(lm.fit, <span class="at" style="color: #657422;">type=</span><span class="st" style="color: #20794D;">"HC0"</span>)[<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">2</span>])</span>
<span id="cb6-82"></span>
<span id="cb6-83">  lm.fit <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">lm</span>(Y.sim <span class="sc" style="color: #5E5E5E;">~</span> A.sim<span class="sc" style="color: #5E5E5E;">*</span>X.sim.center, <span class="at" style="color: #657422;">data =</span> df.sim)</span>
<span id="cb6-84">  hw.int <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">sqrt</span>(<span class="fu" style="color: #4758AB;">vcovHC</span>(lm.fit, <span class="at" style="color: #657422;">type=</span><span class="st" style="color: #20794D;">"HC0"</span>)[<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">2</span>]) </span>
<span id="cb6-85"></span>
<span id="cb6-86">  lm.fit <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">lm</span>(Y.sim <span class="sc" style="color: #5E5E5E;">~</span> A.sim<span class="sc" style="color: #5E5E5E;">*</span>X.sim, <span class="at" style="color: #657422;">data =</span> df.sim)</span>
<span id="cb6-87">  ate <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">coefficients</span>(lm.fit)[<span class="dv" style="color: #AD0000;">2</span>] <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">coefficients</span>(lm.fit)[<span class="dv" style="color: #AD0000;">4</span>]<span class="sc" style="color: #5E5E5E;">*</span><span class="fu" style="color: #4758AB;">mean</span>(df.sim<span class="sc" style="color: #5E5E5E;">$</span>X.sim)</span>
<span id="cb6-88">  myvcov <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">vcovHC</span>(lm.fit, <span class="at" style="color: #657422;">type=</span><span class="st" style="color: #20794D;">"HC0"</span>)</span>
<span id="cb6-89">  ate.sd <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">sqrt</span>(<span class="fu" style="color: #4758AB;">mean</span>(df.sim<span class="sc" style="color: #5E5E5E;">$</span>X.sim)<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">*</span>myvcov[<span class="dv" style="color: #AD0000;">4</span>,<span class="dv" style="color: #AD0000;">4</span>]<span class="sc" style="color: #5E5E5E;">+</span>myvcov[<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">2</span>] <span class="sc" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">*</span><span class="fu" style="color: #4758AB;">mean</span>(df.sim<span class="sc" style="color: #5E5E5E;">$</span>X.sim)<span class="sc" style="color: #5E5E5E;">*</span>myvcov[<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">4</span>])</span>
<span id="cb6-90"></span>
<span id="cb6-91"></span>
<span id="cb6-92">  <span class="fu" style="color: #4758AB;">return</span>(<span class="fu" style="color: #4758AB;">c</span>(ate.add, ate.int, ate.add.se, ate.int.se, hw.add, hw.int, lm.se))</span>
<span id="cb6-93">}</span>
<span id="cb6-94"></span>
<span id="cb6-95">p1.se.truth <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">sqrt</span>(<span class="fu" style="color: #4758AB;">apply</span>(p1.estimates[,<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">2</span>],<span class="dv" style="color: #AD0000;">2</span>,var))</span>
<span id="cb6-96"></span>
<span id="cb6-97">tmle.mean.se <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">apply</span>(p1.estimates[,<span class="dv" style="color: #AD0000;">3</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">4</span>],<span class="dv" style="color: #AD0000;">2</span>,mean)</span>
<span id="cb6-98"></span>
<span id="cb6-99">hw.mean.se <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">apply</span>(p1.estimates[,<span class="dv" style="color: #AD0000;">5</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">6</span>],<span class="dv" style="color: #AD0000;">2</span>,mean)</span>
<span id="cb6-100"></span>
<span id="cb6-101">t <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">t</span>(<span class="fu" style="color: #4758AB;">data.frame</span>(<span class="at" style="color: #657422;">truth =</span> p1.se.truth, <span class="at" style="color: #657422;">tmle =</span> tmle.mean.se, <span class="at" style="color: #657422;">HW =</span> hw.mean.se))</span>
<span id="cb6-102"></span>
<span id="cb6-103"><span class="fu" style="color: #4758AB;">rownames</span>(t) <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="st" style="color: #20794D;">"True SE"</span>,<span class="st" style="color: #20794D;">"Mean TMLE SE"</span>,<span class="st" style="color: #20794D;">"Mean HW SE"</span>)</span>
<span id="cb6-104"></span>
<span id="cb6-105"><span class="fu" style="color: #4758AB;">colnames</span>(t) <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="st" style="color: #20794D;">"Additive Working Regression Model"</span>,<span class="st" style="color: #20794D;">"Interaction Working Regression Model"</span>)</span>
<span id="cb6-106"></span>
<span id="cb6-107">knitr<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">kable</span>(t, <span class="at" style="color: #657422;">align =</span> <span class="st" style="color: #20794D;">"cc"</span>, <span class="at" style="color: #657422;">caption =</span> <span class="st" style="color: #20794D;">"Superpopulation Sampling Framework"</span>)</span></code></pre></div>
<div class="cell-output-display">
<table class="table table-sm table-striped">
<caption>Superpopulation Sampling Framework</caption>
<colgroup>
<col style="width: 15%">
<col style="width: 40%">
<col style="width: 44%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: center;">Additive Working Regression Model</th>
<th style="text-align: center;">Interaction Working Regression Model</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">True SE</td>
<td style="text-align: center;">0.0214373</td>
<td style="text-align: center;">0.0214312</td>
</tr>
<tr class="even">
<td style="text-align: left;">Mean TMLE SE</td>
<td style="text-align: center;">0.0211753</td>
<td style="text-align: center;">0.0211741</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Mean HW SE</td>
<td style="text-align: center;">0.0211780</td>
<td style="text-align: center;">0.0154906</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell">

</div>
<div class="cell">

</div>
</section>
</section>
<section id="references" class="level1">




</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-DLpaper" class="csl-entry">
Anegondi, Neha, Simon S. Gao, Verena Steffen, Richard F. Spaide, SriniVas R. Sadda, Frank G. Holz, Christina Rabe, et al. 2022. <span>“Deep Learning to Predict Geographic Atrophy Area and Growth Rate from Multi-Modal Imaging.”</span> <em>Ophthalmology Retina</em>. <a href="https://doi.org/10.1016/j.oret.2022.08.018">https://doi.org/10.1016/j.oret.2022.08.018</a>.
</div>
<div id="ref-covid" class="csl-entry">
Benkeser, David, Ivan Diaz, Alex Luedtke, Jodi Segal, Daniel Scharfstein, and Michael Rosenblum. 2020. <span>“Improving Precision and Power in Randomized Trials for COVID-19 Treatments Using Covariate Adjustment, for Binary, Ordinal, and Time-to-Event Outcomes.”</span> <em>Biometrics</em>.
</div>
<div id="ref-Colantuoni2015" class="csl-entry">
Colantuoni, Elizabeth, and Michael Rosenblum. 2015. <span>“Leveraging Prognostic Baseline Variables to Gain Precision in Randomized Trials.”</span> <em>Stat. Med</em>.
</div>
<div id="ref-Daniel" class="csl-entry">
Daniel, Rhian, Jingjing Zhang, and Daniel Farewell. 2020. <span>“Making Apples from Oranges: Comparing Noncollapsible Effect Estimators and Their Standard Errors After Adjustment for Different Covariate Sets.”</span> <em>Biometrical Journal</em>.<a href=" https://doi.org/10.1002/bimj.201900297"> https://doi.org/10.1002/bimj.201900297</a>.
</div>
<div id="ref-CUPED" class="csl-entry">
Deng, Alex, Ya Xu, Ron Kohavi, and Toby Walker. 2013. <span>“Improving the Sensitivity of Online Controlled Experiments by Utilizing Pre-Experiment Data.”</span> <em>WSDM</em>.
</div>
<div id="ref-Peng" class="csl-entry">
Ding, Peng, Xinran Li, and Luke Miratrix. 2017. <span>“Bridging Finite and Super Population Causal Inference.”</span> <em>J. Causal Infer.</em>
</div>
<div id="ref-Ding" class="csl-entry">
———. 2019. <span>“Rerandomization and Regression Adjustment.”</span> <em>arXiv</em>. <a href="https://arxiv.org/abs/1906.11291">https://arxiv.org/abs/1906.11291</a>.
</div>
<div id="ref-FDA" class="csl-entry">
FDA, U. S. 2021. <span>“Adjusting for Covariates in Randomized Clinical Trials for Drugs and Biological Products Guidance for Industry.”</span> <a href="https://www.fda.gov/regulatory-information/search-fda-guidance-documents/adjusting-covariates-randomized-clinical-trials-drugs-and-biological-products">https://www.fda.gov/regulatory-information/search-fda-guidance-documents/adjusting-covariates-randomized-clinical-trials-drugs-and-biological-products</a>.
</div>
<div id="ref-Freedman2008a" class="csl-entry">
Freedman, D. A. 2008a. <span>“Randomization Does Not Justify Logistic Regression.”</span> <em>Statistical Science</em>, 2008a.
</div>
<div id="ref-Freedman2008b" class="csl-entry">
Freedman, David A. 2008b. <span>“ON REGRESSION ADJUSTMENTS IN EXPERIMENTS WITH SEVERAL TREATMENTS.”</span> <em>The Annals of Applied Statistics</em>, 2008b.
</div>
<div id="ref-Ge2011" class="csl-entry">
Ge, Miaomiao, L. Kathryn Durham, R. Daniel Meyer, Wangang Xie, and Neal Thomas. 2011. <span>“Covariate-Adjusted Difference in Proportions from Clinical Trials Using Logistic Regression and Weighted Risk Differences.”</span> <em>Drug Information Journal</em>.
</div>
<div id="ref-MLRATE" class="csl-entry">
Guo, Yongyi, Dominic Coey, Mikael Konutgan an Wenting Li, Chris Schoener, and Matt Goldman. 2022. <span>“Machine Learning for Variance Reduction in Online Experiments.”</span> <em>arXiv</em>.
</div>
<div id="ref-rms" class="csl-entry">
Harrell, Frank E. 2011. <em>Regression Modeling Strategies</em>. New York: Springer.
</div>
<div id="ref-imbens" class="csl-entry">
Imbens, Guido, and Jeffrey Wooldridge. 2009. <span>“Recent Developments in the Econometrics of Program Evaluation.”</span> <em>J. Econ. Lit</em>.
</div>
<div id="ref-CUPAC" class="csl-entry">
Li, Jeff, Yixin Tang, and Jared Bauman. 2020. <span>“Improving Experimental Power Through Control Using Predictions as Covariates.”</span> <a href="https://doordash.engineering/2020/06/08/improving-experimental-power-through-control-using-predictions-as-covariate-cupac/" class="uri">https://doordash.engineering/2020/06/08/improving-experimental-power-through-control-using-predictions-as-covariate-cupac/</a>.
</div>
<div id="ref-lin2013" class="csl-entry">
Lin, Winston. 2013. <span>“Agnsotic Notes on Regression Adjustments to Experimental Data: Reexamining Freedman’s Critique.”</span> <em>The Annals of Applied Statistics</em>.
</div>
<div id="ref-Riley2019" class="csl-entry">
Riley, Richard D, Kym Snell, Joie Ensor, Danielle Burke, Frank Harrell, Karel Moons, and Gary Collins. 2019. <span>“Minimum Sample Size for Developing a Multivariable Prediction Model: Part i - Continuous Outcomes.”</span> <em>Stat. Med.</em>
</div>
<div id="ref-Riley2020" class="csl-entry">
Riley, Richard, Joie Ensor, Kym Snell, Frank Harrell, Glen Martin, Johannes Reitsma associate, Karel Moons, Gary Collins, and Maarten van Smeden. 2020. <span>“Calculating the Sample Size Required for Developing a Clinical Prediction Model.”</span> <em>BMJ</em>.
</div>
<div id="ref-ML" class="csl-entry">
Rosenblum, Michael. 2020. <span>“Machine Learning for Leveraging Prognostic Baseline Variables to Gain Precision and Reduce Sample Size in Randomized Trials.”</span> <span class="smallcaps">url:</span>&nbsp;<a href="http://rosenblum.jhu.edu/" class="uri">http://rosenblum.jhu.edu/</a>.
</div>
<div id="ref-TMLE2010" class="csl-entry">
Rosenblum, Michael, and Mark J. van der Laan. 2010. <span>“Simple, Efficient Estimators of Treatment Effects in Randomized Trials Using Generalized Linear Models to Leverage Baseline Variables.”</span> <em>The International Journal of Biostatistics</em>.
</div>
<div id="ref-Rosenblum2009" class="csl-entry">
Rosenblum, Michael, and Mark van der Laan. 2009. <span>“Using Regression Models to Analyze Randomized Trials: Asymptotically Valid Hypothesis Tests Despite Incorrectly Specified Models.”</span> <em>Biometrics</em>.
</div>
<div id="ref-Steingrimsson2016" class="csl-entry">
Rosenblum, Michael, and Jon Steingrimsson. 2016. <span>“Matching the Efficiency Gains of the Logistic Regression Estimator While Avoiding Its Interpretability Problems, in Randomized Trials.”</span> <em>Johns Hopkins University, Dept. Of Biostatistics Working Papers.</em>
</div>
<div id="ref-Rudin" class="csl-entry">
Rudin, Cynthia. 2019. <span>“Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead.”</span> <em>Nat Mach Intell</em>. <a href="https://doi.org/10.1038/s42256-019-0048-x">https://doi.org/10.1038/s42256-019-0048-x</a>.
</div>
<div id="ref-unlearn" class="csl-entry">
Schuler, Alejandro, David Walsh, Diana Hall, Jon Walsh, and Charles Fisher. 2020. <span>“Increasing the Efficiency of Randomized Trial Estimates via Linear Adjustment for a Prognostic Score.”</span> <em>arXiv</em>.
</div>
<div id="ref-Senn" class="csl-entry">
Senn, S. J. 1989. <span>“Covariate Imbalance and Random Allocation in Clinical Trials.”</span> <em>Statistics in Medicine</em>.
</div>
<div id="ref-Senn2010" class="csl-entry">
Senn, Stephen. 2010. <span>“Some Considerations Concerning Covariates in Clinical Trials.”</span> <span class="smallcaps">url:</span>&nbsp;<a href="https://www.page-meeting.org/pdf_assets/9236-SennCovariates.pdf" class="uri">https://www.page-meeting.org/pdf_assets/9236-SennCovariates.pdf</a>.
</div>
<div id="ref-Dichotomania" class="csl-entry">
Senn, Stephen J. 2005. <span>“Dichotomania: An Obsessive Compulsive Disorder That Is Badly Affecting the Quality of Analysis of Pharmaceutical Trials.”</span> In.
</div>
<div id="ref-Tian2012" class="csl-entry">
Tian, Lu, Tianxi Cai, Lihui Zhao, and Lee-Jen Wei. 2012. <span>“On the Covariate-Adjusted Estimation for an Overall Treatment Difference with Data from a Randomized Comparative Clinical Trial.”</span> <em>Biostatistics</em>.
</div>
<div id="ref-owkin" class="csl-entry">
Trower, Antonia, Felix Balazard, and Radha Patel. 2020. <span>“Leveraging Machine Learning to Optimize Clinical Trials: The Power of Covariate Adjustment.”</span> <a href="https://owkin.com/publications-and-news/blogs/leveraging-machine-learning-to-optimize-clinical-trials-the-power-of-covariate-adjustment">https://owkin.com/publications-and-news/blogs/leveraging-machine-learning-to-optimize-clinical-trials-the-power-of-covariate-adjustment</a>.
</div>
<div id="ref-Tsiatis2008" class="csl-entry">
Tsiatis, Anastasios A., Marie Davidian, Min Zhang, and Xiaomin Lu. 2008. <span>“Covariate Adjustment for Two-Sample Treatment Comparisons in Randomized Clinical Trials: A Principled yet Flexible Approach.”</span> <em>Stat. Med.</em>
</div>
<div id="ref-Wang2021" class="csl-entry">
Wang, Bingkai, Ryoko Susukida, Ramin Mojtabai, Masoumeh Amin-Esmaeili, and Michael Rosenblum. 2021. <span>“Model-Robust Inference for Clinical Trials That Improve Precision by Stratified Randomization and Covariate Adjustment.”</span> <em>Journal of the American Statistical Association</em>.
</div>
<div id="ref-covid2" class="csl-entry">
Williams, Nicholas, Michael Rosenblum, and Ivan Diaz. 2021. <span>“Optimizing Precision and Power by Machine Learning in Randomized Trials, with an Application to COVID-19.”</span> <em>arXiv Preprint arXiv:2109.04294.</em>
</div>
<div id="ref-jackknife" class="csl-entry">
Wolbers, Marcel, Alessandro Noci, Paul Delmar, Craig Gower-Page, Sean Yiu, and Jonathan W. Bartlett. 2022. <span>“Standard and Reference-Based Conditional Mean Imputation.”</span> <em>Pharmaceutical Statistics</em>. <a href="https://doi.org/10.1002/pst.2234">https://doi.org/10.1002/pst.2234</a>.
</div>
<div id="ref-Netflix" class="csl-entry">
Wong, Jeffrey, Randall Lewis, and Matthew Wardrop. 2019. <span>“Efficient Computation of Linear Model Treatment Effects in an Experimentation Platform.”</span> <em>arXiv</em>.
</div>
<div id="ref-Tsiatis2001" class="csl-entry">
Yang, Li, and Anastasios A. Tsiatis. 2001. <span>“Efficiency Study of Estimators in a Pretest-Posttest Trial.”</span> <em>The American Statistician</em>.
</div>
</div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{schiffman2022,
  author = {Courtney Schiffman and Christina Rabe and Michel
    Friesenhahn},
  title = {How to Get the Most Out of Prognostic Baseline Variables in
    Clinical Trials. {Effective} Strategies for Employing Covariate
    Adjustment and Stratification.},
  date = {2022-10-12},
  url = {stats4datascience.com},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-schiffman2022" class="csl-entry quarto-appendix-citeas">
Courtney Schiffman, Christina Rabe, and Michel Friesenhahn. 2022.
<span>“How to Get the Most Out of Prognostic Baseline Variables in
Clinical Trials. Effective Strategies for Employing Covariate Adjustment
and Stratification.”</span> <a href="https://stats4datascience.com">stats4datascience.com</a>.
</div></div></section></div> ]]></description>
  <category>covariate adjustment</category>
  <category>stratification</category>
  <guid>https://github.com/stats-4-datascience/blog/posts/covariate_adjustment/index.html</guid>
  <pubDate>Wed, 12 Oct 2022 00:00:00 GMT</pubDate>
  <media:content url="https://github.com/stats-4-datascience/blog/posts/covariate_adjustment/images/increased_precision.png" medium="image" type="image/png" height="65" width="144"/>
</item>
</channel>
</rss>
